 John Mack, conocido popularmente como Pharmaguy (@ pharmaguy ), es el editor de Pharma Marketing News , un boletin electronico mensual independiente que cubre temas de especial relevancia para los ejecutivos de marketing de la industria farmaceutica. John, quien ademas es un prolifico bloguero , presento en Barcelona su ponencia  Pruebas y Tribulaciones de la Industria Farmaceutica en los Medios Sociales , en la que repasa los hitos mas importantes que han tenido lugar durante los ultimos seis anos asi como los principales problemas o fracasos. A continuacion transcribo el contenido integro de la entrevista que le hice con motivo de la mencionada presentacion. La version en castellano se puede leer en elglobal.net . Q. In your presentation your first milestone dates back to April 2006. Much has happened since. What's your assessment of these past years in Social Media and Pharma? A. I started my Pharma Social Media Timeline in 2005/6 because that is when the modern era of social media began with Blogger, for example. I started blogging in January 2005. This was the beginning of what used to be called the Web 2.0 era, where social media tools like Blogger were offered to ordinary users to create content and communities. The pharmaceutical industry was, in some respects, an early adopter of modern social media. But you really have to look at individual pharma companies to assess how sectors of the industry are using social media. GlaxoSmithKline, for example, has truly been an early adopter of the spirit of modern social media I recall that it sponsored an online smoking cessation forum on an a third-party patient site before 2000. But that was a Web 1.0 experience although there was true community amongst forum members at the time. Aside from viewing the pharma industry as a monolithic use of social media, the other problem is defining exactly what aspect of social media should we be looking at. I think the most important aspect is the direct conversation between pharma and its audience via social media such as Twitter, Facebook, and blogs. Some pharma companies are better than others at engaging in this conversation, but the industry as a whole have made great progress. Many companies now routinely handle customer complaints via Twitter and some have Facebook pages or their own sites that allow comments and conversation from and to consumers. Q. What should be the role of pharma companies in relation to Social Media? A. There are many ways that pharma companies can use social media. From the point of view of patients, pharma can use social media to support their products  ie, help patients report AEs [adverse events]. Of course, there is a problem in drawing the line between support and product promotion, which is not allowed in the EU. Q. What are the main mistakes that have been made by pharma companies? A. Thats difficult to say  there have been many kinds of mistakes. Hopefully, the industry has learned from these mistakes. Probably the biggest mistake a company can make when initiating a social media campaign is not to be prepared for crises that may arise. Companies must have robust social media standard operating procedures to cover all contingencies. Q. What have they done correctly? Which companies in your opinion are doing things right? A. Many companies are doing some things right and most are doing some things wrong. So I cannot single out one company that has done everything right, IMHO, with regard to social media. Boehringer Ingelheim has been noteworthy in my view of doing some things really right (eg, engaging consumers/patients via Twitter) and some things really wrong ( humorous YouTube video ). Pfizer is another example. The most recent social media campaign that Pfizer is doing right is the Getting Old campaign . Q. FDA promised guidelines by the end of 2010" and so far they have only issued a draft that focuses exclusively on social media discussion of off-label medication use. What are your expectations for the near future? A. A little-noticed "Miscellaneous Provision" of the " Food and Drug Administration Safety and Innovation Act " (aka PDUFA), which was signed into law by president Obama on July 10, 2012, simply states that not later than 2 years after the date of enactment of this Act, the Secretary of Health and Human Services shall issue guidance that describes Food and Drug Administration policy regarding the promotion, using the Internet (including social media), of medical products that are regulated by such Administration. This, however, may have more bark than bite. It seems that the FDA will dole out social media guidelines as part of other guidelines  as they did with guideline for responding to off-label questions. Yet the FDA has not come up with any guidelines covering topics discussed at the November 2009 public hearing. Its difficult to be optimistic about this given all the other things on FDAs plate. Plus, there are a few legal, first amendment challenges that pharma has raised AGAINST FDAs guideline process. Some pharma companies  notably Pfizer  demand FDA develop new legally-binding REGULATIONS regarding social media. The problem is that guidelines are not legally binding, so a new FDA administration can ignore guidelines written in the past. Q. Pre-moderation or post-moderation for pharma social media sites? What do you prefer or recommend? A. I prefer pre-moderation although it may inhibit social media conversation. In my view, pharma will never have real conversations via social media, so why worry about inhibiting it? Pfizer pre-moderates in Getting Old site and seems to be doing well  although it enhances the conversation by working with patient organization partners who submit most of the content. Q. What advice would you give to a pharma company that is determined to invest their efforts on Social Media and wants to do the right thing? A. Invite me in to discuss Pharmas Social Media Trials Tribulations to learn about the issues and how to avoid making mistakes. ]] John Mack, conocido popularmente como Pharmaguy (@ pharmaguy ), es el editor de Pharma Marketing News , un boletin electronico mensual independiente que cubre temas de especial relevancia para los ejecutivos de marketing de la industria farmaceutica. John, quien ademas es un prolifico bloguero , presento en Barcelona su ponencia  Pruebas y Tribulaciones de la Industria Farmaceutica en los Medios Sociales , en la que repasa los hitos mas importantes que han tenido lugar durante los ultimos seis anos asi como los principales problemas o fracasos. A continuacion transcribo el contenido integro de la entrevista que le hice con motivo de la mencionada presentacion. La version en castellano se puede leer en elglobal.net . Q. In your presentation your first milestone dates back to April 2006. Much has happened since. What's your assessment of these past years in Social Media and Pharma? A. I started my Pharma Social Media Timeline in 2005/6 because that is when the modern era of social media began with Blogger, for example. I started blogging in January 2005. This was the beginning of what used to be called the Web 2.0 era, where social media tools like Blogger were offered to ordinary users to create content and communities. The pharmaceutical industry was, in some respects, an early adopter of modern social media. But you really have to look at individual pharma companies to assess how sectors of the industry are using social media. GlaxoSmithKline, for example, has truly been an early adopter of the spirit of modern social media I recall that it sponsored an online smoking cessation forum on an a third-party patient site before 2000. But that was a Web 1.0 experience although there was true community amongst forum members at the time. Aside from viewing the pharma industry as a monolithic use of social media, the other problem is defining exactly what aspect of social media should we be looking at. I think the most important aspect is the direct conversation between pharma and its audience via social media such as Twitter, Facebook, and blogs. Some pharma companies are better than others at engaging in this conversation, but the industry as a whole have made great progress. Many companies now routinely handle customer complaints via Twitter and some have Facebook pages or their own sites that allow comments and conversation from and to consumers. Q. What should be the role of pharma companies in relation to Social Media? A. There are many ways that pharma companies can use social media. From the point of view of patients, pharma can use social media to support their products  ie, help patients report AEs [adverse events]. Of course, there is a problem in drawing the line between support and product promotion, which is not allowed in the EU. Q. What are the main mistakes that have been made by pharma companies? A. Thats difficult to say  there have been many kinds of mistakes. Hopefully, the industry has learned from these mistakes. Probably the biggest mistake a company can make when initiating a social media campaign is not to be prepared for crises that may arise. Companies must have robust social media standard operating procedures to cover all contingencies. Q. What have they done correctly? Which companies in your opinion are doing things right? A. Many companies are doing some things right and most are doing some things wrong. So I cannot single out one company that has done everything right, IMHO, with regard to social media. Boehringer Ingelheim has been noteworthy in my view of doing some things really right (eg, engaging consumers/patients via Twitter) and some things really wrong ( humorous YouTube video ). Pfizer is another example. The most recent social media campaign that Pfizer is doing right is the Getting Old campaign . Q. FDA promised guidelines by the end of 2010" and so far they have only issued a draft that focuses exclusively on social media discussion of off-label medication use. What are your expectations for the near future? A. A little-noticed "Miscellaneous Provision" of the " Food and Drug Administration Safety and Innovation Act " (aka PDUFA), which was signed into law by president Obama on July 10, 2012, simply states that not later than 2 years after the date of enactment of this Act, the Secretary of Health and Human Services shall issue guidance that describes Food and Drug Administration policy regarding the promotion, using the Internet (including social media), of medical products that are regulated by such Administration. This, however, may have more bark than bite. It seems that the FDA will dole out social media guidelines as part of other guidelines  as they did with guideline for responding to off-label questions. Yet the FDA has not come up with any guidelines covering topics discussed at the November 2009 public hearing. Its difficult to be optimistic about this given all the other things on FDAs plate. Plus, there are a few legal, first amendment challenges that pharma has raised AGAINST FDAs guideline process. Some pharma companies  notably Pfizer  demand FDA develop new legally-binding REGULATIONS regarding social media. The problem is that guidelines are not legally binding, so a new FDA administration can ignore guidelines written in the past. Q. Pre-moderation or post-moderation for pharma social media sites? What do you prefer or recommend? A. I prefer pre-moderation although it may inhibit social media conversation. In my view, pharma will never have real conversations via social media, so why worry about inhibiting it? Pfizer pre-moderates in Getting Old site and seems to be doing well  although it enhances the conversation by working with patient organization partners who submit most of the content. Q. What advice would you give to a pharma company that is determined to invest their efforts on Social Media and wants to do the right thing? A. Invite me in to discuss Pharmas Social Media Trials Tribulations to learn about the issues and how to avoid making mistakes. ]]  Predictive Analytics World / Toronto (PAW) has been a great time for connecting with thought leaders and practitioners in the field. Sometimes there are unexpected pleasures as well, which is certainly the case this time. One of the exhibitors for the eMetrics conference, co-locating with PAW at the venue, was Unilytics , a web analytics company. At their booth there was a cylindrical container filled with crumpled dollar bills with a sign soliciting predictions of how many dollar bills were in the container (the winner getting all the dollars). After watching the announcement of the winner, who guessed $352, only $10 off from the actual $362, I thought this would be the perfect opportunity for another Wisdom of Crowds test,just like the one conducted 9 months ago and blogged on here . Two Unilytics employees at the booth, Gary Panchoo and Keith MacDonald, were kind enough to indulge me and my request to compute the average of all the guesses. John Elder was also there, licking his wounds from finished a close second as his guess, $374 was off by $12, a mere $2 away from the winning entry! The results of the analysis are here (summary statistics created by JMP Pro 10 for the mac). In summary, the results are as follows: Dollar Bill Guess Scores Method Guess Value Error Actual 362 Ensemble/Average (N=61) 365 3 Winning Guess (person) 352 10 John Elder 374 12 Guess without outlier (2000), 3rd place 338 24 Median, 19th place 275 87 So once again, the average of the entries (the "Crowds" answer) beat the single best entry. What is fascinating to me about this is not that the average won (though this in of itself isn't terribly surprising), but rather how it won. Summary statistics are below. Note that the Median is 275, far below the mean. Not too how skewed the distribution of guesses are (skew = 3.35). The fact that the guesses are skewed positively for a relatively small answer (362) isn't a surprise, but the amount of skew is a bit surprising to me. What these statistics tell us is that while the mean value of the guesses would have been the winner, a more robust statistic would not, meaning that the skew was critical in obtaining a good guess. Or put another way, people more often than not under-guessed by quite a bit (the median is off by 87). Or put a third way, the outlier (2000) which one might naturally want to discount because it was a crazy guess was instrumental to the average being correct. In the prior post on this from July 2011, I trimmed the guesses, removing the "crazy" ones. So when should we remove the wild guesses and when shouldn't we? (If I had removed the 2000, the "average" still would have finished 3rd). I have no answer to when the guesses are not reasonable, but wasn't inclined to remove the 2000 initially here. Full stats from JMP are below, with the histogram showing the amount of skew that exists in this data. Distribution of Dollar Bill Guesses - Built with JMP Summary Statistics Statistic Guess Value Mean 365 Std Dev 299.80071 Std Err Mean 38.385548 Upper 95% Mean 441.78253 Lower 95% Mean 288.21747 N 61 Skewness 3.3462476 Median 275 Mode 225 2% Trimmed Mean 331.45614 Interquartile Range 185.5 Note: The mode shown is the smallest of 2 modes with a count of 3. Quantiles Quantile Guess Value 100.0% maximum 2000 99.5% 2000 97.5% 1546.8 90.0% 751.6 75.0% quartile 406.5 50.0% median 275 25.0% quartile 221 10.0% 145.6 2.5% 98.2 0.5% 96 0.0% minimum 96 ]] Ruben's comment that referred to spam reminded me of an old Dilbert comic which conveys the misconception about database marketing (e-marketing) and spam. I know Ruben well and know he was poking fun, though I still have to correct folks who after finding out I do "data mining" actually comment that I'm responsible for spam. Answer: "No, I'm the reason you don't get as much spam!"]] How Companies Learn Your Secrets" by Charles Duhigg with the key descriptions of Target, pregnancy, predictive analytics (blogged on here and here ) certainly generated a lot of buzz; if you are unable to see the NYTimes Magazine article, the Forbes summary is a good summary. However, few know that Eric Siegel booked Andy Pole for the October 2010 Predictive Analytics World conference as a keynote speaker. The full video of that talk is here . In this talk, Mr. Pole discussed how Target was using Predictive Analytics including descriptions of using potential value models, coupon models, and...yes...predicting when a woman is due (if you aren't the patient type, it is at about 34:30 in the video). These models were very profitable at Target, adding an additional 30% to the number of woman suspected or known to be pregnant over those already known to be (via self-disclosure or baby registries). The fact that this went on for over a year after the Predictive Analytics World talk and before the fallout tells me that it didn't cause significant problems for Target prior to the attention brought to the subject related to the NYTimes article. After watching the talk, what struck me most was that Target was applying a true "360 deg" customer view by linking guests through store visits, web, email, and mobile interactions. In addition, close attention was paid to linking the interactions so that coupons made sense: they didn't print coupons to those who had just purchased items they score high for couponing, and they identify which mediums don't generate responses and stop using those channels. I suspect what Target is doing is no different than most retailers, but this talk was an interesting glimpse into how much they value the different channels and try to find ways to provide customers with what they are most interested in, and suppress what they are not interested in.]] John Elder and I worked at the company Barron Associates, Inc. (BAI) in Charlottesville, VA in the 80s (John was employee #1). Hopefully you can identify John in the back right and me in the front right, though it will take some good pattern matching to do so! The Founder and President of the company was Roger Barron. Both John and I were introduced to statistical learning methods at BAI and of course went on to careers in the field now known as Data Mining or Predictive Analytics (among other things). I write about my experience with BAI in the forthcoming book Journeys to Data Mining: Experiences from 15 Renowned Researchers , Ed. by Dr Mohamed Medhat Gaber, published by Springer. Authors in the book include John (thanks to John for recommending my inclusion in the book), Gregory Piatetsky-Shapiro, Mohammed J. Zaki, and of course several others. The photo appeared as I was searching for descriptions of our field back in the 80s and was looking in particular for the Barron and Barron paper " Statistical Learning Networks: A Unifying View ", where "Statistical Learning Networks" was the phrase of interesting, along with "Models", "Classifiers", and "Neural Networks". We used to refer to the field as "Pattern Recognition" and "Artificial Intelligence". It's interesting to note that pattern recognition on Wikipedia contains a list of "See Also" terms that includes the more modern terms such as data mining and predictive analytics. I will post within a couple days on the pattern recognition terms of the day and how they are changing.]] "Police using predictive analytics to prevent crimes before they happen", published by Agence France-Presse on The Raw Story (Jul-29-2012). Setting aside obvious civil liberties questions, consider the application of this technology.  My suspicion is that targeting police efforts by geographic locale and day-of-week/time-of-day using this approach will decrease the overall level of crime, but by how much is not clear. This is typical of problems faced by businesses: It is not enough to predict what we already know, nor is it enough to trot out glowing but artificial technical measures of performance.  Knowledge that real improvement has occurred requires more.  For instance, at least some effect of police effort on the street does not decrease crime, but merely moves it to new locations. Were I mayor of a small town approached by the vendor of such a solution, I'd want to see some sort of experimental design which made apples-to-apples comparison between our best estimates of what happens with the new tool, and what happens without it.  Only once this firm measure of benefit has been obtained could one reasonably weigh it against the financial and political costs. ]] As an example, a common problem in banking is to predict future balances of a loan customer.  The current balance is a matter of record and a host of explanatory variables (previous payment history, delinquency history, etc.) are available for model construction.  It is easy to move forward with such a project without considering carefully whether the raw target variable is the best choice for the model to approximate.  It may be, for instance, that it is easier to predict the logarithm of balance, due to a strongly skewed distribution.  Or, it might be that it is easier to predict the ratio of future balances to the current balance.  These two alternatives result in models whose output are easily transformed back into the original terms (by exponentiation or multiplication by the current balance, respectively).  More sophisticated targets may be designed to stabilize other aspects of the behavior being studied, and certain other loose ends may be cleaned up as well, for instance when the minimum or maximum target values are constrained. When considering various possible targets, it helps to keep in mind that the idea is to stabilize behavior, so that as many observations as possible align in the solution space.  If retail sales include a regular variation, such as by day of the week or month of the year, then that might be a good candidate for normalization: Possibly we want to model retail sales divided by the average for that day of the week, or retail sales divided by a trailing average for that day of the week for the past 4 weeks.  Some problems lend themselves to decomposition, such as profit being modeled by predicting revenue and cost separately.  One challenge to using multiple models in series this way is that their (presumably independent) errors will compound. Experience indicates that it is difficult in practice to tell which technique will work best in any given situation without experimenting, but performance gains are potentially quite high for making this sort of effort.]] What Makes Paris Look like Paris?, attempts to classify images of street scenes according to their city of origin.  This is a fairly typical supervised machine learning project, but the source of the data is of interest.  The authors obtained a large number of Google Street View images, along with the names of the cities they came from.  Increasingly, large volumes of interesting data are being made available via the Internet, free of charge or at little cost.  Indeed, I published an article about classifying individual pixels within images as "foliage" or "not foliage", using information I obtained using on-line searches for things like "grass", "leaves", "forest" and so forth. A bewildering array of data have been put on the Internet.  Much of this data is what you'd expect: financial quotes, government statistics, weather measurements and the like- large tables of numeric information.  However, there is a great deal of other information: 24/7 Web cam feeds which are live for years, news reports, social media spew and so on.  Additionally, much of the data for which people once charged serious bucks is now free or rather inexpensive.  Already, many firms augment the data they've paid for with free databases on the Web.  An enormous opportunity is opening up for creative data miners to consume and profit from large, often non-traditional, non-numeric data which are freely available to all, but (so far) creatively analyzed by few. ]] this means less time spent on that .  I suspect that many modelers enjoy the actual modeling part of the job most.  It is easy to try "one more" algorithm: Already tried logistic regression and a neural network?  Try CART next. Of course, more time spent on the modeling part of this means less time spent on other things.  An important consideration for optimizing model performance, then, is: Which tasks deserve more time, and which less? Experimenting with modeling algorithms at the end of a project will no doubt produce some improvements, and it is not argued here that such efforts be dropped.  However, work done earlier in the project establishes an upper limit on model performance.  I suggest emphasizing data clean-up (especially missing value imputation) and creative design of new features (ratios of raw features, etc.) as being much more likely to make the model's job easier and produce better performance. Consider how difficult it is for a simple 2-input model to discern "healthy" versus "unhealthy" when provided the input variables height and weight alone.  Such a model must establish a dividing line between healthy and unhealthy weights separately for each height.  When the analyst uses instead the ratio of weight to height, this becomes much simpler.  Note that the commonly used BMI (body mass index) is slightly more complicated than this, and would likely perform even better.  Crossing categorical variables is another way to simplify the problem for the model.  Though we deal with a process we call "machine learning", is is a pragmatic matter to make the job as easy as possible for the machine. The same is true for handling missing values.  Simple global substitution using the non-missing mean or median is a start, but think about the spike that creates in the variable's distribution.  Doing this over multiple variables creates a number of strange artifacts in the multivariate distribution.  Spending the time and energy to fill in those missing values in a smarter way (possibly by building a small model) cleans up the data dramatically for the downstream modeling process. ]] Data Mining Pattern Recognition Machine Learning Predictive Analytics Business Analytics The big winner? Big Data of course! It has exploded this year. Will that trend continue? It's hard to believe it will continue, but this wave has grown and it seems that every conference related to analytics or databases is touting "big data". Big Data Data Science I have no plans of calling what I do "big data" or "data science". The former term will pass when data gets bigger than big data. The latter may or may not stick, but seems to resonate more with theoreticians and leading-edge types than with practitioners. For now, I'll continue to call myself a data miner and what I do predictive analytics or data mining. ]] In recent weeks I've been reminded how important it is to know your records. I've heard this described in many ways, four of which are: the unit of analysis the level of aggregation what a record represents unique description of a record For example, does each record represent a customer? If so, over their entire history or over a time period of interest? In web analytics, the time period of interest may be a single session, which if it is true, means that an individual customer may be in the modeling data multiple times as if each visit or session is an independent event. Where this especially matters is when disparate data sources are combined. If one is joining a table of customerID/Session data with another table with each record representing a customerID, there's no problem. But if the second table represents customerID/store visit data, there will obviously be a many-to-many join resulting in a big mess. This is probably obvious to most readers of this blog. What isn't always obvious is when our assumptions about the data result in unexpected results. What if we expect the unit of analysis to be customerID/Session but there are duplicates in the data? Or what if we had assumed customerID/Session data but it was in actuality customerID/Day data (where ones customers typically have one session per day, but could have a dozen)? The answer is just like we need to perform a data audit to identify potential problems with fields in the data, we need to perform record audits to uncover unexpected record-level anomalies. We've all had those data sources where the DBA swears up and down that there are no dups in the data, but when we group by customerID/Session, we find 1000 dups. So before the joins and after joins, we need to do those group by operations to find examples with unexpected numbers of matches. In conclusion: know what your records are supposed to represent, and verify verify verify. Otherwise, your models (who have no common sense) will exploit these issues in undesirable ways! ]] per se, it the interpretation of the statistical test. Yesterday I tweeted (@deanabb) this fun factoid: "Redskins predict Romney wins POTUS #overfit. if Redskins lose home game before election = challenger wins (17/18) http://www.usatoday.com/story/gameon/2012/11/04/nfl-redskins-rule-romney/1681023/" I frankly had never heard of this "rule" before and found it quite striking. It even has its own Wikipedia page (http://en.wikipedia.org/wiki/Redskins_Rule). For those of us in the predictive analytics or data mining community, and those of us who use statistical tests to help out interpreting small data, 17/18 we know is a hugely significant finding. This can frequently be good: statistical tests will help us gain intuition about value of relationships in data even when they aren't obvious. In this case, an appropriate test is a chi-square test based on the two binary variables (1) did the Redskins win on the Sunday before the general election (call it the input or predictor variable) vs. (2) did the incumbent political party win the general election for President of the United States (POTUS). According to the Redskins Rule, the answer is "yes" in 17 of 18 cases since 1940. Could this be by chance? If we apply the chi-square test to it, it sure does look significant! (chi-square = 14.4, p here): It's great data--9 Redskin wins, 9 Redskin losses, great chi-square statistic! OK, so it's obvious that this is just another spurious correlation in the spirit of all of those fun examples in history, such as the superbowl winning conference predicting if the stock market would go up or down in the next year at a stunning 20 or 22 correct. It even was the subject of academic papers on the subject! The broader question (and concern) for predictive modelers is this: how do we recognize when we have uncovered spurious correlations in the data that are merely spurious? This can happen especially when we don't have deep domain knowledge and therefore wouldn't necessarily identify variables or interactions as spurious. In examples such as the election or stock market predictions, no amount of "hold out" samples, cross-validation or bootstrap sampling would uncover the problem: it is in the data itself. We need to think about this because inductive learning techniques search through hundreds, thousands, even millions of variables and combinations of variables. The phenomenon of "over searching" is a real danger with inductive algorithms as they search and search for patterns in the input space. Jensen and Cohen have a very nice and readable paper on this topic (PDF here ). For trees, they recommend using the Bonferroni adjustment which does help penalize the combinatorics associated with splits. But our problem here goes far deeper than overfitting due to combinatorics. Of course the root problem with all of these spurious correlations is small data. Even if we have lots of data, what I'll call here the "illusion of big data", some algorithms make decisions based on smaller populations, like decision trees, rule induction and nearest neighbor (i.e., algorithms that build bottom-up). Anytime decisions are made from populations of 15, 20, 30 or even 50 examples, there is a danger that our search through hundreds of variables will turn out a spurious relationship. What do we do about this? First, make sure you have enough data so that these small-data effects don't bite you. This is why I strongly recommend doing data audits and looking for categorical variables that contain levels with at most dozens of examples--these are potential overfilling categories. Second, don't hold strongly any patterns discovered in your data based on solely on the data, especially if they are based on relatively small sample sizes. These must be validated with domain experts. Decision trees are notorious for allowing splits deep in the trees that are "statistically significant" but dangerous nevertheless because of small data sizes. Third, the gist of your models have to make sense. If they don't, put on your "Freakonomics" hat and dig in to understand why the patterns were detected by the models. In our Redskin Rule, clearly this doesn't make sense causally, but sometimes the pattern picked up by the algorithm is just a surrogate for a real relationship. Nevertheless, I'm still curious to see if the Redskin Rule will prove to be correct once again. This year it predicts a Romney win because the Redskins lost and therefore the incumbent party (D) by the rule should lose. UPDATE: by way of comparison...the chances of having 17/18 or 18/18 coin flips turn up heads (or tails--we're assuming a fair coin after all!) is 7 in 100,000 or 1 in 14,000. Put another way, if we examined 14K candidate variables unrelated to POTUS trends, the chances are that one of them would line up 17/18 or 18/18 of the time. Unusual? Yes. Impossible? No! ]] 1. The data miner has little or no programming skill. Most work environments require someone to extract and prepare the data.  The more of this process which the data miner can accomplish, the less her dependence on others.  Even in ideal situations with prepared analytical data tables, the data miner who can program can wring more from the data than her counterpart who cannot (think: data transformations, re-coding, etc.).  Likewise, when her predictive model is to be deployed in a production system, it helps if the data miner can provide code as near to finished as possible. 2. The data miner is unable to communicate effectively with non-data miners. Life is not all statistics: Data mining results must be communicated to colleagues with little or no background in math.  If other people do not understand the analysis, they will not appreciate its significance and are unlikely to act on it.  The data miner who can express himself clearly to a variety of audiences (internal customers, management, regulators, the press, etc.) is of greater value to the organization than his counterpart who cannot.  The data miner should should receive questions eagerly. 3. The data miner never does anything new. If the data miner always approaches new problems with the same solution, something is wrong.  She should be, at least occasionally, suggesting new techniques or ways of looking at problems.  This does not require that new ideas be fancy: Much useful work can be done with basic summary statistics.  It is the way they are applied that matters. 4. The data miner cannot explain what they've done. Data mining is a subtle craft: there are many pitfalls and important aspects of statistics and probability are counter-intuitive.  Nonetheless, the data miner who cannot provide at least a glimpse into the specifics of what they've done and why, is not doing all he might for the organization.  Managers want to understand why so many observations are needed for analysis (after all, they pay for those observations), and the data miner should be able to provide some justification for his decisions. 5. The data miner does not establish the practical benefit of his work. A data miner who cannot connect the numbers to reality is working in a vacuum and is not helping her manager (team, company, etc.) to assess or utilize her work product.  Likewise, there's a good chance that she is pursuing technical targets rather than practical ones.  Improving p-values, accuracy, AUC, etc. may or may not improve profit (retention, market share, etc.). 6. The data miner never challenges you. The data miner has a unique view of the organization and its environment.  The data miner works on a landscape of data which few of his coworkers ever see, and he is less likely to be blinded by industry prejudices.  It is improbable that he will agree with his colleagues 100% of the time.  If the data miner never challenges assumptions (business practices, conclusions, etc.), then something is wrong. ]] 0 0 1 1195 6812 Abbott Analytics 56 15 7992 14.0 We all know that given reasonable data, a good predictive modeler can build a model that works well and helps make makes better decisions than what is currently used in your organization (at least in our own minds). Newer data, sophisticated algorithms, and a seasoned analyst are all working in our favor when we build these models, and if success were measured by accuracy (as they are in most data mining competitions), we're in great shape. Yes, there are always gotchas and glitches along the way. But when my deliverable is only slideware, even of the modeling is hard, I'm confident of being able to declare victory at the end. However, the reality is that there is much more to the transition from cool model to actual deployment than a nice slide deck and paper accepted at one's favorite predictive analytics, data mining or big data conference. In these venues, the winning models are those that are "accurate" (more on that later) and have used creative analysis techniques to find the solution; we won't submit a paper when we only had to press the "go" button and have the data mining software give us a great solution! For me, the gold standard is deployment. If the model gets used and improves the decisions an organization makes, I've succeeded. Three ways to increase the likelihood your models are deployed are: 1) Make sure the model stakeholder designs deployment into the project from the beginning The model stakeholder is the individual, usually a manager, who is the advocate of predictive models to decision-makers. It is possible that a senior-level modeler can do this task, but that person must be able to switch hit: he or she must be able to speak the language of management and be able to talk technical detail to analytics. This may require more than one trusted person: the manager, who is responsible and makes the ultimate decisions about the models, and the lead modeler, who is responsible for the technical aspects of the model. It is more than "talking the talk" and knowing buzz-words in both realms; the person or persons must truly be "one of" both groups. For those who have followed my blog posts and conference talks, you know I am a big advocate of the CRISP-DM process model (or equivalent methodologies, which seem to be endless). I've referred to CRISP-DM often, including on topics related to what data miners need to learn and Defining the Target Variable , just as two examples. The stakeholder must not only understand the business of objectives of the model (Business Understanding in CRISP-DM), but must be present during discussions take place related to which models will be built. It is essential that reasonable expectations are put into place from the beginning, including what a good model will "look like" (accuracy and interpretability) and how the final model will be deployed. I've seen far too many projects die or become inconsequential because either the wrong objectives were used in building the models, meaning the models were operationally useless, or because the deployment of the models was not considered, meaning again that the models were operationally useless. As an example, on one project, the model was assumed to be able to be run within a rules engine, but the models that were built were not rules at all, but were complex non-linear models that could not be translated into rules. The problem obviously could have been avoided had this disconnect been verbalized early in the modeling process. 2) Make sure modelers understand the purpose of the models The modelers must know how the models will be used and what metrics should be used to judge model performance. A good summary of typical error metrics used by modelers is found here . However, for most of the models I have deployed in customer acquisition, retention, and risk modeling, the treatment based on the model is never applied to the entire population (we don't mail everyone, just a subset). So the metrics that make the most sense are often ones like "lift after the top decile", maximum cumulative net revenue, top 1000 scores to be investigated, etc. I've actually seen negative correlations between the ranking of models based on global metrics (like classification error or R^2) vs. the ranking based on subset selection ranking, such as top 1000 scores; very different models may be deployed depending on the metric one uses to assess them. If modelers aren't aware of the metric to be used, the wrong model can be selected, even one that does worse than the current approach. Second, if the modelers don't understand how the models will be deployed operationally, they may find a fantastic model, one that maximizes the right metric, but is useless. The Neflix Prize is a great example : the final winning model was accurate but far too complex to be used. Netflix extracted key pieces to the models to operationalize instead. I've had customers stipulate to me that "no more than 10 variables can be included in the final model". If modelers aren't aware of specific timelines or implementation constraints, a great but useless model can be the result. 3) Make sure the model stakeholder understands what the models can and can't do In the effort to get models deployed, I've seen models elevated to a status they don't deserve, most often by exaggerating their accuracy and expected performance once in operation. I understand why modelers may do this: they have a direct stake in what they did. But the manager must be more skeptical and conservative. One of the most successful colleagues I've ever worked with used to assess model performance on held-out data using the metric we had been given (maximum depth one could mail to and still achieve the pre-determined response rate). But then he always backed off what was reported to his managers by about 10% to give some wiggle room. Why? Because even in our best efforts, there is still a danger that the data environment after the model is deployed will differ from that used in building the models, thus reducing the effectiveness of the models. A second problem for the model stakeholder is communicating an interpretation of the models to decision-makers. I've had to do this exercise several times in the past few months and it is always eye-opening when I try to explain the patterns a model is finding when the model is itself complex. We can describe overall trends ("on average", more of X increases the model score) and we can also describe specific patterns (when observable fields X and Y are both high, the model score is high). Both are needed to communicate what the models do, but have to connect with what a decision-maker understands about the problem. If it doesn't make sense, the model won't be used. If it is too obvious, the model isn't worth being used. The ideal model for me is one where the decision-maker nods knowingly at the "on average" effects (these should usually be obvious). Then, once you throw in some specific patterns, he or she should scrunch his/her eyes, think a bit, then smile as the implications of the pattern dawns on them as that pattern really does make sense (but was previously not considered). As predictive modelers, we know that absolutes are hard to come by, so even if these three principles are adhered to, other factors can sabotage the deployment of a model. Nevertheless, in general, these steps will increase the likelihood that models are deployed. In all three steps, communication is the key to ensuring the model built addresses the right business objective, the right scoring metric, and can be deployed operationally. NOTE: this post was originally posted for the Predictive Analytics Times at http://www.predictiveanalyticsworld.com/patimes/january13/ ]] For posted in 2012, in order of popularity: Target, Pregnancy, and Predictive Analytics, Part I Target, Pregnancy, and Predictive Analytics, Part II Predictive Analytics World Had the Target Story First Why Defining the Target Variable in Predictive Analytics is Critical Dilbert, Database Marketing, and Spam Im also adding #6 because Will post in December did very well, but of course has had only one month to accumulate views. 6 Reasons You Hired the Wrong Data Miner From posts prior to 2012, in order of popularity for 2012: What Do Data Miners Need to Learn (June 2011) Free and Inexpensive Data Mining Software (November 2006) This post needs to be updated! Why Normalization Matters for K Means (April 2009) It always amazes me why this post persists as one of the most popular, but nearly 14 of the visits used the search term K Means Noisy Data Data Mining Data Sets (April 2008) This post also needs to be updated Business Analytics vs. Business Intelligence (December 2009) One final note: When I look back at visits since the start of this blog, 4 of the top 5 posts are the top 4 prior to 2012 above. The #5 most popular post over all the years Ive had the blog is one by Will from 2007,  Missing Values and Special Values: The Plague of Data Analyis , one that I have always liked very much. Best to all of you in 2013! ]] So, how might one use geographic data? Possible answers depend on several factors, most importantly the volume and type of such data. A company serving a national market in the United States, for instance, will have customer shipping and billing addresses (not necessarily the same thing) for each customer (possibly for each transaction). These addresses normally come with a range of spatial granularities: street address, town, state, and associated ZIP Code (a 5-digit postal code). Even at the largest level of aggregation, the state level, there may be over 50 distinct values (besides the 50 states, American addresses may be in Washington D.C. [technically not part of any state], or any of a number of other American territories, the most common of which is probably Puerto Rico). With 50 or so distinct values, significant data volume is needed to amass the observations needed to draw conclusions about each value. Given the best case scenario, in which all states exhibit equal observation counts, 1,000 observations breaks out into 50 categories of merely 20 observations each- not even enough to satisfy the old statistician's 30 observation rule of thumb. In data mining circles, we are accustomed to having much larger observation counts, but consider that the distribution of state values is never uniform in real data. Using individual dummy variables to represent each state may be possible with especially large volumes.  Possibly an "other" category covering the least frequent so many states will be needed. Another technique which I have found to work well is to replace the categorical state variable with a numeric variable representing a summary of the target variable, conditioned by state. In other words, all instances of "Virginia" are replaced by the average of the target variable for all Virginia cases, all instances of "New Jersey" are replaced by the average of the target variable for all New Jersey cases, and so on. This solution concentrates information about the target which comes from the state in a single variable, but makes interactions with other predictors more opaque. Ideally, such summaries are calculated on special hold-out set of data, used just for this purpose, so as to avoid over-fitting. Again, it may be necessary to lump the smallest so many states together as "other". While I have used American states in my example, it should not be hard for the reader to extend this idea to Canadian provinces, French departements, etc. Most American states are large enough to provide robust summaries, but as a group they may not provide enough differentiation in the target variable. Changing the spatial scale implies a trade-off: Smaller geographic units exhibit worse summary variance, but improved geographic differentiation. American town names are not necessarily unique within a given state and similar names may be confused (Newtown, Pennsylvania is quite a distance from Newtown Square, Pennsylvania, for instance). In the United States, county names are unambiguous, and present finer spatial detail than states. County names do not, however, normally appear in addresses, but they are easily attached using ZIP Code/County tables easily found on-line. Another possible aggregation is the Section Code Facility, or "SCF", which is the first 3 digits of the ZIP Code. In the American market, other types of spatial definitions which can be used include: Census Bureau definitions, telephone area codes and Metropolitan Statistical Areas ("MSAs") and related groupings defined by the U.S. Office of Management and Budget. The Census Bureau is a government agency which divides the entire country in to spatial units which vary in scale, down to very small areas (much smaller than ZIP Codes). MSAs are very popular with marketers. There are 366 MSAs at present, and they do not cover the entire land area of the United States, though they do cover about 85% of its population. It is important to note that nearly all geographic entities change in size, shape and character over time. While existing American state and county boundaries almost never change any more, ZIP code boundaries and Census Bureau definitions, for instance, do change. Changing boundaries obviously complicates analysis, even though historic boundary definitions are often available. Even among entities whose boundaries do not change, radical changes in behavior may happen in geographically distinct ways. Consider that a model built before hurricane Katrina may no longer perform well in areas affected by the storm. Also note that some geographic units, by definition, "respect" other definitions. American counties, for instance, only contain land from a single state. Others don't: the third-most populous MSA, Chicago-Joliet-Naperville, IL-IN-WI, for example, overlaps three different states. Being creative when defining model inputs can be as helpful with geographic data as it is with more conventional data. In addition to the billing address itself, consider transformations such as: Has the billing address ever changed (1) or not (0)? How many times has the billing address changed? How often has the billing address changed (number of times changed divided by number of months the account has been open)? How far is the shipping address from the billing address? And so on... Much more sophisticated use may be made of geographic data than has been described in this short posting. Software is available commerically which will determine drive time contours about locations, which would be useful, for instance when modeling retail store location revenue models. Additionally, there is an entire of statistics, called spatial statistics , which defines an entire class of analysis procedures specific to this sort of thing. I encourage readers who have avoided geographic data to consider even simple mechanisms to include it in model construction. Opening up a new dimension in your analysis may provide significant returns. ]] Predictive Analytics World (PAW)? There are many reasons conferences are valuable including interacting with thought leaders and practitioners, seeing software and hardware tools (the exhibit hall), and learning principles of predictive analytics from talks and workshops. This post focuses on the talks, and in particular, case studies. There is no quicker way to upgrade our capabilities than having someone else who has "been there" tell us how they succeeded in their development and implementation of predictive models. When I go to conferences, this is at the top of my list. In the best case studies I am able to see different way of looking at a problem than I had considered before, how the practitioner overcame obstacles, how their target variable was defined, what data was used in building the models, how the data was prepared, what figure of merit they used to judge a model's effectiveness, and much more. Almost all case studies we see at conferences are success stories; we all love winners. Yes, we all know that we learn from mistakes and many case studies actually enumerate mistakes. But success sells and given time limitations in a 20-50 minute talk, few mistakes and dead-ends are usually described in the talks. And, as we used to say in when I was doing government contracting, one works like crazy on the research and then when the money runs out, one declares victory. Putting a more positive spin on the process, we do as well as we can with the resources we have, and if the final solution improves the current system, we are indeed successful. But once we observe the successful approach, what can we really take home with us? There are three reasons we should be skeptical taking case studies and applying them directly to our own problems. The first two reasons are straightforward. First, our data is different from the data used in the talk. Obviously. But it is likely to be different enough that one cannot not take the exact same approach to data preparation or target variable creation that one sees at a conference. Second, our business is different. The way the question was framed and the way predictions can be used are likely to differ in our organization. If we are building models to predict Medicare fraud, they way the suspicious claim is processed and which data elements are available vary significantly for each provider (codes being just one example). The third reason is more subtle and more difficult to overcome. In a fascinating New Yorker article entitled, "The Truth Wears Off: Is there something wrong with the scientific method?" , author Jonah Lehrer describes an effect seen by many researchers over the past few decades. Findings in major studies, published in reputable journals, and showing statistically significant results have been difficult to replicate by the original researcher and by others. This is a huge problem because replicating results is what we do as predictive modeler: we assume that behavior in the past can and will be replicated in the future. In one example, researcher Jonathan Schooler (who was originally at the University of Washington as a graduate student) demonstrated that subjects shown a face and asked to describe it were much less likely to recognize the face when shown it later than those who had simply looked at it. Schooler called the phenomenon verbal overshadowing. The study turned him into an academic star." A few years later, he tried to replicate the study didnt succeed. In fact, he tried many times over the years and never succeeded. The effect he found at first waned each time he tried to replicate the study with additional data. "This was profoundly frustrating. It was as if nature gave me this great result and then tried to take it back. There have been a variety of potential explanations for the effect, including regression to the mean. This might very well be the case because even when we show statistically significant results defined by having a p value less than 0.05, there is still a chance that the effect found was not really there at all. Over thousands of studies, dozens find effects therefore that aren't really there. Let's assume we are building models and there is actually no significant difference between responders and non-responders (but we don't know that). However, we work very hard to identify an effect, and eventually we find the effect on training and testing data. We publish. But the effect isn't there; we happened upon the effect just had good luck (which in the long run is actually bad luck!). Even if the chance of finding the effect by chance is 1 in 100, or 1 in 1000, if we experiment enough and search through enough variables, we may happen upon a seemingly good effect eventually. This process, called "over searching" by Jensen and Cohen (see " Multiple Comparisons in Induction Algorithms "), is a real danger. So what do we do at conferences? We should take home ideas, principles, and approaches rather than recipes. It should spur us to try ideas we either hadn't yet tried or even thought about before. (An earlier version of this post was first published in the Predictive Analytics Times February 2013 issue)]] To understand when data mining is not applicable, it will be helpful to define precisely when it is applicable. Data mining (inferential statistics, predictive analytics, etc.) requires data stored in a machine format of sufficient volume, quality and relevance so as to permit the construction of predictive models which assist in real-world decision making. Most of our time as data miners is spent worrying over the quality of the data and the process of turning data into models, however it is important to realize the usual context of data mining. Most organizations can perform basic decision making competently, and they have done so for thousands of years. Whether the base decision process is human judgment, a simple set of rules or a spreadsheet, much performance potential is already realized before data mining is applied. Consultants' marketing notwithstanding, data mining typically inhabits the margin of performance, where it tries to bring an extra "edge". So, if the above two paragraphs describe conditions conducive to data mining success, what sorts of real-world situations defy data mining? The most obvious would be problems featuring data that is too small, too narrow, too noisy or of too little relevance to allow effective modeling. Organizations which have not maintained good records, which still rely on non-computer procedures and those with too little history are good examples. Even within very large organizations which collect and store enormous databases, there may be no relevant data for the problem at hand (for instance, when a new line of business is being opened, or new products introduced). It is surprising how often business people expect to extract value from a situation when they have failed to invest in appropriate data gathering. Another large area with minimal data mining potential is organizations whose basic business process is so fundamentally broken that the usual decision making procedures have failed to do the usual "heavy lifting". Any of us can easily recall experiences in retail establishments whose operation was so flawed that it was obvious that the profit potential was not nearly being exploited. Data mining cannot fine tune a process which is so far gone. No amount of quantitative analysis will fix unkept shelves, weak product offering or poor employee behavior.]] March 2013 Edition of the Predictive Analytics Times) Predictive analytics is just a bunch of math, isnt it? After all, algorithms in the form of matrix algebra, summations, integrals, multiplies and adds are the core of what predictive modeling algorithms do. Even rule-based approaches need math to compute how good the if-then-else rules are. I was participating in a predictive analytics course recently and the question a participant asked at the end of two days of instruction was this: its been a long time since Ive had to do this kind of math and Im a bit rusty. Is there a book that would help me learn the techniques without the math? The question about math was interesting. But do we need to know the math to build models well? Anyone can build a bad model, but to build a good model, dont we need to know what the algorithms are doing? The answer, of course, depends on the role of the analyst. I contend, however, that for most predictive analytics projects, the answer is no. Lets consider building decision tree models. What options does one need to set to build good trees? Here is a short list of common knobs that can be set by most predictive analytics software packages: 1. Splitting metric (CART style trees, C5 style trees, CHAID style trees, etc.) 2. Terminal node minimum size 3. Parent node minimum size 4. Maximum tree depth 5. Pruning options (standard error, Chi-square test p-value threshold, etc.) The most mathematical of these knobs is the splitting metric. CART-styled trees use the Gini Index, C5 trees use Entropy (information gain), and CHAID style trees use the chi-square test as the splitting criterion. A book I consider the best technical book on data mining and statistical learning methods, The Elements of Statistical Learning, has this description of the splitting criteria for decision trees, including the Gini Index and Entropy: To a mathematician, these make sense. But without a mathematics background, these equations will be at best opaque and at worst incomprehensible. (And these are not very complicated. Technical textbooks and papers describing machine learning algorithms can be quite difficult even for more seasoned, but out-of-practice mathematicians to understand). As someone with a mathematics background and a predictive modeler, I must say that the actual splitting equations almost never matter to me. Gini and Entropy often produce the same splits or at least similar splits. CHAID differs more, especially in how it creates multi-way splits. But even here, the biggest difference for me is not the math, but just that they use different tests for determining "good" splits There are, however, very important reasons for someone on the team to understand the mathematics or at least the way these algorithms work qualitatively. First and foremost, understanding the algorithms helps us uncover why models go wrong. Models can be biased toward splitting on particular variables or even particular records. In some cases, it may appear that the models are performing well but in actuality they are brittle. Understanding the math can help remind us that this may happen and why. The fact that linear regression uses a quadratic cost function tells us that outliers affect overall error disproportionately. Understanding how decision trees measure differences between the parent population and sub-populations informs us why a high-cardinality variable may be showing up at the top of our tree, and why additional penalties may be in order to reduce this bias. Seeing the computation of information gain (derived from Entropy) tells us that binary classification with a small target value proportion (such as having 5% 1s) often won't generate any splits at all. The answer to the question if predictive modelers need to know math is this: no they dont need to understand the mathematical notation, but neither should they ignore the mathematics. Instead, we all need to understand the effects of the mathematics on the algorithms we use. Those who ignore statistics are condemned to reinvent it, warns Bradley Efron of Stanford University. The same applies to mathematics. ]] Supercharging Prediction: Hands-On with Ensemble Models. The workshop was intended to introduce predictive modelers to the concept of ensembles through a combination of lecture to provide an overview of model ensembles and hands-on to gain experience building ensembles using Salford Systems SPM v7.0 (Salford Systems sponsored the workshop). This morning, Heather Hinman, a Marketing Communications Manager at Salford Systems, posted comments on attending that workshop at the Salford Systems blog . Two comments were particularly interesting, especially their implications vis a vis my last blog post on math and predictive analytics: I will admit I was intimidated at first to be participating in a predictive modeling workshop as I do not have a background in statistics, and only have basic training on decision tree tools by Salford Systems' team of in-house experts. Despite my basic knowledge of decision trees, I was thrilled that I was able to follow along with ease and understanding when learning about tree ensembles and modern hybrid modeling approaches. Marketing folk building predictive models? Yes, we can! and Now back at the office in San Diego, along with my usual responsibilities, I feel confident in my ability to build predictive models and gain insights into the data at hand to achieve the email marketing and online campaign goals for our communication efforts! In the post, Heather also outlines some of the principles she learned and how she used them to build the predictive models in the workshop. The point is this: if one uses good software that uses solid principles for building predictive models, and one understands key principles of building predictive models, someone without a mathematics background can build good, profitable models. ]] But what's the key to unlock the big data door? In his interview with Eric Siegel on April 12, Ned Smith of Business News Daily (http://www.businessnewsdaily.com/4326-predictive-analytics-unlocks-big-data.html) starts with this apt insight: "Predictive Analytics is the 'Open Sesame' for the world of Big Data." Big data is what we have; predictive analytics (PA) is what we do with it. Why is the data so big? Where does it come from? We who do PA usually think of doing predictive modeling on structured data pulled from a database, probably flattened into a single modeling table by a query so that the data is loadable into a software tool. We then clean the data, create features, and away we go with predictive modeling. But according to a 2012 IBM study, "Analytics: The real-world use of big data" , 88% of big data comes from transactions, 73% from log data, and significant proportions of data come from audio and video (still and motion). These are not structured data. Log files are often unstructured data containing nothing more than notes, sometimes freehand, sometimes machine-created, and therefore cannot be used without first preprocessing the data using text mining techniques. For all of us who have built models augmented with log files or other text data, we know how much work is involved in transforming text into useful attributes that can then be used in predictive models Even the most structured of the big data sources, transactional data, often are nothing more than dates, IDs and very simple information about the nature of the transaction (an amount, time period, and perhaps a label about the nature of the transaction). Transactional data is rarely used directly; it is usually transformed into a form more useful for predictive modeling. For example, rather than building models where each row is a web page transaction, we transform the data so that each row is a person (the ID) and the fields are aggregations of that persons history for as long as their cookie has persisted; the individual transactions have to be linked together and aggregated to be useful. The big data wave we are experiencing is therefore not helpful directly for improving predictive models, we need to first determine the level of analysis needed to build useful models, i.e., what a record in the model represents. The unit of analysis is determined by the question the model is intended to answer, or put another way, the decision the model is intended to improve within the organization. This is determined by defining the business objectives of the models, normally by a program manager or other domain expert in the organization, and not by the modeler. The second step in building data for predictive modeling is creating the features to include as predictors for the models. How do we determine the features? I see three ways: the analyst can define the features based on his / her experience in the field, or do research to find what others have done in the field through google searching and academic articles. This assumes the analyst is, to some degree, a domain expert. the key features can be determined by other domain experts either handed down to the analyst or through interviews of domain experts by the analyst. This is better than a google search because the answers are focused on the organizations perspective on solving the problem. the analyst can rely on algorithm-based features creation. In this approach, the analyst merely provides the raw input fields and allows the algorithms to find the appropriate transformations of individual fields (easy) or multivariate combinations (more complex). Some algorithms and implementations of algorithms in software can do this quite effectively. This third approach I see advocated implicitly by data scientists in particular. In reality, a combination of all three is usually used and I recommend all three. But features based on domain expertise almost always provides the largest gains in model performance compared with algorithm-based (automatic) feature creation. This is the new thee-legged stool of predictive modeling: big data provides the information, augmenting what we have used in the past, domain experts provide the structure for how to set up the data for modeling, including what a record means and the key attributes that reflect information expected to be helpful to solve the problem, and predictive analytics provides the muscle to open the doors to what is hidden in the data. Those who take advantage of all three will be the winners in operationalizing analytics. First posted at The Predictive Analytics Times ]] Popular Mechanics article, "Why the NSA Wants All That Verizon Metadata" (Jun-06-2013), by Glenn Derene. Since the initial report connecting the NSA with Verizon, details have emerged suggesting similar large-scale information-gathering by the American government from other telecommunication and Internet companies. Some applications of data mining to law enforcement and anti-terrorism problems have clearly been fruitful (for detection of money laundering, for instance, which is one source of funding for criminal and terrorist organizations). On the other hand, direct application of these techniques to plucking out the bad guys from large numbers of innocents strikes this author as dubious, and has long been criticized by experts, such as Bruce Schneier. What's plain is that people in democratic societies must remain vigilant of the balance of information and power granted to their governments, lest the medicine become worse than the disease. ]] The recent leaks about the NSAs use data mining and predictive analytics has certainly raised awareness of our field and has resulted in hours of discussions with family, relatives, friends and reporters about what predictive analytics can (and cant) do with phone records, emails, chat messages, and other structured and unstructured data. Eric Siegel and I have been interviewed on multiple occasions to address this issue from a Predictive Analytics perspective, and in case, in the same article:  What the NSA cant do with your data (probably) . Part of my goal in these conversations has been to bring back to reality many of the inflated expectations of what can be done with predictive analytics: predictive analytics is a powerful approach to finding patterns in data, but it isnt magic, nor is it fool-proof. First, let me be clear: I have no direct knowledge of the analytics the NSA is doing. I have worked on many fraud detection projects for the U.S. Government and private sector, some including what I would describe as a social networking component to them where the connections between parties is an important part of the risk factors. The phone call meta data shows simple information about each phone call: origination, destination, date of the call, duration, and perhaps some geographic information about the origination and destination. One of the valuable aspects of the data is that connections can be made between origination and destination numbers, and as a results, one can build social networks of every origination phone number in the data. The U.S. has more than 326.4 millions cell phones subscriptions as of December 2012 according to CTIA. The Pew Research survey found that individual cell phone users had on average 664 social connections (not all of which are cell connections). The number of links needed to build a U.S.-wide social map of phone call connections easily outstrips any possible visualization method, and therefore, without filtering connections and networks, these social maps would be useless. One of the factors working in our favor, if we are concerned with privacy issues related to this meta data, is therefore the sheer size of the network. The networks of phone calls, I believe, are particularly useful in connecting high-risk individuals with others whom the NSA may not know beforehand are connected to the person of interest. In other words, a starting point is needed first and the social network is built from this starting point. If one has multiple starting points, one can also find linkages between networks even if the networks themselves dont overlap significantly. The strength of a link can include information such as number of calls, duration of calls, regularity of calls, most recent call, oldest call, and more. Think of these as a cell-phone version of RFM analysis. The networks can be pruned easily based on thresholds for these key features, simplifying the networks considerably. But even if the connections are made, this data is woefully incomplete on its own. First, there is no connection to the person who actually made the call, only the phone number and who it is registered to. Finding who made the calls requires more investigation. Second, it doesnt necessarily connect all the phones an individual might use. If a person uses 5 or 6 cell phones, one doesnt know that the same person is behind these phone numbers. Third, one certainly doesnt know the intent or content of the call. Given these limitations, what value is there to the network of calls? These networks are usually best used as lead-generation engines. Which other phone numbers in a network are connected to multiple high-risk individuals (but werent here-to-fore considered high risk)? Is the timeline of calls temporally correlated with other known events? Analytics, and link analysis in particular, provide tremendously powerful techniques to identify new leads and remove unfruitful leads by finding connections unlikely to occur randomly. NOTE: this article first appeared as an article in the PATimes: http://www.predictiveanalyticsworld.com/patimes/the-nsa-link-analysis-and-fraud-detection/ ]] Of the tremendous volume of material written on this subject, nearly all assumes that the analyst knows precisely which items are missing from the data. In reality, this is sometimes not the case. Relational databases and statistical software files, as a rule, have a special value to indicate "missing", though that does not mean that it is always used. Some file formats offer only indirect provision for missings, if any at all, and how software reacts to such missings varies. Consider, too, the popular practice of using special values (such as -9999) to represent missing values. What could possibly go wrong? For one thing, the person writing the data may not consider whether the flag value might represent a legitimate value. Is it possible, for instance, to have an account balance of -9999 dollars (euros, etc.)? In my career, I have seen databases which used different flag values for each field (-99, -9999, -99999, etc.), making the writing of code against such data extremely tedious (and error-prone). I have also seen -9999 used to indicate one type of missing value, and -9998 to indicate another type of missing. When the hand-off of information from one person (system, process, etc.) to another is confused or incomplete, interpretation of the data becomes incorrect. Another aspect of this problem is the precise definition given to fields, and their possible misinterpretation by data consumers (such as data miners). Imagine that a particular integer field is being used to record the number of times each customer has made a payment on their loan, within the past 6 months. As customers begin their tenure, this variable starts with a value of zero. Suppose our model included this field as an independent variable. Presumably low risk customers have higher values, while higher risk customers have lower values. Without missing any payments, early lifecycle customer are penalized arbitrarily by the model. One could make the argument that this variable should be undefined (recorded as a database missing value flag) until a customer has a full 6-month track record, but this is exactly the sort of conversation which very often fails to materialize in real organizations. These are all instances of "phantom data": Items in the database which are missing values, but mistaken for real data. It shouldn't take much imagination on the reader's part to conjure similar problematic situations in his or her own field. The lesson is to look beyond the known missings for more subtle gaps in the data. Time spent investigating the nature of database systems, company procedures and so forth which generate data is insurance against being burned by serious misunderstanding of the data. ]] Despite using real data, the problems, as formulated, are somewhat artificial. Questions of sampling and initial variable selection have already been decided, as have the evaluation function and the model's part in the ultimate solution. To some extent, these are necessary constraints, but they are constraints nonetheless. In real world data mining, all of these questions are the responsibility of the data miner and his or her clients, and they are not trivial considerations. In most larger organizations, the data is large enough that there is always "one more" table in the database which could be tapped for candidate predictors. Likewise, how the model might best be positioned as part of the total solution is not always obvious, especially in more complex problems. A minority of contests permit the use of outside data, but even this is somewhat unrealistic since real organizations have budgets for the purchase of outside data, such as demographic data to be appended to a customer population. I've yet to learn of anyone paying for outside variables to append to competition data, though. Another issue is the large number of competitors which these contests attract. Though it is good to have many analysts take a crack at a problem, one must wonder about the statistical significance of having hundreds of statisticians test God-only-knows how many hypotheses against the same data. Further, the number of competitors and the similarity of top contestants' performance figures make selection of a single "winner" a dubious proposition. Finally, it has become rather common for winners of the contests to construct solutions of vast proportions- typically ensembles of gigantic number of base models. While such models may be feasible to deploy in some circumstances, they far too computationally demanding to execute on many real databases quickly enough to be practical. Some of these criticisms are probably unavoidable, especially the ones regarding the pre-selected, pre-digested contest data. Still, it'd be interesting to see future data mining competitions address at least some of these issues. For one thing, it might be interesting to see solution sizes (lines of SQL or C++ or something similar) limited to something which ordinary IT departments would be capable of executing during a typical overnight run. Averaging across an increased number of tasks might begin to improve the significance of differences among contestants' performances. ]] Predictive Modeling competitions, once the arena for a few data mining conferences, has now become big business. Kaggle ( kaggle.com ) is perhaps the most well-known forum for modeling competitions, using a crowd-sourcing mentality: if more people try to solve a problem, the likelihood that someone will create an excellent solution to that problem increases. The participants, and there have been 10s of thousands of participants since their 2011 beginning, sometimes have no predictive modeling background and sometimes an extensive data science background. Some very clever algorithms and solutions have been developed with, on some occasions, ground-breaking results One conclusion to draw from these competitions is that what we need in the predictive analytics space is more data scientists with different, innovative ideas for solving problems, and perhaps more in-depth training of data scientists so they can create these innovative solutions. After all, the Netflix prize winner created a solution that was an ensemble of model ensembles, comprised of hundreds of models (not a Kaggle competition, but one created by and for Netflix ). This idea of the importance of machine learning expertise was the topic of a Strata conference debate in 2012, tackling the question, which is more important, domain expertise or machine learning expertise, or the way it was phrased for the debate, who should your first hire be: a domain expert or data scientist? The conclusion of the majority at the Strata conference was the machine learning is more important, but even the moderator, Mark Driscoll, concluded the following, Could you currently prepare your data for a Kaggle competition?  If so, then hire a machine learner.  If not, hire a data scientist who has the domain expertise and the data hacking skills to get you there. ( http://medriscoll.com/post/18784448854/the-data-science-debate-domain-expertise-or-machine ) The point is that defining the competition objectives and the data needed to solve the problem is critically important. Non-domain experts, the data scientists, can not ever hope to understand the domain well enough to determine what the most effective question to answer would be, where to find the data to build a modeling data set, what the target variable should be, and how one should assess which model is best. These are business domain specific. Even companies building the same kinds of models, lets say customer retention or churn, will approach them differently depending on the kind of business, the lead time needed to act on potential churners, and the metrics for churn that relate to ROI for that company. Ive build models for companies in the same domain area that took very different approaches; even though I had some domain experience from customer 1, that didnt translate into developing business objectives well for company 2. Its the partnership that matters. I often think of these partnerships within an organization as the three-legged stool, all of which are needed for the modeling project to succeed: a business stakeholder who understands what business objectives matter to the company and how to articulate them, IT staff who know where the data is, what it means, and how to access it, and the analysts who know how to take the data and the business objectives and translate them into modeling objectives that address the business problem. Without all three, projects fail. We modelers could build the best models in the world that solve the wrong problem exceedingly well! (first posted at http://www.predictiveanalyticsworld.com/patimes/a-good-business-objective-beats-a-good-algorithm/) ]]  ... ]] Click to view a price quote on HLF . ]] NAMPA, Idaho, Jan. 24, 2014 (GLOBE NEWSWIRE) -- Home Federal Bancorp, Inc. ("Company") (Nasdaq:HOME), the parent company of Home Federal Bank ("Bank"), today announced results for the quarter and year ended December 31, 2013. For the quarter ended December 31, 2013, the Company reported a net loss of $2.3 million, or $(0.17) per diluted share, compared to net income of $219,000, or $0.02 per diluted share, for the same period a year ago. For the year ended December 31, 2013, the Company reported net loss of $255,000, or $(0.02) per diluted share, compared to net income of $1.8 million, or $0.12 per diluted share, for the same period a year ago. The Bank has entered into shared-loss agreements with the Federal Deposit Insurance Corporation ("FDIC") in connection with two acquisitions. The loans and foreclosed assets purchased in these acquisitions that are subject to the shared-loss agreements are referred to as "covered loans" or "covered assets." Loans and foreclosed and repossessed assets not subject to shared-loss agreements with the FDIC are referred to as "noncovered loans" or "noncovered assets." ... Click to view a price quote on HOME . ]] SHREVEPORT, La., Jan. 23, 2014 (GLOBE NEWSWIRE) -- Home Federal Bancorp, Inc. of Louisiana (the "Company") (Nasdaq:HFBL), the holding company of Home Federal Bank, reported net income for the three months ended December 31, 2013 of $645,000, a decrease of $236,000 compared to net income of $881,000 reported for the three months ended December 31, 2012. The Company's basic and diluted earnings per share were $0.31 and $0.30, respectively, for the quarter ended December 31, 2013, compared to basic and diluted earnings per share of $0.36 and $0.35, respectively, for the quarter ended December 31, 2012. The Company reported net income of $1.4 million for the six months ended December 31, 2013, a decrease of $461,000 compared to $1.8 million for the six months ended December 31, 2012. The Company's basic and diluted earnings per share were $0.64 and $0.63, respectively, for the six months ended December 31, 2013, compared to $0.73 and $0.71, respectively, for the six months ended December 31, 2012. ... Click to view a price quote on HFBL . ]] IRVINE, Calif., Jan. 23, 2014 (GLOBE NEWSWIRE) -- Sabra Health Care REIT, Inc. ("Sabra") (Nasdaq:SBRA) today announced that certain subsidiaries of Sabra (the "Issuers") had received as of 5:00 p.m., New York City time, on January 22, 2014 (the "Consent Expiration"), tenders and consents from the holders (the "Holders") of approximately $210.9 million in aggregate principal amount, or approximately 99.8%, of the Issuers' outstanding 8.125% Senior Notes due 2018 (the "Notes") in connection with their previously announced cash tender offer (the "Offer") to purchase any and all of the Notes and the related solicitation of consents ("Consent Solicitation") from Holders of the Notes to proposed amendments that would eliminate substantially all of the restrictive covenants and certain events of default provisions (the "Proposed Amendments") contained in the indenture governing the Notes (as supplemented, the "Indenture"). The remaining conditions upon which the consummation of the Offer and Consent Solicitation are subject have also been satisfied. As a result, on January 23, 2014, payment of the tender offer consideration of $1,068.37 for each $1,000 principal amount of Notes (the "Tender Offer Consideration"), consent payment of $30.00 for each $1,000 principal amount of Notes and accrued and unpaid interest up to, but not including, January 23, 2014 to Holders who validly tendered and did not revoke Notes prior to the Consent Expiration was made and the supplement to the Indenture implementing the Proposed Amendments was entered into and became operative. The Offer is scheduled to expire at 11:59 p.m., New York City time, on February 5, 2014, unless extended by the Issuers ("Expiration Time"). Tendered Notes may no longer be withdrawn, except to the extent that the Issuers are required by law to provide additional withdrawal rights. Holders who tender their Notes after the Consent Expiration and prior to the Expiration Time will be eligible to receive on the final settlement date, which is expected to be February 6, 2014, the Tender Offer Consideration, plus accrued and unpaid interest to the final settlement date. ... Click to view a price quote on SBRA . ]] Editor's Note: Any reference to TheStreet Ratings and its underlying recommendation does not reflect the opinion of TheStreet, Inc. or any of its contributors including Jim Cramer or Stephanie Link. Trade-Ideas LLC identified American Realty Capital Properties Inc Clas (ARCP) as a "storm the castle" (crossing above the 200-day simple moving average on higher than normal relative volume) candidate. In addition to specific proprietary factors, Trade-Ideas identified American Realty Capital Properties Inc Clas as such a stock due to the following factors:... Click to view a price quote on ARCP . ]] Subscription revenue of $17.6 million up 21% year-over-year Enterprise business unit revenue up 87% year-over-year 42 new customer agreements signed in Q3 Signs Cisco partnership agreement; focus on Internet of Everything DETROIT, Jan. 23, 2014 (GLOBE NEWSWIRE) -- Covisint Corporation (Nasdaq:COVS), provider of a leading cloud engagement platform, today announced financial results for the third quarter of its fiscal 2014 ended December 31, 2013. "Covisint continues to produce strong gains in subscription revenue, with a 21% increase year-over-year in Q3," said David McGuffie, President and CEO of Covisint. "We are pleased with our subscription revenue growth, which was driven by both new and existing customer agreements. We expect that subscription revenue will continue to become a larger part of our revenue mix going forward, as the investments we have made in the platform for delivering repeatable, frictionless deployments is paying off for our customers." ... ]] In trading on Friday, shares of the SPDR SP Insurance ETF entered into oversold territory, changing hands as low as $58.96 per share. We define oversold territory using the Relative Strength Index, or RSI, which is a technical analysis indicator used to measure momentum on a scale of zero to 100. A stock is considered to be oversold if the RSI reading falls below 30. In the case of SPDR SP Insurance, the RSI reading has hit 26.9  by comparison, the RSI reading for the SP 500 is currently 38.2. START SLIDESHOW:Find out what 9 other oversold stocks you need to know about  A bullish investor could look at KIE's 26.9 reading as a sign that the recent heavy selling is in the process of exhausting itself, and begin to look for entry point opportunities on the buy side. ... Click to view a price quote on KIE . ]] NEW YORK (TheStreet) -- As I mentioned in The Wrap in yesterday's column, the gold price did nothing through all of Far East and early London trading on their Friday.  But the rally that developed shortly after 11 a.m. GMT got capped about 30 minutes later, with the high tick coming at the noon silver fix. From there, the gold price got sold down until just before noon in New York---and then it chopped quietly higher for the remainder of the day. ... Click to view a price quote on GLD . ]] Contango ORE, Inc. (OTCBB:CTGO.PK) announced today that it has engaged Petrie Partners Securities, LLC to advise the Company on its options going forward, which may include a merger with an existing mining concern, sale for cash, stock, or a combination thereof, a joint venture, or additional exploration on the Tetlin Lease if sufficient funds are available to the Company. The engagement follows the receipt by the Company of the initial mineral resource estimate for the Peak Zone located within the Companys approximately 700,000-acre gold-copper-silver project in east-central Alaska on the Tetlin Lease. The roadaccessible Peak Zone covers only 40 acres of the total lease area. The indicated and inferred resources for the Peak Zone are limited to the area that was intensely drilled in 2012 through 2013, and do not include any estimates for potential extensions of the Peak Zone or any other prospect on the companys historically under-explored leasehold. The following indicated and inferred resource estimates were completed by G.H. Giroux of Giroux Consultants Ltd. of Vancouver, BC, Canada. G.H. Giroux is independent of the company and is the qualified person responsible for the resource estimate. Mr. Giroux is a qualified person by virtue of education, experience and membership in a professional association. Indicated Resources at the Peak Zone, Tetlin project, Alaska.             Grade Cut-off             Cut-off (g/t) (AuEq)     Tonnes Cut-off (tonnes)     Au (g/t)     Ag (g/t)     Cu (%)     AuEq (g/t)     Total Grams     Total Ounces 0.3     6,100,000     3.39     11.67     0.25     4.01     24,461,000     786,439 0.4     6,070,000     3.41     11.71     0.25     4.03     24,462,100     786,475 0.5     5,970,000     3.46     11.00     0.25     4.08     24,357,600     783,115 0.6     5,830,000     3.54     11.95     0.26     4.17     24,311,100     781,620 0.7     5,610,000     3.67     12.14     0.26     4.31     24,179,100     777,376 0.8     5,410,000     3.79     12.29     0.26     4.44     24,020,400     772,274 0.9     5,210,000     3.92     12.45     0.27     4.58     23,861,800     767,175 1.0     5,030,000     4.04     12.57     0.27     4.71     23,691,300     761,693                             Inferred Resources at the Peak Zone, Tetlin project, Alaska.             Grade Cut-off             Cut-off (g/t) (AuEq)     Tonnes Cut-off (tonnes)     Au (g/t)     Ag (g/t)     Cu (%)     AuEq (g/t)     Total Grams     Total Ounces 0.3     3,980,000     2.01     14.07     0.23     2.62     10,427,600     335,255 0.4     3,950,000     2.02     14.11     0.23     2.63     10,388,500     333,998 0.5     3,850,000     2.07     14.28     0.23     2.69     10,356,500     332,969 0.6     3,680,000     2.16     14.56     0.24     2.79     10,267,200     330,098 0.7     3,550,000     2.23     14.76     0.24     2.87     10,188,500     327,568 0.8     3,410,000     2.30     14.95     0.24     2.95     10,059,500     323,420 0.9     3,250,000     2.39     15.24     0.25     3.06     9,945,000     319,739 1.0     3,050,000     2.51     15.64     0.25     3.19     9,729,500     312,811 ... Click to view a price quote on CTGO . ]] COSTA MESA, Calif., Jan. 23, 2014 /PRNewswire/ -- Experian , the leading global information services company, today announced a new product in its market-leading Trended Solutions SM suite  the Estimated Interest Rate Calculator (EIRC) for Revolving Credit  which provides lenders with insightful, optimized tools to make the right credit offers to the right customers. This product uses longitudinal credit data to determine the last six months of interest on consumers' credit cards, which can help credit card lenders make better decisions in the prospecting, underwriting and account management processes. These insights add another dimension to the credit tools lenders are using by providing the ability to identify the effective annual percentage rate (APR), as well as interest and fee revenue generated by each consumer. (Logo: http://photos.prnewswire.com/prnh/20130131/LA51658LOGO) ... ]] Discover Financial Services (NYSE: DFS) today reported net income of $602 million or $1.23 per diluted share for the fourth quarter of 2013, as compared to $539 million or $1.06 per diluted share for the fourth quarter of 2012. The company's return on equity for the fourth quarter of 2013 was 22%. Fourth Quarter Highlights Revenue net of interest expense was up $113 million, or 6%, from the prior year to $2.1 billion. Total loans grew $3.2 billion, or 5%, from the prior year to $65.8 billion. Credit card loans grew $2.0 billion, or 4%, to $53.1 billion and Discover card sales volume increased 3% from the prior year. Net charge-off rate for credit card loans increased 4 basis points from the prior quarter to 2.09% and delinquencies over 30 days past due increased 5 basis points from the prior quarter to 1.72%. Payment Services pretax income was down $6 million from the prior year to $26 million. Transaction dollar volume for the segment was $49.8 billion, down 1% from the prior year. ... Click to view a price quote on DFS . ]] HIGHLIGHTS: Net Income of $26.5 million for the current quarter increased 28 percent from the prior year fourth quarter and net income of $95.6 million for the current year increased 27 percent from the prior year.   Dividend declared of $0.16 per share during the current quarter, the third increase since December 2012, totaling 23 percent.   Glacier Bancorp, Inc. stock price of $29.79 at December 31, 2013 increased 21 percent from the prior quarter and 103 percent from the prior year.   The loan portfolio increased $61.7 million, or 6 percent annualized, during the current quarter. Excluding the acquisitions, the loan portfolio increased $278 million, or 8 percent, during the current year.   Transaction deposit accounts of $3.1 billion increased $103 million, or 14 percent annualized, during the current quarter.   Current quarter net interest margin, on a tax-equivalent basis, of 3.88 percent, an increase of 32 basis points from the prior quarter net interest margin of 3.56 percent.   Interest income for the current quarter of $73.9 million, an increase of 6 percent from the prior quarter, and interest income for the current year of $264 million, an increase of 4 percent from the prior year. Results Summary     Three Months ended Year ended (Dollars in thousands, except per share data) December 31, 2013 September 30, 2013 December 31, 2012 December 31, 2013 December 31, 2012 Net income  $ 26,546 25,628 20,758 95,644 75,516 Diluted earnings per share  $ 0.36 0.35 0.29 1.31 1.05 Return on average assets (annualized) 1.33% 1.27% 1.06% 1.23% 1.01% Return on average equity (annualized) 10.96% 10.85% 9.17% 10.22% 8.54% ... Click to view a price quote on GBCI . ]] Independent Bank Corp., (NASDAQ: INDB), parent of Rockland Trust Company, today announced 2013 fourth quarter net income of $10.6 million, or $0.45 per diluted share, as compared to $14.7 million, or $0.64 per diluted share, in the prior quarter. The Company acquired Mayflower Bancorp, Inc. (Mayflower) on November 15, 2013 and recorded $6.2 million of merger and acquisition costs during the fourth quarter. Net income for the full year was $50.3 million, or $2.18 on a diluted earnings per share basis, as compared to $42.6 million, or $1.95 for the prior year. Net income for the fourth quarter and full year 2013 contained items, such as merger and acquisition expenses and gains on life insurance benefits, security sales, and debt extinguishment, which the Company considers non-core. When excluding such items, net operating earnings for the fourth quarter were $14.2 million, or $0.61 per diluted share, versus the prior quarters net operating earnings of $14.4 million, or $0.63 per diluted share. Net operating earnings for 2013 were $55.2 million, or $2.39 on a diluted earnings per share basis, an increase of 17.2% and 10.7%, respectively, when compared to net operating earnings of $47.1 million, or $2.16 per diluted share in 2012. ... Click to view a price quote on INDB . ]] Highlands Bancorp, Inc. (OTC BB: HSBK.OB) parent company of Highlands State Bank, reported fourth quarter net income of $563,000 in 2013 compared to net income of $1,012,000 for the same period of 2012. Fourth quarter net income available to common stockholders was $545,000 or $.30 per diluted share in 2013 compared to $995,000 or $.56 per diluted share for the same period in 2012. Net income for the full year 2013 was $2,270,000 compared to $1,548,000 for the full year 2012. Net income available to common stockholders for the full year 2013 was $2,201,000 or $1.23 per diluted share compared to $1,484,000 or $.83 per diluted share for the year of 2012. The quarterly and annual results for both 2013 and 2012 were positively impacted by partial reversals of the Companys valuation allowance on deferred tax assets which resulted in tax benefits of $333,000 or $.19 per share for the fourth quarter of 2013 and $640,000 or $.36 per share for the fourth quarter of 2012, and $1,310,000 or $.73 per share and $640,000 or $.36 per share for the years of 2013 and 2012, respectively. Net interest income increased by $134,000 to $2,123,000 for the fourth quarter of 2013 compared to net interest income of $1,989,000 for the fourth quarter of 2012. For the year ended December 31, 2013, net interest income increased to $8,036,000 from $7,237,000 for 2012 as a result of loan portfolio growth and lower cost of funds. The provision for loan losses was $339,000 for the quarter and $727,000 for the year ended December 31, 2013. In 2012, the Companys provision totaled $373,000 and $1,077,000 for the fourth quarter and year respectively. The decrease in the provision for loan losses reflects improvement in non-performing loans and managements continued assessment of the reserves maintained on non-performing loans. Loans increased $28.8 million in 2013 compared to $29.8 million in 2012. Charge-offs for the year ended December 31, 2013 were $762,000 compared to charge-offs of $564,000 in 2012. Recoveries of previously charged off loans totaled $2,000 in 2013, compared to $34,000 in 2012. The ratio of non-performing loans and performing TDRs to total loans declined to 2.69% at year end 2013 from 4.18% at year end 2012. ... Click to view a price quote on HSBK . ]] Click to view a price quote on HLF . ]] TOMS RIVER, N.J., Jan. 23, 2014 (GLOBE NEWSWIRE) -- OceanFirst Financial Corp. (Nasdaq:OCFC), (the "Company"), the holding company for OceanFirst Bank (the "Bank"), today announced that, as a result of the planned series of strategic initiatives disclosed in the third quarter earnings release, diluted earnings per share amounted to $0.11 for the quarter ended December 31, 2013, as compared to $0.23 for the corresponding prior year period. For the year ended December 31, 2013, diluted earnings per share amounted to $0.95, as compared to $1.12 for the prior year. Diluted earnings per share for the quarter and the year ended December 31, 2013 were adversely impacted by $0.19 per diluted share due to the previously announced strategic initiatives relating to the prepayment of $159.0 million of Federal Home Loan Bank ("FHLB") advances, at a cost of $4.3 million, and the consolidation of two branches into newer, in-market facilities, at a cost of $579,000. ... Click to view a price quote on OCFC . ]] LONDON, Jan. 23, 2014 /PRNewswire/ -- Reportbuyer.com just published a new market research report: US LEDs High Efficiency Lighting Market ... ]] FITZGERALD, Ga., Jan. 24, 2014 (GLOBE NEWSWIRE) -- Colony Bankcorp, Inc. (Nasdaq:CBAN), today reported net income available to shareholders of $1,241,000, or $0.15 per diluted share for the fourth quarter of 2013 compared to $203,000, or $0.02 per diluted share for the comparable 2012 period, while net income available to shareholders for twelve months ended December 31, 2013 was $3,120,000, or $0.37 per diluted share compared to $1,206,000, or $0.14 per share for the comparable 2012 period. This increase of 158.71 percent in net income for the comparable twelve month period was primarily driven by an increase in net interest income and noninterest income (excluding securities gains)  along with a reduction in provision for loan losses. "In addition to significant core earnings improvement, Colony again had marked improvement in asset quality. Substandard assets to tier one capital plus loan loss allowance ratio improved to 39.22 percent at December 31, 2013 from 40.94 percent at September 30, 2013 and 55.60 percent at December 31, 2012. Though we were successful in having regulatory agencies lift the memorandum of understanding during 2013 and realized significant improvement in earnings and asset quality, we still are committed to reducing our problem assets to an acceptable level and returning to acceptable earnings," said Ed Loomis, President and Chief Executive Officer. "Momentum carrying over from 2013 initiatives has Colony positioned for a successful and productive year in 2014." Capital ... Click to view a price quote on CBAN . ]] The Phoenix Companies, Inc. (NYSE:PNX) today commenced its previously announced solicitation of bondholders holding its 7.45% Quarterly Interest Bonds Due 2032 (CUSIP 71902E 20 8) seeking a consent to amend the indenture governing the bonds and provide a related waiver. The amendment to the terms of the indenture would allow Phoenix to extend to March 16, 2015 the deadline for all SEC reports required to be delivered to the bond trustee prior to that date, but will not alter the companys current obligation to pay principal and interest on the bonds as provided for in the indenture. ... Click to view a price quote on PNX . ]] Fourth Quarter and 2013 Highlights: GAAP earnings of $0.20 per diluted share, consistent with the prior quarter Full-Year 2013 earnings of $0.75 per diluted share compared to $0.40 per diluted share in 2012 Pre-Tax Pre-Provision income increased 3% QOQ Operating expenses declined to $225 million excluding certain nonrecurring items Net interest margin increased 1 basis point QOQ to 3.41% Fee income declined 2% driven by seasonally lower insurance commissions revenues Organic loan growth continues, with average loans up 9% annualized QOQ Average commercial business and real estate loans increased 7% QOQ Continued momentum in indirect auto loans, which increased by $246 million QOQ Noninterest-bearing checking balances increased 8% annualized QOQ Average core deposits increased 3% QOQ driven by higher customer balances Transactional deposits averaged 36% of deposits, up from 32% a year-ago Strong credit quality maintained NCOs averaged 0.34% of originated loans in 2013, consistent with prior year Nonperforming loans equaled 93 basis points of originated loans at year-end BUFFALO, N.Y., Jan. 24, 2014 (GLOBE NEWSWIRE) -- First Niagara Financial Group, Inc. (Nasdaq:FNFG) today reported net income available to common shareholders of $70.1 million or $0.20 per diluted share for the fourth quarter of 2013, highlighted by stable net interest margin, balance sheet growth and positive operating leverage. ... Click to view a price quote on FNFG . ]] Editor's Note: Any reference to TheStreet Ratings and its underlying recommendation does not reflect the opinion of TheStreet, Inc. or any of its contributors including Jim Cramer or Stephanie Link. McKesson (MCK) pushed the Wholesale industry higher today making it today's featured wholesale winner. The industry as a whole closed the day down 0.2%. By the end of trading, McKesson rose $2.69 (1.6%) to $172.37 on heavy volume. Throughout the day, 3,347,840 shares of McKesson exchanged hands as compared to its average daily volume of 1,731,800 shares. The stock ranged in a price between $170.90-$174.30 after having opened the day at $174.14 as compared to the previous trading day's close of $169.68. Other companies within the Wholesale industry that increased today were: Watsco (WSO.B), up 28.5%, China Metro-Rural Holdings (CNR), up 17.1%, Armco Metals Holdings (AMCO), up 3.4% and Louisiana-Pacific (LPX), up 3.2%.EXCLUSIVE OFFER: Jim Cramer's Protege, Dave Peltier, only buys Stocks Under $10 that he thinks could potentially double. See what he's trading today with a 14-day FREE pass.... Click to view a price quote on MCK . ]] BRIDGEVILLE, Pa., Jan. 24, 2014 (GLOBE NEWSWIRE) -- Universal Stainless Alloy Products, Inc. (Nasdaq:USAP) announced today that it will report financial results for the fourth quarter of 2013 on Wednesday, February 5, 2014. In conjunction with the earnings release, the Company will host a conference call at 10:00 a.m. (Eastern) on February 5 th. The call will be webcast simultaneously for all interested parties over the Internet. Listeners can access the conference call live at www.univstainless.com . Please allow 5 minutes prior to the call to visit the site to download and install any necessary audio software. After the call has taken place, its archived version will be available at this web site. ... Click to view a price quote on USAP . ]] In trading on Thursday, shares of Rubicon Minerals Corporation (TSX: RMX.TO) crossed above their 200 day moving average of $1.41, changing hands as high as $1.48 per share. Rubicon Minerals Corporation shares are currently trading up about 14.7% on the day. The chart below shows the one year performance of RMX shares, versus its 200 day moving average: START SLIDESHOW:Click here to find out which 9 other Canadian stocks recently crossed above their 200 day moving average ... ]] SAN DIEGO, Jan. 24, 2014 /PRNewswire/ -- SolidProfessor, a SolidWorks Solution Partner focused on delivering the best eLearning resources for CAD, CAM and BIM organizations, today announced that it will demonstrate its industry leading eLearning platform at the SolidWorks World 2014 Conference, being held from January 26-29, 2014 at the San Diego Convention Center, San Diego, California, USA. The 16th annual SolidWorks World Conference brings together designers, engineers, managers, and partners to discuss ideas, trends, and the technology shaping the future of product design. Show attendees will see how SolidProfessor and SolidWorks software allow customers to stay ahead of the curve with the latest design practices, standardize skills across a global team, and get the most out of the design tools they rely on. (Logo: http://photos.prnewswire.com/prnh/20140124/PH52037LOGO) ... ]]   Apparatus After identifying the materials and subjects of the study, the apparatus or equipment used in the study are described. If the apparatus is not well known within an area of study, it should be described in detail. If an apparatus is used by participants in the study, you will need to describe from the participants perspective how this apparatus was used: As soon as the participants heard the bell ring, they pressed a button placed one foot from their desk, which recorded the time lapse between hearing the bell and pressing the button. You will also need to provide enough detail for anyone wishing to replicate the study. (Szuchman, L. T, 1999). This may range from describing an apparatus as simple as the mesh-size of a net used to sample fish, to details about a novel use for complex high-throughput devices for sequencing millions of DNA samples. Be sure to mention and limitations of the apparatus, and precautions taken to compensate for these shortcomings. If it was operating on settings other than the default settings, be sure to list whatever changes were made to the settings of the apparatus. Also, when describing elements of the apparatus, be sure to be consistent with the names used. In other words, do not say that participants pushed a red button in one section, and then later refer to it as a red switch. (Szuchman, L. T., 1999). Typically the names and addresses of manufacturers of well known apparatuses are given in parentheses. Any modifications to a well known apparatus should described here as well. A schematic diagram or photograph of the apparatus may be included, especially for novel or non-standard equipment used. If you need to describe in detail a novel apparatus that you constructed for the study, you may include this information in an appendix. (Szuchman,1999) In chapter 6, we noted how subjunctive clauses are used to communicate message of urgency or necessity. You might see warnings in a method section like the sentences below using the subjunctive. If you do not remember this construction, you may want to review subjunctive clauses in chapter 6. It is imperative that this flammable coating is applied in a well ventilated room. The manufacturers guidelines require that samples be sealed in a water-proof container. Since this is a known carcinogen it is highly recommended that standard laboratory safety practices be carefully followed when using this reagent. http://www.academicenglishsolutions.com/AES/home.html ]] Data Analysis The content of the Data Analysis section will vary according to the type of statistical analysis used in your study. This section typically begins with a statement on how the data was summarized, including a measure of central tendency and the variability (Zeiger, M. 1999). For normally distributed data, the mean and the standard deviation typically summarize the data. For skewed data, the mean and the inter-quartile range give a more precise summary of the data: The median water withdrawn was 2.0 m 3 of water per ton of sugar cane, and the lower and upper quartile were 1.4 and 4.0 m 3 of water per ton of sugar cane, respectively (Fig. 2). In this section, the tests used for statistical analysis are identified. If you used a well-known test such as the Students t test, Chi-Square, or Wilcoxian signed-rank test, no reference is necessary. For less well-known tests, include a reference to a source that describes the test in detail. The following standard statistical tests were used for between-group comparison purposes: chi-square test; Fishers exact test; Students independent sample t  test; Mann-Whitney test, and Kruskal-Wallis test. This section should also include a statement on the P value in which differences are considered statistically significant: A p-value of 0.002 was considered significant. The data gathered will determine the statistical tests used in your study (Leedy P. D., 1997). While there is a wide variety of methods for testing and analyzing data, statistical testing has its limits. Gustavii, for example, advises caution when comparing baseline characteristics by statistical tests since a minor imbalance in a key prognostic factor can have a profound effect on a treatment comparison, even when the imbalance shows no significance. Significance testing can thus obscure an important imbalance. Therefore, in addition to the statistical testing (some experts say instead of statistical testing) compare the baseline characteristics with the use of clinical experience- and common sense ( Gustavii, B. 2008). Identify the computer software used in your data analysis, including the version, release number, and any non-default settings used. All analyses were performed through a SPSS software package for Windows Release 11.0.1 (15 Nov 2001) Various software programs are commonly used to analyze data, presenting numerous advantages including accuracy, speed and variety of perspectives: numerous tests and analysis can be performed on data in a matter of minutes. Yet, whatever analysis is being done, the ultimate interpretation will depend on the researchers understanding of the various tests and results of the analyses. For example, testing and rejecting the null hypothesis, only indicates that a significant difference between one data set and another exists. No computer software can fully interpret what this difference means. Ultimately, it is the researcher who needs to synthesize and interpret the results of the data analysis, providing compelling and new perspectives or insights into the data (Leedy, P. D. 1997) http://www.academicenglishsolutions.com/AES/home.html ]] Extended vs. Abbreviated Methods An overview of the study design is followed by a description of the methods and apparatus used. For quantitative studies, your goal is to give just enough information for a competent researcher in your field to be able to understand what you did, so that if they wished to, they could replicate your experiment. Although it may not be likely that anyone would replicate your study, if the journal editors are not convinced that they could carry out and replicate your study, your paper would be rejected. Consequently, you will need to take great care, being quite meticulous and detail-oriented in writing the Materials and Methods section. Of course, your ability to present a precisely detailed methods section is contingent upon your ability to maintain detailed notes during the experiments in a lab notebook. However, while you need to provide sufficient precise and detailed information for another researcher to replicate your study, you will need to be very careful not to give unnecessary detail. As Goldbort points out, rather than exhaustive detail or familiar minutiae, the priniciples on which the methods are based are more important to explain. (2006) A great deal of the information recorded in your lab notebook will not be necessary to include in the methods section. Finding a balance between providing too much or too little is a typical trouble spot of this section of the paper. If you are using a well-known method, it should not be described in detail, but instead a simple one sentence citation will suffice: Balb/c mice were immunized according to the protocol described by Farias et al. (2002). The amplification and primer selection was carried out as previously described (Wilson et al., 2007). Colonies were quantified and identified by classical phenotypic tests [17]. However, be careful with this sort of citation: if the method has been published but is not well-known, the reader would likely appreciate a few details about what was done: We surveyed the participants using the Primary Care Evaluation of Mental Disorders (Nelson, 2009). NOT: We surveyed the participants as done elsewhere (Nelson, 2009). Additionally, if you make significant changes to a well-known protocol, you need to specify the changes that you made. Swales and Feaks note that in general the material and method section can be either abbreviated or extended (2004). Using an abbreviated methods section assumes the reader has certain background knowledge about the subject. Consequently, abbreviated methods tend to provide less explanation, justification, definitions, examples and subsections with headings. An abbreviated methods section is more apt to rely on acronyms, abbreviated information in parenthesis, and citations without much elaboration. Abbreviated methods also tend to use compound verb clauses as opposed to a series of longer sentences with single verbs. See chapter 7 on telegraphic style for examples. Conversely, extended methods sections tend to provide more background knowledge, using subsections with headlines, as well as purpose statements, examples, and definitions. Extended methods sections are more commonly used in a dissertation, as well in research papers that use novel unknown methods. The subject of your paper and your audience will influence your choice using an extended or abbreviated method. If for example, the method of your paper is novel and significantly different from previous attempts to approach the topic you are considering, then you will need to create a more extended methods section. If however, the method you use in your paper has been commonly used in the past to address your topic, then an abbreviated method section would be the best choice. http://www.academicenglishsolutions.com/AES/home.html ]] Study Design and Procedures Carried Out The descriptions of the materials, animal subjects or human participants and apparatus of the study is often followed by an overview of the study design, which begins by re-naming the original question or purpose identified in the introduction. This section typically lists in chronological order the interventions, measurements and experiments performed, as well as the duration of these events (Zeiger, 1999). In general, the study is described from the perspective of how the researcher organized the experiments. However, if human participants are involved, describe the particular tasks they performed from their perspective. In other words, do not say the experimenter gave the participants something to read, rate, rank, listen to or watch. Instead say the participants read, rated, ranked, listened to, or watched something (Szuchman, 1999). This section names the independent variables (interventions) and dependent variables (variables measured), as well as information about control groups and baseline measurements. This section will also clearly name any underlying assumptions the experimental design is based upon (Zieger, 1999). A strong study design will aim to minimize the influence of extraneous (Zobel, 2004) factors on the results. Be aware of any factors in a given experiment that may make it a special case limited to the variables and conditions of your study rather than a case that can be generalized beyond your study (Zobel, 2004). Certain benchmark tests and standard data are typically used to ensure that results are not the product of differences in data sets. When it not possible to used standardized data, then you should try to ensure that the data is representative (Zobel, 2004). Specify the conditions of the study and whether all animal subjects and human participants were subject to the same conditions or whether they were divided into groups subjected to various conditions. Whenever possible, for the sake of the reader, use names for groups that more clearly identify rather than general names such as Group A or Group B (Szuchman,1999). Non-alcohol consuming group exhibited better absorption of the formulation than the alcohol consuming group. NOT: Group A exhibited better absorption of the formulation than Group B. In some rare cases where this may be relevant, it is recommended to list hazards or potential problems that may ensue when replicating the experiments. You may indicate warnings in a separate paragraph labeled Caution( http://owl.english.purdue.edu , 2009). Choosing the best possible study design will require extensive knowledge of the limits and strengths of various possible experiment designs as well as a strong understanding of your data. It is useful to explain to the reader why you chose one design over another: do not take for granted that your reader will understand why you made certain choices. (Leedy P. D., 1997). If it is unclear to the reader as to why certain procedures were done, then clearly explain why: This lineage was chosen because it was derived from normal lung tissue of a 12-week-old male fetus and therefore was intended to be similar to the lung tissue of patients. A strong study design, in addition to including experiments that will most likely verify your hypothesis, should consider data in which the hypothesis is least likely to be verified as well. Such experiments and results may be even more instructive than those that are likely to verify your hypothesis. If experiments produce anomalous results, then further tests should be designed to investigate and eliminate these results. Ideally, experiments should be designed that provide unambiguous results that answer the main question of the text Include some details about how the study was well randomized. Again if your method of randomizing is well known, then great detail is not necessary: A randomized crossover trial was used as study design... The experimental design was a 4 x 4 factorial in complete randomized blocks, with four replications. http://www.academicenglishsolutions.com/AES/home.html ]] There are unlimited possible topics for research papers, and the research and methods used to address the questions of these papers are equally varied. Some possible topics of Engineering research papers might include studying the wear characteristics of building materials; innovations in electronic circuitry or neural networks; improving algorithms for computer graphics functions, improving plans of escape on passenger airlines; evaluating and improving an emergency fire-response system on an oil tanker; improving GPS and other monitoring systems of illegal logging in the Amazon; understanding the signal pathway of certain human cells; designing and improving insulators for heavy-duty electrical power lines. The methods and procedures used for each of these problems require a wide variety of methods that are not easily categorized. Add to this, the variety of types of research done in the Physical Sciences, Social Sciences and Humanities, and it becomes difficult to provide precise general rules that apply in all cases to writing the methods sections. Yet, despite the wide variety of research methods, some general patterns can be seen across in most fields of study. We will review some of these common features below and end the chapter considering some of the differences seen across these areas of study. The main purpose of the method section is to report on what experiments, simulations, interviews, analysis of proofs, surveys, modeling, or fieldwork was done to answer the main question or hypothesis of the introduction. Depending upon your field of study, this section is sometimes identified as the Materials and Methods, the Experimental Design, Theory, Protocol, or Procedure. In section three, we review the sections of a research paper in their typical chronological order, beginning with the title and abstract in chapter 8, and ending with the discussion, yet, most authors do not write their paper in this same chronological order. In fact, after creating their tables and figures, they begin by writing the materials and method section and then results, and not the abstract and introduction. Since you will be very familiar with the content of this section, being the most concrete section of your paper, and since many of the protocols of this section are well known and have been previously published, and since the methods section does not require you to interpret the meaning of the results but simply report what was done, you may be like most authors who find this section the easiest to write. My own editing experience supports this claim with the material and method section having the fewest number of errors compared to other sections of a paper. Nonetheless, journal editors commonly reject papers due to errors in the method section. So, great care and attention need to be given to accuracy and detail of the method section. Journal editors and referees may criticize a method section for several reasons. While you cannot anticipate every particular criticism of a referee, you should at least be able to address the following shortcomings: a failure to elaborate on experimental assumptions and design; the experiments are not verifiable or reproducible; the text is confusing; incorrect technical specifications; numbers do not add up; insufficient number of experiment repetitions; too much irrelevant information; too little description of anomalous experiments and results; no discussion of limits of the apparatus and equipment or precautions taken to avoid limitations; statistical analysis that are inadequate, or dubious choices in the study design. This chapter focuses on some typical features found in a variety of methods sections in a wide range of studies and fields. Knowing some general guidelines, features and patterns of the methods section before you begin writing will make the task easier. In addition, this chapter focuses on several common English trouble spots of the Methods section. Organization The organization pattern of the materials section typically follows the chronological order of your experiments, analysis of proofs or field work, but there may be some sections within the chronological scheme that use a most-to-least-important structure. For instance, while the overall methods section may be organized chronologically, for a complex topic under a certain sub heading that has more than one paragraph, a most-to-least important structure may used to organize this section. If for example, numerous variables were measured together, then it is common to report the most important results first, followed by less important results. Another possible organizational pattern is general to specific. Subheadings are commonly used to indicate the organizational order of the methods section. We will consider subheadings later, but for now, note some typical organizational subheadings found in biomedical papers in Table 1 Table 1 Animal Studies Clinical Studies Materials Animals Preparation Study Design Interventions Methods of Measurement Calculations Analysis of Data Study Participants Inclusion Exclusion Criteria Study Design Interventions Methods of Measurement Calculations Analysis of Data The best guide for organizing your materials section will be provided either by the journal to whom you will submit your paper or one of the style guides mentioned below. You can find guidelines for organizing the methods section at the journals webpage in a section called guidelines for authors or information for authors. Some journals will specify their expectations in great detail, others will name a few minor guidelines and others will simply refer the authors to a certain text such as the CBE Manual for Authors, The Publication Manual of the American Psychological Association, the Chicago Manual of Style, MLA Handbook for Writers of Research Papers, the AMA Handbook of Style or the IEE Editorial Style Manual. If the journal does not specify guidelines for subheadings, consider one these more general style manuals affiliated with your field of study. http://www.academicenglishsolutions.com/AES/home.html ]] Numbers There are some common rules regarding numbers that represent quantitative data in research papers. Knowing these rules will be helpful for writing the material and method section as well as other sections of the paper: Represent numbers under eleven using the word: eleven, nine, five, etc, whereas for numbers above eleven use digits: 12, 136, 1,200, etc. Use numerals even for numbers below ten when comparing with numbers above ten: A total of 5 out of 24 of the respondents dropped out of the study. Be careful with convention of using commas with numbers. Brazil as well as other countries in the world use a comma (virgula) with numbers where a decimal (ponto) point is used in the US: 3.5 % NOT: 3,5 % Be careful with compound nouns that report numbers. All words preceding the head noun must be singular since they function like adjectives. In English, adjectives are always singular: A 36-day-old rat. NOT: a 36 day s old rat. A three-month old infant NOT: a three month s old infant Do not begin a sentence with a digit; instead use the word form for the number in question. Fifty-six rats were used. NOT: 56 rats were used. Do not confuse certain words dealing with quantity and distance: Lower/lesser; further/farther Lower = below another quantitative measurement: Lower temperatures were recorded for sample B. Lesser = not so great or important as something else; applied more to abstract concepts. Although this economic plan had serious flaws, it was the lesser of two evils. Farther = a greater measurable distance than another. Farther down the mountain, the vegetation turns to grassland. Further = to a greater degree than something else; applies more to abstract concepts. Further studies are needed to understand the mating habits of this mollusk. The terms twice vs. two times have essentially the same meaning, except that twice is favored for being shorter. The specimens were disrupted by sonication two times for 45 s at 5  C. The specimens were disrupted by sonication twice for 45 s at 5  C. Use the term circa with historical dates, but not with measurements. The temple was destroyed circa 1432 BCE. NOT: Circa 542 birds were sighted. NOT: Circa 2ml was added to the buffer. Note below that certain journals such as Science Magazine advise against using such imprecise expressions as a three-fold rise, an eight-orders-of-magnitude increase, two-fold increase, two times as much , but instead prefer the author to give a more precise numerical percentage (Science/AAAS: Science Magazine 2009): Avoid using "-fold" because expressions such as "20-fold smaller" are imprecise; use percentages, proportions, orders of magnitude, or "factor of" instead. (Exception: Usage such as "1000-fold excess" is appropriate.) When describing a decade use this form: In the 1970s During the 1980s NOT: In the 70s NOT: In the Seventies NOT: in the 70s Ordinals, such as first, eleventh, ninety-second , among others, are used far more commonly in English than in some languages. Consequently, it is worth investing your time to learn their form, especially if you plan to do research in an English speaking country, where they are used regularly to record certain quantitative data. Do not confuse their form: 23 rd NOT: 23th 2 nd NOT: 2st Eleventh, twelfth, thirteenth, NOT: eleventeen, twelveteen, Use the short numerical form rather than the longer word form in when discussing centuries: Then 19 th Century NOT: The nineteenth Century Use the percent symbol (%) whenever a numeral accompanies it: A total of .053 % NOT: .053 percent http://www.academicenglishsolutions.com/AES/home.html ]] http://www.academicenglishsolutions.com/AES/home.html ]] Qualitative Methods of Social Science and Arts and Humanities papers A scholar or researcher from the Social Sciences or Arts and Humanities may not find the information provided thus far not entirely useful for the type of research they do, nor papers they write. Consequently, some points are made below on the differences between Physical Science versus Social Science and Arts and Humanities methods. Social Sciences While the methods section of a Social Science text often resemble the quantitative model of Physical Science papers, they may also incorporate a mix of quantitative and qualitative data or be solely based on qualitative research and data. Likewise, some Physical Science papers are a mix of quantitative and qualitative research. This change from quantitative to qualitative methods in recent decades reflects a growing disenchantment among Social Scientists with the limits of quantitative data and experimental models of the Physical Sciences, as well as a greater appreciation for qualitative data in understanding certain social phenomena. Instead of the hard numerical data and methods of Physical Science, Social Scientists focus more on soft verbal qualitative data for significant social and behavior trends, revealing these trends may involve observations, taped-interviews, surveys, and even researcher engagement and dialogue with the target community, and other soft data and methods. In general terms, the Physical sciences deal with numerical data, while the Social Sciences and Arts and Humanities deal more with verbal data. Such differences in data require different paradigms of research. Since qualitative research operates in an entirely different paradigm than the Physical Sciences, you should not squander precious space trying to justify your choice of qualitative methods by doing an in-depth literature review on the history of qualitative research. Researchers in your field will know the history of qualitative research methods and will not need to be convinced of their value. Those researchers from the Physical Sciences who may happen to read your paper will either be compelled by the results of your study or not, but lengthy justifications may have little impact on swaying such readers. So do not be tempted to dress your research in a quasi-objective scientific manner: instead, rest confident that your work addresses topics that would be poorly served by a strict scientific model. The data of qualitative research in most cases are fairly straightforward narrative responses from the subjects of the study. In qualitative studies, there will be less focus on the methodical and precise measurements of the Physical Sciences, and more painstaking attention is given to thick description and deep contextualization of the subjects of the study. While qualitative data can now be processed by software resembling those of the Physical Sciences, many qualitative researchers primarily rely on simply spreading out surveys and other data on the living room floor, and meticulously sorting, performing multiple re-readings and interpretations to compile results. Likewise, qualitative researchers are not concerned with providing the kind of elaborate details about the method used as can be found in the methods of a Physical Science paper. Again, the story that is told with the results will be more compelling than the software or other techniques used to compile or analyze the data. Unlike in the Physical Sciences, the qualitative researcher does not need to provide precise elaboration of their method because they are not claiming that their work can be precisely reproduced. In fact, it is clear that another researcher asking the same questions in a survey, for example, will most likely get a different answer ( Wolcott, H. F. 2009). Novice authors of qualitative research should avoid the urge to justify their method section by steeping it in a highly technical or contrived scientific guise: no amount of justification will convince certain skeptics. The strongest proof of the method is a unique perspective resulting from fairly simple methods. This new perspective will succeed if it resonates with the reader and provides new and richer insight into the phenomena at hand. Organization and Study Design The following section is indebted to (Leedy P. D., 1997). Below I have paraphrased his description of the organization and study design of qualitative research. Qualitative research can involve various methods. I begin with a summary of case studies and ethnography and follow with a summary of some of the other common qualitative methods commonly used. Case studies focus on one phenomena or subject. Their goals may be to learn about a specific and unique subject, or they may seek to learn more about a larger group by focusing on one person as an exemplar. They may aim simply to describe in detail a certain phenomenon, or they may attempt to evaluate it as well. The primary goal is to describe certain phenomenon from the perspective of the participants. A case study, may involve, for example, learning how residents of a psychiatric residential site experience life in their community. Typically, researchers spend extended periods of time in the study site, primarily watching people in their own territory interacting with them in their own language, on their own terms (Leedy P. D., 1997). The researchers often interact with the people studied and become personally involved with the people and phenomenon studied. Another common qualitative study involving observation is ethnography (Leedy P. D., 1997). As opposed to the more singular focus of a case study, ethnography typically involves a larger focus on a group and its culture. While ethnography originally developed in Anthropology, it is commonly found in education, psychology and sociology. While a culture was once defined as practices of a large populations of people, currently it is not uncommon to see studies at the micro level considering the culture of a coffee shop, or culture of a graffiti artists. Data Collection Case studies may involve a certain amount of quantitative data as well as qualitative data. Various quantitative measurements such as medical tests, academic achievement tests, can be incorporated into the study. Yet the researchers primary means of data collection will be engaging the subject in conversations, or observing them interacting in the study site. The focus is in understanding how a phenomenon is experienced by the subjects of the study. As with case studies, the data collection for ethnography involves fieldwork at a specific site, involving conversations with community members as well as observations of group interactions. (Leedy). While the study originally begins with a focus on a narrowly defined set of questions, these questions may evolve and be redefined as the researcher is further immersed in understanding the culture studied. Likewise, as this happens, the interviews and observations will become more focused from a new perspective. (Leedy) Ethnographic data collection may involve a combination of a data sets. The researchers themselves may participate in certain events to gain a certain perspective. Unlike the random sampling procedure of quantitative studies, ethnographers may intentionally choose a certain person to be interviewed with the explicit purpose of gaining a unique insights from them. The questions of the interviews will tend to also be open-ended to allowing the study subject maximum freedom to answer. In addition, ethnographic data collection may involve examining various artifacts including letters, journals and diaries, as well as legal documents , such as internal communication of an organization, and objects such as artwork, music lyrics, drawings, and posters. The researcher may use an audio or video record as well as extensive notes in a journal. The primary data sources (interviews and observations of subjects) may be combined with secondary data such as questionnaires and other documents such as student test scores or census reports. Data Analysis There are two common types of data analysis for case studies and ethnography (Leedy). The first type, according to Leedy involves searching for patterns, themes, structures or concepts in the data. This first approach does not attempt to interpret the meaning of the data, but simply reports the patterns observed. This first approach may strive to create a more objective and neutral tone of language as seen in quantitative studies. A second approach commonly used to analyze the data involves more explicit evaluation of the data (Leedy P. D., 1997). This approach may be more apt to rely on intuition and judgment in analyzing the data. The authors of such studies may be less concerned about appearing objective and neutral in tone, and more willing to use literature or drama to bring the reader closer to the life experience of the subject of the study. While case studies are more focused on analyzing the meaning of a specific phenomenon for a particular person, ethnographers will analyze the meaning of their data as it has meaning to a group in relation to its culture. The analysis may involve slow careful readings of interviews and thoughtful summaries of their meaning. Such analysis will be repeated again and again until layers of meaning develop and begin to suggest a larger connected perspective. Authors of ethnographies and case studies will typically quote some event or dialogue that is emblematic of culture or specific phenomenon studied. The authors summarize and highlight certain points or assertions that are embodied in such quotes. Such assertions are the outcome of lengthy and rigorous examining, categorizing, analyzing and synthesizing the data. In addition to a case study or ethnography, there are numerous types of qualitative research methods used in a wide variety of studies, including phenomenological-hermeneutical, grounded theory, critical theory, action network, dialectical, constructivist, and postmodernist. These different approaches have different goals and fundamental assumptions. As Leedy points out, Phenomenological-hermenutical studies typically focus on topics that are meaningful to the authors, relying solely on interviews of small groups purposefully chosen for their unique perspectives, focusing on their analysis on certain phrases of the respondents considered heavily laden with meaning. Critical theory, on the other hand, is explicitly committed to supporting a certain disempowered community. For example, the methodology of Participatory Rural Appraisal and the Antigonish Movement is heavily influenced by Paulo Freires educational philosophy, which assumes that rural community development depends upon active, empowered, and informed community members. Action-network research is another method that aims to address practical problems, but more typically is used to evaluate programs and policies, possibly in a school, institution, or other social organization. Some type of action is initiated then the outcome is studied with the intention of finding practical and immediate solutions. Aside from the differences in methods and goals, these various approaches have much in common with case studies and ethnography: repeated cycles of careful reflection and interpretation of social situations. Arts and Humanities Research papers from the Arts and Humanities are even further removed from the preoccupation with hard data of the Physical Sciences. The data of Humanities papers may involve multiple close readings, both analyzing and interpreting the meaning of a philosophical text, painting, musical score, poem, historical artifact, or cultural practice. The heart of a Humanities paper resembles qualitative research in that it may involve numerous cycles of examining and interpreting, and then examining and re-interpreting, adding each time nuanced levels of understanding and depth to the papers topic. In recent decades, Humanities paper have re-examined classic texts as well as cultural phenomena through various methods and interpretive approaches including novel perspectives of post-colonial discourse, hermeneutics, discourse analysis, postmodernism, deconstruction, post-structuralism, as well as analysis of class, race or gender. Depending upon the journal, a Humanities paper can be quite freeform compared to more formulaic Physical Science papers. You may not find the distinct sections of the standard IMRAD paper in a humanities paper, nor the elaborate materials and methods section like in Physical Science papers. However, there may be a parallel discussion about the materials examined and the methods used to examine the material. The success of a Humanities paper is determined more by the compelling logic of its interpretations and arguments as well as creative use of language and reasoning than by empirical data as in Physical Science papers. For example, a history paper will need to elaborate the method of collecting data. Documentation of primary and secondary sources will be a major focus as well as clear and persuasive interpretations of the data. Despite differences between papers from Social Science, Humanities and Physical Sciences, all three types of papers share a common structure and characteristic of further explicating certain unknown ideas and phenomena, providing new methods and means of addressing certain problems, as well as contributing new perspectives to their field of study. ======================================================================== In this chapter, we reviewed various topics relevant to the materials and method section of research papers, in the next chapter, we will review various topics critical to creating a well structured and coherent results section of a research paper. http://www.academicenglishsolutions.com/AES/home.html ]] Tense of Methods Section As previously mentioned, the materials and method section is mostly written in the past tense and passive voice . Note the differences between the passive and active voice below. Active voice: In a sentence formed in the active voice, the subject performs the action of the verb: We dried and stored the samples. (subject) (verb) Passive voice: In a sentence written in the passive voice, the subject is the recipient of the action of the verb: Samples were dried and stored. (subject) (verb) The past passive tense is frequently used in the method section. The past tense is used because the experiments are finished and done. The passive voice is used because it places the focus on what was done and not who did something: Samples were plated and store . NOT: We plated and stored samples. The passive voice is traditionally used in methods sections of research papers. Occasionally the passive voice is used with a by-phrase naming the agent of the action as well: A census was administered by the department of Anthropology of USP . On the other hand, increasingly more writing style guidebooks recommend using the active voice in research papers. Despite the general preference of the active voice in general writing guidebooks, most of the Physical Sciences continue, for good reasons, to use the passive voice since it places the focus on what was done to something, but not who did something to something. In addition, a series of sentences using the active voice could be monotonous: We sorted and weighed the samples. Then we freeze dried them and then we stored them for 2 weeks. Next we thawed out the samples in.... Russey points out, there is no need to wax poetic. Readers expect to put up with a multitude of bland, impersonal, passive constructions in this section, although you should try not to build every sentence according to the tiresome was done model. Alternative formulations supply welcome variety. (Russey, W. E., Ebel, H. F., Bliefert, C. 2006). For example, consider such alternatives to the passive voice: It proved advantageous to... This was accomplished by... In this way it became possible http://www.academicenglishsolutions.com/AES/home.html ]] http://www.academicenglishsolutions.com/AES/home.html ]] Figures and Tables Figures and Tables allow authors to present complex data in a simple visual display that allows the reader to understand results far easier than by a narrative description. Since Figures and Tables are time-consuming to create, and costly to publish, one of the first things you will need to determine whether you need to use them instead of a narrative description. If the results are simple and straight forward and easily described in a few sentences, then figures and tables are unnecessary. However, if you have multiple variables and numerous readings of these values, then figures and tables are needed. Likewise, if you are using figures and tables you should be sure that you do not repeat the same information in a table that can already be viewed in a given figure. Another common error is describing in detail in a text what can be seen clearly in figures or tables. While a certain amount of description may be necessary and inevitable, strive not to repeat what can readily seen in the tables and figures. Figures include anything in a research paper that is not a table, text, or formula. To create figures and tables, you should begin by reading the guidelines for authors provided by the journal you are targeting. We cannot review here all the various technical details related to creating various figures and tables since they could easily fill a book by themselves, but instead, we will look at a few issues related to the language of figures and tables. While there are many good guidebooks that provide extensive coverage of the technical details involved in creating figures and tables, I recommend The Art of Scientific Writing (Matthews, J. R., 2007). Every figure should be accompanied by the term Figure followed by a decimal point and a number: Figure.1, Figure.2, etc. You can write out the entire word figure or provide it as an abbreviation: fig. 1, for example. While you have a choice here to abbreviate or not, it is more common in a citation to abbreviate. Figures for longer texts such as dissertations and books may also have two numbers. The first number designates the chapter or subdivision of a text, while the next number, followed by a dash (-) designates the figure number: Chapter 1: Fig. 1-1, Fig. 1-2, Fig. 1-3, Fig. 1-4, etc. Chapter 2: Fig. 2-1, fig. 2-2, Fig. 2-3, etc Whatever form you choose, be consistent throughout the text: Fig. 1, Fig. 2, Fig. 3, etc. NOT: figure 1, Fig 2, Figure. 3.0, figure four, etc. This marker should be accompanied by a short description of the figure, which is known as a caption and functions much like the title of a book or research paper. It should be brief and without verbs. Yet even though it is not a complete sentence it should be followed by a period: Fig. 1. I mmunohistochemical staining for KDZ1 in non-tumoral and tumoral cells. If the caption does not sufficiently describe the figure for the reader, authors commonly include a legend to supplement the caption. A legend provides additional text that may explain certain results, methods, parameters and measurements that may be needed to clarify the figure: Figure 1 . RP-HPLC profiles of the crude Gly-Cys-Phe-NH 2 (10 m g) resulting from traditional SPPS ( A ), SPPS at 60 C using conventional heating ( B ) and microwave- assisted SPPS ( C ). Gly-D-Cys-L-Phe-NH 2 ( D ). RP-HPLC iden tification of the isomer present in B spiked by D ( E ). Analytical conditions : Column: Vydac C 18 , solvent A: 0.1% TFA/H 2 O; solvent B: 90% ACN/H 2 O containing 0.09% TFA, l : 210 nm, flow rate: 1.0 mL/min, linear gradient: 0 to 66% B in 33 min. Components identified by LC/ESI-MS: 1 = desired LLL peptide; 2 = analogue Gly-Cys-Phe-Phe-NH 2 ; 3 = peptide isomer LDL Again, legends should be as brief as possible. As mentioned in chapter 8, gapping can be used to shorten captions and legend of figures by simply omitting non-essential prepositions, articles and other words to create a telegraphic style. See chapter 8 for examples. While figures and tables typically appear at the end of a paper in the results or discussion section, they are often the first part of a paper to be created by the author. Just as a picture is worth a thousand words, strong figures provide a visual summary of the data that quickly communicate the main findings as well as or better than lengthy description may. Consequently, it is logical that the author begins by creating figures and tables since they display the heart of the research and define the direction of the papers story. Ultimately, figures and tables should be able to stand-alone: the reader should be able to understand them without referring to the text. Since figures and tables are such unique visual tools used to powerfully and efficiently communicate the core meaning of a study, it is essential that they are carefully crafted to communicate the message as clearly as possible. Take as much time as possible to revise them so that they seem as clear and cogent as possible. http://www.academicenglishsolutions.com/AES/home.html ]] The most common tense form used in the results section is the past tense. Results are reported in the past tense since the experiments are completed. X increased but Y remained constant. ` However, the present tense is also common in the results section since you will need to discuss factual details that are continuously true. The present tense describes general unchanging laws of nature as well as patterns and behaviors that are generally consistent and unchanging like laws of nature: Water freezes at 0 celsius. The State Park of Serra do Mar is located in the State of Sao Paulo. Capybaras live close to some body of water and even mate in water. Scavenging, or necrophagy, is a carnivorous feeding behaviour in which a predator consumes corpses or carrion that were not killed to be eaten by the predator or others of its species (Wikipedia). Vocabulary for Describing Data Certain words are commonly used in the results section to describe the degree of intensity of phenomena. Below in Table 1 you will find adverbs, nouns and adjectives categorized according to a numerical percentage of frequency or intensity. Some words can describe sounds (faint, blaring, muted); health conditions (robust, feeble); evaluations of importance (inconsequential, substantial); sensations (faint); colors (dim, pale, subdued). Also note that in some contexts, some of these words have additional subjective negative connotations and should be used with this in mind (paltry, trivial, trifling, feeble, pitiful, meager, utterly, inordinately, extremely, merely). Study the meaning of these words. If words like never seem too basic to you, then study less commonly known words like plethora, seldom, markedly, hardly, perfused, and barely. Note that few X means a lesser amount than a few X and also connotes an insufficiency. Also note that these words can be used in adjective as well as adverb form: substantial, substantially. While these words are commonly used to report the results, their meaning can be subjective and unclear. As Le Brun points out what is fast today is slow in tomorrows research (Lebrun, 2007). To avoid ambiguity, these words are typically combined with precise numerical values: a fast thristor is less clear than a 20Ghz thristor (Lebrun, 2007). Note some examples below: Inflation increased considerably during the second (12.7%) and third (11.6%) quarter of 2009. NOT: Inflation increased considerably during the second and third quarter of 2009. The enamel wear seen in protocol 2 was significantly higher than that observed for protocol 3, for both layers (t =5.737, p NOT: The enamel wear seen in protocol 2 was significantly higher than that observed for protocol 3, for both layers. Note that in English, unlike in Portuguese, adverbs are typically placed before the verb they modify: The respondents (13%) occasionally complained about noise from the nearby airport. However, when used to stress something, adverbs can follow after the verb. This pattern in English is also the opposite of Portuguese: The sleep of some respondents due to the noise was reduced considerably. While there is no simple formula for choosing appropriate words, you should be aware of the different meanings and connotations these words carry. Journal editors and referees often question the particular words chosen by an author to describe the results and data, so it is worth your trouble to study and be careful with words choice in describing data. Table 1 (Woops! By posting here, I lost the formatting and table columns. Hope you can make sense of this by noting the degrees represented by various minus (-) and plus signs (++++) Vocabulary Describing Degree Intensity 0-10% 10-40% 40-70% 70-90% 90-100% (-) never none empty ====== faint almost never barely dim few X hardly immaterial in short supply incidental inconsequential inconsiderable insignificant irrelevant lacking mere miniscule muted negligible pale paltry pitiful rarely scant scarcely seldom slightly subdued trifling trivial (+) a few X a smattering of at times as little as every so often from time to time in some measure infrequently intermittently minor now and then occasionally once in awhile on occasion partly periodically quite rather scattered some sometimes spasmodically sporadically to a lesser degree to a lesser degree to a point to lesser extent to name a few (++) a greater number of a large part a majority of customarily fairly common habitually moderately oftentimes predominant preponderance quite common rather common regularly routinely some sometimes somewhat the bulk of to a certain degree to some degree to some extent (+++) a great deal of a plethora of X a rash of X an abundance of X considerably copious amounts countless frequently greatly highly innumerable intensely major multitude of myriad notable noteworthy numerous often plentiful significantly strongly substantially to a great extent typically widely widespread worldwide (++++) absolutely accutely almost full blaringly completely entirely deeply exceedingly exceptionally extremely filled fully highly keenly inordinately markedly mostly nearly all overwhelmingly perfused profoundly rife with saturated sorely thoroughly totally utmost utterly vastly wholly without exception Exercise 3 Choose the correct words to complete the sentences below. 1. Phosphate run-off into nearby rivers from farming fertilizers increased (500-fold) ___________(considerably/hardly), resulting in a ________ (plethora/scarcity) of algae blooms in the pond. 2. Since the reception towers were blocked by hundreds miles of rainforest trees, only a _______ (faint/strong) signal could be heard. 3. A ___________ (negligible/) (.5%) number of study participants reported negative side effects from the drug. 4. The addition of gravel to the cement had little effect on the samples strength; likewise, the resistance was _______(hardly/considerably) affected. 5. A very ______(slight/marked) increase (.03) was recorded, but this increase _________ (fully/barely) had an affect. http://www.academicenglishsolutions.com/AES/home.html ]] http://www.academicenglishsolutions.com/AES/home.html]] Writing the discussion section of a research paper Outline Beginning the Discussion Middle of the Discussion Taking Credit For Accomplishments of the Study Indicating the Novelty of the Study Noting Similarity in Findings Noting Differences in Findings Explaining Why Certain Results Were Obtained Concluding the Discussion Discussing Limitations Possible Applications of the Findings A Call for Future Studies Concluding Statement Acknowledgements The quality of the writing of the discussion will heavily influence whether a journal accepts a paper for publication; consequently, great care should be taken in writing this section(Goldbort, R., 2006). While there are several common patterns typically followed in discussions, there are also a wide variety of forms. This lack of structure makes the discussion the most challenging section to write. One danger in such a broad undefined structure is that it can result in a rambling verbose discussion. Consequently, journal editors commonly find many unnecessary details in this section. While revising the English of hundreds of research papers, I have found the greatest number of stylistic and grammatical errors in the discussion section. Perhaps this is because authors struggle with the lack of structure as well as the complexities of interpreting and synthesizing their findings in relation to the larger field. Consequently, I suspect that the authors pay less attention to grammar and style issues in this chapter because they are overwhelmed by other issues. By using the common rhetorical patterns of the discussion reviewed in this chapter as a guide, you should be able to avoid some of the confusion and lack of structure of the discussion section. Before reviewing specific topics and patterns, some general features of discussions should be pointed out. One distinct feature of the discussion section is that it is characterized by a series of points , rather than facts as in the results section (Swales and Feaks, 2004). These points are interpretive rather than descriptive as in the factual reporting in the results section. Another general feature of the discussion section is that compared to the results section, it tends to be: more abstract, general, theoretical, related to the real world, concerned with practical applications and implications, as well as engaged in dialogue with the larger field of study (Swales and Feaks, 2004). The discussion section is where you will draw out, delineate and consolidate the meaning of the results, as well as tactfully dialogue with the broader community of your field and attempt to build community consensus on the meaning of your findings (Swales and Feaks, 2004) It should also be pointed out that while the result section will present more concrete and specific quantitative data, the discussion should present more general comments about the findings, introduced by formulaic phrases such as these: In general, we saw that... Overall, we observed that... On average, we found that.... By and large, we found that... On the whole, we found that... We found that X tends to... In addition to considering the general features of discussions mentioned above, it may be helpful to think of the discussion as a story, and like other clear and coherent stories, the discussion has a beginning, middle and end (Zeiger, 1999) In the beginning of the story, you will introduce the reader to your original purpose, and then name your major findings and present results that support these findings. In the middle, you may show how other work supports your findings; report unexpected results; report on the novelty of the your work; explain conflicting results; suggest why your results are the way they are; explain how results of other studies do not agree with your findings, as well as defending your proposed answer against other possible answers to your main research question. At the end of the story, you may also discuss the limits of the method and validity of the assumptions as well as describing applications or implications of your findings. There are also three common devices discussed in chapter 9 that can be used to give coherency to the various topics of the discussion. First you can guide the reader from one topic to the next by using topic sentences. Second, you can use subheadings to identify the general outline for the reader. Third, you can repeat key ideas to connect one paragraph to the next. See chapter 9 for more details about topic sentences, subheadings and repeating key ideas to create coherency. Beginning the Discussion There are three common patterns used to begin a discussion. The first pattern begins the discussion by addressing the statement of purpose or question that directed the study (Zieger, 1999). In this study, we determined that asymptomatic cigarette smokers exhale more H2O2 than healthy nonsmokers. We found that multiplex ELISAs are more precise, cost effective and labor saving compared to conventional ELISAs. The second pattern is to restate the original research question or statement of purpose, followed by the main findings. This study aimed to determine whether asymptomatic cigarette smokers exhale more H2O2 than healthy nonsmokers. We determined that asymptomatic cigarette smokers exhale more H2O2 than healthy nonsmokers The objective of this study is to compare results of conventional ELISAs with the results of multiplex ELISAs. We found that multiplex ELISAs are more precise, cost-effective and labor-saving compared to conventional ELISAs. A third common pattern includes a brief summary of the context of the research, followed by the findings. This pattern is preferred since it is less blunt than beginning with naming the findings in the first sentence. It is also quickly places the work in a larger meaningful context without the detail of the introduction. This can be as short as one sentence or two sentences at most. Previous studies have found that ...... However we found that.... If you choose one of these three popular patterns, you will need to follow by reporting the major findings and then immediately provide the results that support these findings. Note that major findings that you will list in the discussion are not the same as the results. The major findings will be a general summary and synthesis of the data. Findings are more general interpretations of results. The results will be a more elaborate set of data obtained from your experiments. You will need to report as much of your results that will justify your findings to the reader and no more than this. (Zieger, 1999) It should be mentioned that other less common strategies could also be used for beginning the discussion. Another pattern could be to restate the original research question or statement of purpose, followed by a brief summary of the method, followed by the findings. In addition to these common patterns, a wide range of patterns can be found in discussion sections of actual published papers, including a review of the literature; stating economic and social importance of the topic; and reflecting on the site of the study (Swales and Feaks, 2004). You may consider different ways to begin the discussion, depending on the nature of your paper and the results you found. You can choose a pattern that makes your discussion sound most compelling. For example, it may be important to discuss the method that you used because this method is what distinguishes your work from previous studies. Likewise, you may wish to focus on the site location of your study if it seems particularly relevant to your findings. The point is that whatever way you begin the discussion, you should choose a strategy that best suits presenting your unique work. However, the first two patterns mentioned above are the most commonly used and are considered highly effective since they clearly guide the reader by going directly to the purpose of the study, followed by the main findings. (Zieger, 1999) While you have a certain choice and flexibility in beginning the discussion, there are some strategies that you should avoid: do not create a second introduction, nor repeat what you should have reported in detail in the results. A second introduction can be confusing to the reader, as well as a waste of print space. The only case where you may consider writing another introduction would be when you reader may be confused by the complexity of the results. A brief introduction may help re-orient the reader in this case. The results should have been summarized in the results section. While a certain amount of repetition of the results is inevitable and even necessary, the discussion should restate results in general terms as an entry-point into interpreting and making some general points about the results. (Matthews, J. R., 2007). This beginning section may be presented, ideally, in as little as one paragraph. http://www.academicenglishsolutions.com/AES/home.html ]] @font-face { font-family: "Arial"; }@font-face { font-family: "Courier New"; }@font-face { font-family: "Times"; }@font-face { font-family: "Wingdings"; }@font-face { font-family: "Verdana"; }@font-face { font-family: "Tahoma"; }@font-face { font-family: "Gill Sans"; }@font-face { font-family: "MS Gothic"; }p.MsoNormal, li.MsoNormal, div.MsoNormal { margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: "Times New Roman"; }h1 { margin: 0in 0in 0.0001pt; page-break-after: avoid; font-size: 14pt; font-family: "Times New Roman"; font-style: italic; }h2 { margin: 0in 0in 0.0001pt; line-height: 150%; page-break-after: avoid; font-size: 16pt; font-family: "Times New Roman"; font-style: italic; }h3 { margin: 0in 0in 0.0001pt; page-break-after: avoid; font-size: 12pt; font-family: "Times New Roman"; }h4 { margin: 0in 0in 0.0001pt; line-height: 150%; page-break-after: avoid; font-size: 48pt; font-family: "Times New Roman"; font-style: italic; }h5 { margin: 0in 0in 0.0001pt; page-break-after: avoid; font-size: 14pt; font-family: "Times New Roman"; font-weight: normal; }h6 { margin: 0in 0in 0.0001pt; page-break-after: avoid; font-size: 24pt; font-family: "Times New Roman"; }p.MsoHeading7, li.MsoHeading7, div.MsoHeading7 { margin: 0in 0in 0.0001pt; page-break-after: avoid; font-size: 28pt; font-family: "Times New Roman"; font-weight: bold; }p.MsoHeading8, li.MsoHeading8, div.MsoHeading8 { margin: 0in 0in 0.0001pt; page-break-after: avoid; font-size: 10pt; font-family: "Times New Roman"; font-weight: bold; }p.MsoHeading9, li.MsoHeading9, div.MsoHeading9 { margin: 0in 0in 0.0001pt 0.25in; line-height: 150%; page-break-after: avoid; font-size: 20pt; font-family: "Times New Roman"; font-weight: bold; }p.MsoNormalIndent, li.MsoNormalIndent, div.MsoNormalIndent { margin: 0in 0in 0.0001pt 0.5in; font-size: 12pt; font-family: "Times New Roman"; }p.MsoFootnoteText, li.MsoFootnoteText, div.MsoFootnoteText { margin: 0in 0in 0.0001pt; font-size: 10pt; font-family: "Times New Roman"; }p.MsoCommentText, li.MsoCommentText, div.MsoCommentText { margin: 0in 0in 0.0001pt; font-size: 10pt; font-family: "Times New Roman"; }p.MsoHeader, li.MsoHeader, div.MsoHeader { margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: "Times New Roman"; }p.MsoFooter, li.MsoFooter, div.MsoFooter { margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: "Times New Roman"; }p.MsoCaption, li.MsoCaption, div.MsoCaption { margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: "Times New Roman"; font-weight: bold; }span.MsoFootnoteReference { vertical-align: super; }span.MsoCommentReference { }p.MsoEndnoteText, li.MsoEndnoteText, div.MsoEndnoteText { margin: 0in 0in 0.0001pt; text-align: justify; font-size: 10pt; font-family: "Times New Roman"; }p.MsoList, li.MsoList, div.MsoList { margin: 0in 0in 0.0001pt 0.25in; text-indent: -0.25in; font-size: 12pt; font-family: "Times New Roman"; }p.MsoListBullet, li.MsoListBullet, div.MsoListBullet { margin: 0in 0in 0.0001pt; text-align: justify; font-size: 10pt; font-family: "Times New Roman"; }p.MsoList2, li.MsoList2, div.MsoList2 { margin: 0in 0in 0.0001pt 0.5in; text-indent: -0.25in; font-size: 12pt; font-family: "Times New Roman"; }p.MsoListBullet2, li.MsoListBullet2, div.MsoListBullet2 { margin: 0in 0in 0.0001pt 0.5in; text-indent: -0.25in; font-size: 12pt; font-family: "Times New Roman"; }p.MsoTitle, li.MsoTitle, div.MsoTitle { margin: 0in 0in 0.0001pt; text-align: center; line-height: 200%; font-size: 12pt; font-family: "Times New Roman"; font-weight: bold; font-style: italic; }p.MsoBodyText, li.MsoBodyText, div.MsoBodyText { margin: 0in 0in 0.0001pt; font-size: 48pt; font-family: "Times New Roman"; font-weight: bold; font-style: italic; }p.MsoBodyTextIndent, li.MsoBodyTextIndent, div.MsoBodyTextIndent { margin: 0in 0in 0.0001pt; text-align: justify; text-indent: 27.75pt; font-size: 14pt; font-family: "Times New Roman"; font-weight: bold; }p.MsoBodyText2, li.MsoBodyText2, div.MsoBodyText2 { margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: "Times New Roman"; color: black; }p.MsoBodyText3, li.MsoBodyText3, div.MsoBodyText3 { margin: 0in 0in 0.0001pt; line-height: 150%; font-size: 12pt; font-family: "Times New Roman"; font-style: italic; }p.MsoBodyTextIndent2, li.MsoBodyTextIndent2, div.MsoBodyTextIndent2 { margin: 0in 0in 0.0001pt 28.35pt; font-size: 12pt; font-family: "Times New Roman"; }p.MsoBodyTextIndent3, li.MsoBodyTextIndent3, div.MsoBodyTextIndent3 { margin: 0in 0in 0.0001pt 0.25in; line-height: 150%; font-size: 12pt; font-family: "Times New Roman"; }a:link, span.MsoHyperlink { color: blue; text-decoration: underline; }a:visited, span.MsoHyperlinkFollowed { color: purple; text-decoration: underline; }strong { }em { }p.MsoDocumentMap, li.MsoDocumentMap, div.MsoDocumentMap { margin: 0in 0in 0.0001pt; background: none repeat scroll 0% 0% navy; font-size: 12pt; font-family: "Times New Roman"; }p.MsoCommentSubject, li.MsoCommentSubject, div.MsoCommentSubject { margin: 0in 0in 0.0001pt; font-size: 10pt; font-family: "Times New Roman"; font-weight: bold; }p.MsoAcetate, li.MsoAcetate, div.MsoAcetate { margin: 0in 0in 0.0001pt; font-size: 8pt; font-family: Tahoma; }span.Heading1Char { font-family: Arial; font-weight: bold; font-style: italic; }span.Heading2Char { font-family: Arial; font-weight: bold; font-style: italic; }span.Heading3Char { font-family: Times; font-weight: bold; }span.Heading4Char { font-family: Arial; font-weight: bold; font-style: italic; }span.Heading5Char { font-family: Arial; }span.Heading6Char { font-family: Times; font-weight: bold; }span.Heading7Char { font-family: Times; font-weight: bold; }span.Heading8Char { font-family: Arial; font-weight: bold; }span.Heading9Char { font-family: Times; font-weight: bold; }span.BodyTextIndentChar { font-family: Arial; font-weight: bold; }span.EndnoteTextChar { }span.BodyText2Char { font-family: Arial; color: black; }span.BodyTextIndent2Char { }span.BodyTextChar { font-family: Arial; font-weight: bold; font-style: italic; }span.FooterChar { }span.FootnoteTextChar { font-family: Courier; }span.BodyTextIndent3Char { font-family: Times; }span.BodyText3Char { font-family: Times; font-style: italic; }span.HeaderChar { font-family: Times; }p.Text, li.Text, div.Text { margin: 0in 0in 0.0001pt; text-align: justify; text-indent: 10.1pt; line-height: 105%; font-size: 10pt; font-family: "Times New Roman"; }span.DocumentMapChar { font-family: Helvetica; background: none repeat scroll 0% 0% navy; }span.cesartextoresposta1 { font-family: Verdana; color: black; font-weight: normal; }span.TitleChar { font-family: Arial; font-weight: bold; font-style: italic; }p.Byline, li.Byline, div.Byline { margin: 0in 0in 0.0001pt; font-size: 48pt; font-family: "Times New Roman"; font-weight: bold; font-style: italic; }span.CommentTextChar { font-family: Times; }span.CommentSubjectChar { font-family: Times; font-weight: bold; }span.BalloonTextChar { font-family: Tahoma; }div.Section1 { page: Section1; }ol { margin-bottom: 0in; }ul { margin-bottom: 0in; } Middle of the Discussion The beginning of the discussion includes renaming initial research questions and stating the main findings and naming the results that support your findings followed by the middle section of the discussion. The organization of this middle section is typically organized from most to least important topics. This section will take up the great majority of the discussion. Below we will consider several possible topics that may be relevant to your discussion. As in the beginning of the discussion, several of these topics are optional: the nature of your paper and the findings will determine whether you need to address these topics, as well as the degree of importance of each topic. Taking Credit For Accomplishments of the Study The first topic we will consider is taking credit for the accomplishments of your study, followed by an in depth consideration of hedging. Often one sentence can summarize the accomplishments of a study: In this study , we showed that postnatal neuroretinas contained retinal stem cells which are part of the radial glia population. Here we show that over the past 20 years, all the trends in the Sun that could have had an in?uence on the Earths climate have been in the opposite direction to that required to explain the observed rise in global mean temperatures. Since accomplishments commonly add to what has been extensively studied in your field, it is also common to acknowledge and cite the accomplishments of previous studies . Indicating the Novelty the Study It is also optional to point out the novelty of the study. This should have been done in the introduction where you indicate how your study fills a neglected niche in your area of study. If you want to mention this again, you may name what has been done previously and note how your work adds to this. You may have seen this phrase commonly used to introduce the novelty of a study: For the first time we report that... Some guidebooks advise against using this type of sentence since it is possible that the same or similar research exists in the literature in another country in English or some other language. If however you feel the need to make such a claim, you may choose a more cautious hedged version of this sentence: To the best of our knowledge, this is the first study to... Additionally, the following phrases can be used to indicate that your findings are a novel solution to a long-standing problem: These results provide insight into the issue of X. These results further explicate the issue of X. These findings shed light on the issue of X. These findings cast light on the issue of X. In order to report very dramatic solutions to former problems in the field, you may use the following phrases and sentences. However, read the section on hedging in the discussion section that follows. Use these types of sentences carefully and with discretion: These are groundbreaking findings in regards to the origins of X. These findings represent a sea change in regards to the origins of X. These findings overturn the conventional paradigm explaining the origin of X. Other phrases can also be used to express that a new direction can be taken after considering the new findings: In light of these findings, it seems reasonable to modify our approach to X. Considering these findings it seems reasonable to modify our approach to X. Taking into account these findings it seems reasonable to modify our approach to X. In view of these findings, it seems reasonable to modify our approach to X. Do not literally translate similar expressions from Portuguese: Based on these findings, it seems reasonable to modify our approach to X. NOT: With base in these findings, it seems reasonable to modify our approach to X. NOT: Based these findings, it seems reasonable to modify our approach to X. Consequently , it seems reasonable to modify our approach to X. In this regard, it seems reasonable to modify our approach to X. In this respect , it seems reasonable to modify our approach to X. NOT: In this way, it seems reasonable to modify our approach to X. NOT: In this line... NOT: In this perspective... Exercise 1 Revise the sentences below involving phrases used to highlight. 1. This brings up a whole bunch of questions about the defective mechanism. 2. These findings point the need for further understanding of X. 3. These findings show why its important to look at XYZ. Noting Similarities in Findings As mentioned above, taking credit for your accomplishments, may involve reporting studies with similar findings as yours. When discussing findings that are similar to those of yours, it is typical to use sentences like these: These findings are consistent with other studies. (Farias, 2007) These results agree with those of other researchers. (1, 2, 5, 7) Our results were similar to those of Olivera et al. These results concur with other studies. (6, 9) We share the view of Smith and Jones in regards to this issue. Regarding our results, we subscribe to the common belief that X causes Y. Our results confirmed the findings of Wilson et al. (5) Turner and colleagues suggest that X causes Y, likewise or findings suggest the same. However, do not literally translate the typical phrases used in Portuguese to express this idea of agreement because they are not correct in English: These findings are consistent with other studies. (1, 2, 5, 7) NOT: These findings according with other studies. NOT: These findings are in concordance to other studies. NOT: These findings collaborate to other studies. NOT: These findings combine to other studies. Note that the verbs collaborate, concordance, combine and may look like certain words in Portuguese verbs, but in English their meaning is different. See the section on false cognates in chapter 3 for more information on this topic. Note that certain comparative words such as when and as are unnecessary and can be omitted in comparisons: X was more effective than Y in removing the contaminants. X was more effective in removing the contaminants compared to Y. NOT: X was more effective in removing the contaminants when compared to Y. NOT: X was more effective in removing the contaminants as compared to Y. Also be careful not to misuse the verb observe when reporting results. You can obtain results but not observe results: We obtained similar results as Martinelli et al. NOT: W e observed similar results as Martinelli et al. Noting Differences in Findings In addition to reporting on studies with similar findings as yours, you may also be a need to discuss differences of your findings with other studies. A good strategy would involve acknowledging the findings of other studies while explaining why your finds are more accurate or appropriate: Although Smith et al. report that this adhesive was sufficiently hydrophobic, our findings indicate otherwise. There are several possible explanations for this. There are several ways to express differences, discrepancies or disagreements. First, simply add the word not to the sentences above: Our results were not consistent with other studies (1,3,5). Our results did not concur with other studies (1,3,5). Alternately, consider these sentences for expressing differences: Our findings are at variance with Jones et al. Our findings differed from Jones et al. (1,3,5). We found that x cause y, as opposed to Jones et al (1999). Albeit Preston and others found that x caused y, our results suggest the contrary. Even though it is widely believed that x causes y; nevertheless our findings offer a compelling alternative. These findings raise the issue as to whether Farias is correct in claiming that X causes Y. Our results raise a question as to whether Farias is correct in claiming that X causes Y. (Note the similarity to the Portuguese: Gera uma pergunta) Our results call into question Fariass claim that X causes Y. Our results put into question Fariass claim that X causes Y. (less formal) Our results raised doubts about Fariass claim that X causes Y. Be careful with common phrases that have different meaning, but look similar. For example, in contrast is probably the most commonly used phrase in research papers to point out differences in findings: In contrast to Smith et al., we found this method to be effective. Contrary to means conflicting with or counter to and it often appears as a collocation as in the boldface words below: Contrary to popular belief, pre-school children do not seem to be as physically active as previously thought. On (or quite ) the contrary is used to intensify a denial of what has just been implied or stated: Despite the extra-support, students performed not better than the control. On the contrary, roughly 23% performed even worse. To the contrary with the opposite meaning or implication: Despite medical advice to the contrary , many people believed this homeopathic medicine could help them. In chapter 7, we recommended using the active voice, especially in case where the passive voice is vague or confusing. In the discussion section in general where it is common to interpret findings, and especially in cases where you are disagreeing with the findings of others, you can use the active voice and the subject pronoun we to indicate clearly and strongly that this is the opinion of your group and not a universally accepted truth. Note this pattern below: Notwithstanding the findings of previous studies, we propose conversely that x causes y. NOT: Notwithstanding the findings of previous studies, it has been shown conversely that x causes y. Notice some other examples of formulaic sentences using the subject pronoun we and the active voice: Contrary to Jones et al, we argue this outcome is more precise. As opposed to Jones et al, we argue that... Some authors have told me that they were taught early on in their writing career to never use the pronoun we in scientific papers . While there may be several places in a research paper such as the methods section where the passive voice is prescribed, the active voice can, and sometimes should, be used throughout the text, particularly in the introduction, results, and discussion. For example, high impact journals such as Science and Nature prescribe using the active voice whenever possible to be more direct and clear. Explaining Why Certain Results Were Obtained Another possible topic for the discussion is reporting why certain results were obtained. This may include explaining unexpected findings, which can be major or minor. This could involve a certain amount of speculation as to why you obtained these unexpected findings: The lack of relation between infertility and cocaine abuse were unexpected. We propose three possible explanations. First... You may also need to name results obtained that do not support your main finding. This may involve showing how these results in fact may not conflict with your main findings. Sometimes a direct and simple declaration of inexplicable findings is appropriate: We simply do not have a good explanation for having found poorer soils at 300 m altitude on less steep slopes of the Coastal Atlantic Forest. http://www.academicenglishsolutions.com/AES/home.html ]] @font-face { font-family: "Arial"; }@font-face { font-family: "Times"; }@font-face { font-family: "Verdana"; }@font-face { font-family: "Verdana"; }@font-face { font-family: "Gill Sans"; }@font-face { font-family: "Times-Roman"; }p.MsoNormal, li.MsoNormal, div.MsoNormal { margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: Times; }p.MsoFooter, li.MsoFooter, div.MsoFooter { margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: "Times New Roman"; }p.MsoEndnoteText, li.MsoEndnoteText, div.MsoEndnoteText { margin: 0in 0in 0.0001pt; text-align: justify; font-size: 10pt; font-family: "Times New Roman"; }p.MsoBodyTextIndent, li.MsoBodyTextIndent, div.MsoBodyTextIndent { margin: 0in 0in 0.0001pt; text-align: justify; text-indent: 27.75pt; font-size: 14pt; font-family: Arial; font-weight: bold; }p.MsoBodyText2, li.MsoBodyText2, div.MsoBodyText2 { margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: Arial; color: black; }span.BodyTextIndentChar { font-family: Arial; font-weight: bold; }span.EndnoteTextChar { }span.BodyText2Char { font-family: Arial; color: black; }span.FooterChar { }.MsoChpDefault { font-size: 10pt; }div.WordSection1 { page: WordSection1; }ol { margin-bottom: 0in; }ul { margin-bottom: 0in; } Concluding the Discussion The great majority of the discussion section should be devoted to interpreting the meaning of the results, which also involves strengthening these interpretations by synthesizing them with other related studies from the field that support them. After completing this task, it is common to conclude the discussion. It is recommended that you do not abruptly end the discussion by simply calling for further studies (Zeiger, 1999). A stronger conclusion could also mention limitations of the study; speculate on implications for possible future applications and finally, make specific recommendations related to future studies, as well as restating the major findings of the study. Discussing Limitations Discussing the limits of the studies method or the validity of the results is optional, but a fairly common practice. Although you may think that discussing the limitations of your study is counter-productive to the overall goal of consolidating your interpretation of the results, there are instances where acknowledging the obvious limits of a study could deflate criticisms of the reader and journal editor and ultimately make your claims more compelling (Swales, 2004). If you choose to name limitations, consider focusing on what can be concluded from the data and not the overall weakness of study as in the example below: The positive effects attributed to the dance/movement therapy in this case study of an Multiple Sclerosis patient may be viewed with caution. Some implications of the findings of this single study may not be generalized to a broader cross section of this population . Comparable investigations using an equivalent methodology must be undertaken, with a large sample of MS patients, before more conclusive inferences can be made. Imagine some of the most critical objections your reader may have, and then thoroughly addressing them, is a very powerful strategy that may work to deflate much of the criticism of your work. If you are so close to your work you may sometimes have trouble imagining criticism, yet it can help if you imagine yourself as a reader who is invested in a different outcome than what your paper implies. What might be their response? You can consider at least three general types of objections. ( Alley, M. 1996) . First, is your paper focused on a single cause-effect relation? Consider addressing or at least acknowledging other research that considers other possible causes for your topic. Second, are there other counterexamples to your thesis in the literature? Consider acknowledging them. Third, is it possible that the reader may question the definition of the terms of your work? Consider addressing this as well. See chapter 14 on Qualifying language, more specifically concession clauses. Such acknowledgment of another perspective might be as simple as the sentence below: Notwithstanding contrary evidence of other studies, we conclude that X plays a role in the development of Y. Although other researches propose alternative mixtures, we chose X as the most representative sample for this task. Possible Applications of the Findings Another optional topic for the conclusion of the discussion is speculating about applications of the findings. Note how the boldface words are qualified and modest: These results may have practical applications for algorithms involving large networks of low-performance hardware equipped with wireless communication. Despite the minor limits of this study, we believe our findings may contribute to innovations in the field of industrial recycling of concrete slag waste. A Call for Future Studies Another optional topic for the conclusion involves calling for future studies. This is also a hedging strategy that acknowledges the limits of the study and proposes the need for further studies. Such strategies typically look like these examples: These shortcomings of our study suggest the need to delve deeper into this issue in future studies . Further studies will be necessary to determine whether long-term Zanokrik use may indeed reduce the risk of endometrial carcinoma. Future studies will be necessary to determine the actual reasons for the recent decline in annual SIB mortality. More studies are needed to determine whether these unexpected positive effects are due to a placebo effect. These ambiguous elements in our findings will need to be investigated further in future studies. Women with advanced maternal age have an increased risk of stillbirth, but further studies are needed to determine the causes. More data are required to determine if these results are unique to this particular group of participants. These findings only highlight the need for further studies. Do not confuse farther and further in this type of sentence: farther refers to greater concrete measurable distance, whereas further refers to a greater degree in regards to abstract concepts. See chapter 7 on commonly confused words for further explanation and examples. A call for future studies that names specific recommendations will be more interesting and compelling than a general one: General: Future studies will be needed to further understand this drug. Specific: In regards to future research , well-conducted randomized controlled trials will be essential to establish the effectiveness of Serizone in depressed adolescents. Concluding Statement Ending the paper with a call for future studies can be somewhat dull and anti-climatic. A stronger ending would rename the main finding. As in the examples below: Overall, people who consumed high amounts of saturated fats had 36% higher relative rates of pancreatic cancer compared with those who consumed low amounts (BBC News, 2009) In conclusion, we found an easy method for the synthesis of compounds derived from 1,2,4-triazoles with good performance through the use of agents such as EDC and HOBT. The operational simplicity of this method and the good yield of the products make it valuable. Additionally, this reaction eliminates the use of toxic reagents such as lead tetraacetate or phosphorus oxychloride with each reaction. We propose a procedure that can be used in the synthesis of new compounds with pharmacological activity ( Gildardo Rivera , 2009). Note that the concluding statement is often a single statement summarizing the main findings of your study. Miller recommends that the concluding statements are as general as possible, paraphrasing and simplyfing the specific numerical data and technical language that may have been used in the methods or analytic sections of the text, providing the reader with the most general outline of the work, as well as connecting to more real world applications and considerations (Miller, J. E., 2004). You can use some of the formulaic phrases below to form your own concluding statement. Our findings highlight the importance of.... In conclusion, we found that... In summary, we found that... Overall, we conclude that... Acknowledgements The acknowledgement section is typically used to thank anyone who made a meaningful or substantial contribution to the paper. In the acknowledgments, you may wish to thank a sponsor such as FAPESP or CNPq for providing funding. You may also wish to thank other researchers, technicians, lab mates, or anyone who contributed to your paper. This may include someone who loaned you equipment, provided critical feedback on the paper, or even revised the English of the text. You should read your target journals guideline for authors for specifications for writing the acknowledgments. You may find some guidelines like these: The Acknowledgments page: Include brief statements about granting agencies, important aid received from institutions, and any potential conflicts of interest (as detailed in the LO Ethics statement section 3.4 and 3.4.1). Thank anyone who made a substantial contribution to the work (e.g., data collection, analysis, or writing or editing assistance) but who did not fulfill the authorship criteria, along with their specific contributions. You are responsible for ensuring that all persons named in the Acknowledgments section know and agree to being identified there (since it may be interpreted as endorsement of the data or conclusions). If the journal does not provide guidelines, you might read acknowledgements from other papers for ideas on how to write yours or you may consider using some of the common formulaic sentences found in this acknowledgement: We wish to thank Professors Ted E. Bear and A. Nony Mous for their careful reading and feedback on the paper. We also want to thank Dr. H. Simpson for assisting with DNA sequencing and bioinformatics analysis. We also want acknowledge that this work would not be possible without the generous funding and support of FAPESP (The State of Sao Paulo Research Foundation). Finally, we wish to thank Jim Hesson of Academic English Solutions for revising the English. Exercise 2 In this chapter, we have reviewed numerous formulaic sentences that are commonly used in a research paper. For practice in using these phrases correctly, try correcting the various errors below involving formulaic phrases from research papers. 1. Some economists have pointed that low inflation is not necessarily a good thing. 2. Although to a low extent, the disruption was related to worker experience. 3. Technical advances in the last years have drastically improved the efficiency and capability for storing food. 4. Pesticides have been used everywhere more and more. 5. These conductors have a role on the final outcome. 6. It remains unknown whether they have an effect in skin cancer. 7. The former were feed a supplement in their diet, while the ladder were not. 8. The present evidences support a role of polyamines in X-disease. 9. With relation to X, we found that... 10. In the last decades, numerous studies have shown that x causes y 11. One of such factors is the influences of weathers fluctuations. 12. TRIR spectroscopy is especially sensitive to compounds with carbonyl ligands and permits an insight into the problem of X. 13. As said before, these findings are not consistent with the literature. 14. Our findings put a little more light on this issue. 15. The problem of X needs a deep understanding of Y. This chapter reviewed some typical rhetorical conventions of the discussion section of research papers. If you have done the exercises and studied carefully, you should notice an improvement in your ability to write research papers in English. If you have read through quickly, you may want to go back and review whatever sections you did not study carefully. While everything you have read here, may not be applicable to your field of study or writing. In that case, take what is useful and simply disregard the rest. In time, hopefully writing will even become a pleasurable experience. If you are going to make research or academia your profession, ultimately, you a great deal of your time will be devoted to writing, particularly in English. Hopefully the lessons you learned here will serve you while writing your own papers. Enjoy the adventure and good luck with publishing! http://www.academicenglishsolutions.com/AES/home.html ]] http://www.academicenglishsolutions.com/AES/home.html http://www.academicenglishsolutions.com/AES/home.html ]] Tips for Responding to an Editor's Request for a Revision of a Manuscript What do you do when you submit your paper to a journal, and you receive a letter asking you to revise and resubmit it? As a copyeditor, I have read a fair amount of these messages to various authors. And sometimes the authors of the reviewed paper complain quite bitterly that the editors/referees are being outlandish and unfair in the criticism. Admittedly, an occasional referee may be too harsh, critical, arrogant or unprofessional in the tone of their message. Why w ould they do this? Who knows why people in general may do rude things, no less a journal editor or referee who should show professional etiquette. Yet more importantly, I feel your response to such messages should take the higher road, being generous, calm and diplomatic as possible, and not stooping to the same level by writing back angry insulting comments in response. I recommend a cautious diplomatic response because your ultimate goal is to get the paper published, and if they are just asking you to make some reasonable changes, it is not worth responding to the rude comments of the referees. Another reason I recommend a moderate diplomatic response, is because I have also seen situations where the authors respond to what they perceived as an insult or slight by the referee, but when I read the message of the referee, I saw no such slight. Perhaps it is easy to misread an insult where there is none if English is not your native tongue. Also, I think this may occur because authors are very close to their research, having put their blood, sweat, and tears into the work, so when they hear criticism of something they are generally pleased with, if not quite proud of, they may be overly sensitive when they expected a more positive reception from the editor, and so they may not be able to hear nuance in the criticism, and instead of hearing the positive, they hear only the negative, which is all perfectly natural and understandable. I have seen some cases where authors are quite defensive about a studys shortcomings pointed out by the referee, yet the authors will assume it is a nationalistic differences between the authors and the referees. I have heard many accusations about referees from developed nations not respecting the research from certain developing nations. While there may be some truths to these claims, I think there is also quite a lot of defensive reactivity when authors receive critical comments. As Swales and Feak recommend, it may be best to just read the comments, and let some time pass before responding. Not much is too be gained by a vitriolic response. Most of these referees are volunteering to do this work, so I think they mostly have good intentions, yet perhaps they may not realize their tone is overly harsh or possibly arrogant. I think sometimes it is most difficult for a junior researcher who may be submitting their first research paper, and they may have inadvertently brought unrealistic expectations, believing their work is beyond criticism and without problems. In most cases journal editors can be quite positive about a paper, but they seem to be nitpicking when they go on and on for pages pointing out every minor error they see. Although this may be tough on some egos, ultimately, I think they have the best intentions and want the final research to meet high standards. You can even see such attention to detailed criticism as a compliment ultimately, since they are showing a certain care and concern. And even in cases where ultimately the comments of are referee are entirely irrelevant, incorrect, impolite, biased and prejudices, I still recommend using cautious comments like this in response: While we have fully considered referee Bs comments about X, we respectfully disagree and wish to refer him to recent studies demonstrating X. OR: We politely beg to differ with referee As criticism of our method. We wish to note that our method is fully consistent with the literature on this method. Here are some general guidelines for corresponding with the journal editor quoted directly from Navigating Academia: Writing Supporting Genres (John M. Swales, Christine Feak): 1. Remember that an invitation to revise is usually a positive sign. So do not take criticisms personally. 2. Read the editors letter and reviewers comments carefully. 3. If the editor suggests getting help with the English, choose someone who has some understanding of you research area as well as a good knowledge of the language. 4. Respond to each of the major comments. (Minor ones such as spelling corrections or corrections to references do not need detailed commentary). 5. In your response, help the editor by using detailed references to the text such as p. 2 first para. 6. Thank people for useful suggestions but do not automatically defer to the editor or reviewers. If you disagree with a comment, explain why. 7. If you have made additional changes not suggested by the reviewers briefly explain what they are and why you have made them. 8. If you have been asked to revise and resubmit, do so as quickly as convenient. 9. Explain what you are doing about any page charges or fees, if this is appropriate. 10. If you do not plan to revise, inform the editor of you decision. Other issues to think about include, careful consideration of what the editors want you to change, and whether you chances of being accepted for publication are good after these changes are made. As Annesley puts it, "...Some rejection letters (Example 2) offer an opportunity to resubmit. You still have your foot in the door, but you need to carefully consider whether there is a realistic chance that you can improve the paper to the reviewers' satisfaction. After finding numerous deficiencies, reviewers sometimes stop providing comments because their recommendations are clear by the time they are partway through the paper. If you decide to resubmit, it is possible that (1) the reviewers already have a poisoned view of the work and (2) you will receive additional criticisms of your work when the reviewers look at other parts of the paper they did not read carefully during the first round of reviews. The third example is also a revise-and-resubmit letter, but it tells you that the paper should be acceptable after you have satisfactorily responded to the reviewers' comments. In that case, it is in your best interests to improve the paper and send it back with minimal delay." Another point to consider is what issues are worth defending and insisting upon. Again, as Annesley points out, "If your scientific paper is typical, the reviewers will ask you to make more than one modification. Some changes you will agree are worthwhile, some you will think are irrelevant, and some you will disagree with. Even if you do not fully agree with the reviewers on some points, you need to choose your battles wisely. If a change to a sentence or paragraph requested by the reviewer does not affect the intended meaning, do your best to make the change. It does not hurt you and sends the message that you took their suggestions seriously; however, if you believe that a requested change will negatively affect the paper, go ahead and respectfully disagree. It is your name on the title page. But do not respond by stating that the reviewer is wrong without allowing the reviewer, wherever possible, to save face. Explain where the reviewer may have misinterpreted the section and that you want to keep the text intact. You might find, however, that as you explain the rationale for keeping the text as is, some of the wording and logic you use to respond to the reviewer might be worth adding to the paragraph in question to help the reader better understand the paper." From: http://www.clinchem.org/ content/57/4/551.full For further consideration also see: http:// eprints.nottingham.ac.uk/859/2/ How_to_reply_to _referees.pdf If you the journal editors request a review of the English of your paper, consider our service which provides a full guarantee that your paper will not be rejected due to the English: http:// www.academicenglishsolutions.co m/AES/Editing.html Finally, I am very curious to hear your experiences and suggestions about responding to editors requests to have your papers revised. Do you have any advice for the readers's here? http://www.academicenglishsolutions.com/AES/home.html ]] I am sharing an amusing series of Tweets called Overly Honest Methods. This post may be particularly funny for people in the natural and physical sciences, but there may be a little humor in there for everyone. Hope you enjoy: https://twitter.com/search?q=OverlyHonestMethods http://www.academicenglishsolutions.com/AES/home.html ]] If you are applying for a science job in the US, you should follow certain conventions employers expect from applicants. Although I usually give advice on writing scientific manuscripts, I have been getting queries lately on resumes and cover letters. I am not familiar with conventions for applying for jobs in other countries, but I have seen people applying for jobs in the US without using a cover letter. If you simply send a resume for a job without a cover letter, it is highly unlikely that an employer will respond to you. Introducing yourself with a cover letter is standard practice in the US. If you are not sure about the form of the cover letter, you will need to write a letter in which you point out 2-3 things in your resume that are relevant to the job you are applying to. Also, point out 2-3 things in your personality or educational history that is relevant to this job. You can use phrases like, "As can be seen in my resume, I worked for 2 years as a....".  The cover letter serves as a bridge to connect your general lists of skills and experience found on your resume by pointing out how you are particularly suited for a certain job. While it's critical to use a cover letter to point out your skills, work history and expertise, I also believe it is critical to mention personality features that make you suitable for the job. You may be surprised to know that many companies and HR directors are very interested in your personality and whether it will fit in with their team. While it's common for all applicants to have impressive technical skills, not everyone has the right personality for particular companies. Most companies can train you on the job if your skills don't fully match, but changing a personality is much trickier. So if you have the right personality, point this out too. Companies are fearful of hiring someone who is difficult, someone who may make others dread coming to work, so you should reassure them of how your personality is right for the job. Phrases like "I work well in teams", "I am considerate of others", "I am a good team player", "I work hard to be respectful and get along with my colleagues", etc., can be helpful. I should also mention that e rrors in cover letters or CVs, no matter how minor they may seem, are considered unprofessional; consequently, I would polish and improve your CV as much as possible. For expert advice on writing a cover letter for a job in science, I recommend Writing a Winning Cover Letter by John K Borchardt on Science Careers from Science Magazine. Below is an excerpt: The Match "An effective cover letter doesn't just emphasize your best qualities; it also shows how well those qualities are likely to mesh with the open position. Applicants should begin by reading advertisements for faculty positions carefully and be sure that their background and goals are appropriate for the position in question. You lose credibility if you can't make a case that you fit the ad, says Whitmire.If the cover letter is to be effective, it must definitely be tailored to the particular institution. There's no excuse for not writing a cover letter that shows how your education, experience, and interests fit with what the institution is seeking, warns Julia Miller Vick, co-author of the Academic Job Search Handbook (University of Pennsylvania Press, July 2001). Not doing this would reflect laziness, observes Horvitz. At best, adds Vick, a form letter or one that is generic doesn't accomplish much and leaves how the application is reviewed completely up to the reviewing committee." At worst, a generic cover letter can make you seem undesirable. While many people applying for academic positions tend to think that the review process is an evaluation of their previous work--how good is it?--the issue that is as important is the match," says Whitmire. "How will this person fit in here? The former is necessary, but the decision to interview will often be made upon research area or some other measure of fit to the department's needs at that moment in time. For the complete article click on the link below: http://sciencecareers.sciencemag.org/career_magazine/previous_issues/articles/2006_10_27/nodoi.10341484552738357724 For writing resumes (CVs) consider Tips for a Successful CV by Sarah A. Webb, (October 27, 2006) also from Science Careers from Science Magazine: Here is an excerpt: Basic CV do's and don'ts (for U.S. audiences)* Do: Include all relevant contact information: address, e-mail address, phone number, cell phone number. Include a comprehensive listing of professional experiences. Proofread for correct spelling and grammar. Use relatively simple and consistent formatting. Organize the document so that a reader can find important information quickly. Use reverse chronological order throughout the document. List all publications in the same reference style. Be sure references are up-to-date and that your references know about jobs you're applying for. Get feedback from colleagues, mentors, and career advisers. Don't: Use complicated formatting. Include certain personal information such as your Social Security number, date of birth, marital status, or number of children. Include information about unrelated hobbies or interests that doesn't show professional experience or qualifications. Pad your CV with extra words or spacing to make it appear longer. *Standards vary elsewhere, so get local advice. For the complete story click on the link below: http://sciencecareers.sciencemag.org/career_magazine/previous_issues/articles/2006_10_27/nodoi.10341484552738357724 Also simply do a search online for "form of cover letter" for more information on this topic. If you need a revision of your cover letter, resume, research paper or dissertation, check out our website for more information about our editing service or online classes by Skype: http://www.academicenglishsolutions.com/AES/home.html Also keep in touch,  you can add us as a connection on Linkedin by submitting this email address: jim@AcademicEnglishSolutions.com http://www.academicenglishsolutions.com/AES/home.html ]] 1. ARRANGE FOR HOUSING Make arrangements to either move directly into housing (dormitory, apartment, sublet) or live somewhere temporarily, such as a hotel, or apartment sublet, while you search for permanent housing. Most universities provide temporary housing, but you need to make arrangements with them before you come. Contact the international student adviser office by email to get help in advance. If you plan to rent off-campus, explore the neighborhood before you rent. Is it safe? is it close to your classes and lab? Close to public transportation? Is it served by the university's own system? If you can't check these things out in-person, try to learn as much as possible from websites. For example, you can access transportation maps online, as well as maps that display high crime rate areas of cities and towns. Also look for other nearby conveniences such as a laundromat, post office, supermarket, convenience stores, banks, parks, libraries, coffee shops, or other services you may want to live near. Google map typically displays business and buildings located in a certain area. Yelp is an extremely useful website ( http://www.yelp.com ) that can be used to find any type of business in a particular area, and it can be used to find directions. You can find excellent reviews on Yelp by customers, which can give you a quick idea about whether you want to shop in a particular store or eat in a particular restaurant.  Craigslist is a great means of finding housing. If you have never heard of it in your country, you should know that it is a very popular website and service used in the US.  If you plan to stay less than a year, search for "sublets" or "short-term housing" 2. MEET WITH THE INTERNATIONAL STUDENT ADVISER Schedule an appointment after you arrive. Come prepared. Bring list of questions and ask for whatever information you need. Ask when the next student orientation sessions will be held. There will be three different sessions worth attending: -- One for all new students -- One especially for international students, and -- One to show all new students how to use the library. For graduate students, there will be mandatory academic orientation for you department, division, or school. Some orientation sessions are optional, but you should attend all you can. They are valuable and will save you time and effort later. 3. GET YOUR UNIVERSITY ID AND E-MAIL ACCOUNT Be sure to check your university e-mail account regularly. It is how faculty, staff and advisers will contact you for all university matters. if you maintain other e-mail accounts, such as those with Gmail or Hotmail, then you may wishy to forward their mail to your university account or your university mail to them. Whatever you choose, you should have a regular, easy way to check your official university account. 4. SET UP A LOCAL CELL-PHONE ACCOUNT Having a phone as soon as possible will be a great convenience that will allow you to make calls in your apartment search, as well as call utility companies (electric, gas, heat, internet, etc. See #7 below).  You will also need a phone to record names and numbers of new contacts (students, professors, adviser, administrators, etc.) as well as to give your number to others. So many business transactions are done by phone in the US, rather than by visiting some office. So this is important to take care of as soon as possible. Carefully check the different plans various phone companies offer; they vary greatly. Also, if you have a phone from your country, it will likely not function in the US as the technical specifications differ. Also, once you get a computer and internet account, you may want to use Skype for free international chats with your family and friends back home. 5. GET a SSIN, SSN, or ITIN. If you are not eligible to apply for a Social Security Number (SSN) as most international students are not, you are eligible to apply for an Individual Taxpayer Identification number (ITIN). An ITIN cannot be used like the SSN to apply for work, but it can be used as an identification card which will be helpful on rental applications, setting up a bank account, and other financial accounts. Here is the website for the Individual Taxpayer Identification number (ITIN): http://www.irs.gov/Individuals/Revised-Application-Standards-for-ITINs Here is a link to the online application: http://www.irs.gov/file_source/pub/irs-pdf/fw7.pdf 6. SET UP A LOCAL BANK ACCOUNT You will need an ID to open an account. You can use your ID from the university you will attend. You can also use an Individual Taxpayer Identification number mentioned above. You can also use a driver's license. Consider getting an international driver's license before you come to the US. Depsoit cash or traveler's checks in your new account so you have funds available immediately in your account. Your banks should treat a deposit of traveler's checks the same as a deposit of cash. That's why you should bring $2,000 or more in such checks, unless you can use a credit card or bankcard from home to withdraw that amount from a local teller machine (ATM, ABM) After you get your account, you can order personalized checks You can also order an ATM card for withdrawing cash from an automated teller machine.  ATM is the typical name used to refer to these machines located near business for quick cash withdrawals. You can also get the routing number from your bank, which is required for international wire transfers. If your funding agency in your native country intends to send you money, you will need this number to complete the transaction. This is critical for your regular survival, paying bills, purchasing food, paying rent, etc., etc. You can also purchase a debit card from your bank. 7. SET UP UTILITY ACCOUNTS FOR YOUR NEW APARTMENT. Utilites are all the services including electric, gas, water, phone, internet, TV and other services you will need in your new home. After you sign a lease to secure an apartment, you will need to call each utility company to get service in your name. You will need to let them know the date that you wish service to start. If you delay in this process, you may have no service (lights, electric, heat, phone, etc) for several days which can be quite challenging. In some cases, your new apartment may include the cost of utilities in the rent, but you should find this out as soon as possible. Typically, water or trash collection services are included in the rental fee, but not electricity or heat. Utility companies may require that you give a "security deposit" (money which can be used by the company if you decided to leave without paying). This is common if you don't have a long credit history in the US. Also, you will want to call the utility companies even before you move into your new apartment, and schedule that service be turned for the first day that you move into your new place. 8. MAKE A LIST OF EMERGENCY PHONE NUMBERS. You may want to store number in a notebook or phone under "in case of emergency" in case you were found unconscious and unable to communicate with a doctor or police. You can place numbers of your family and friends there. You may also want to have numbers to your consulate, university police, international student adviser, your doctor, your adviser, emergency numbers to your utility company (phone, electric) as well as bank. You can print out this list and keep in a place that is easily accessible. The general number for most emergencies is 911. This will connect you with the police, fire department, and ambulance service. 9. APPLY FOR A CREDIT CARD Sometimes it is difficult to get a credit card from major credit card companies in the US for international students who don't have a long credit history. In this case, it may be easier for someone to get card from Target, Walmart, a gas station such as Exxon. These cards can only be used in these stores, but once you establish good credit, you can then apply for a major credit card. A debit card is another option since it is easy to get from your bank. Unlike a credit card, it takes money directly from your bank account, but can be used like a credit card. One drawback is that they do not build your credit history. 10. TOUR YOUR NEW SCHOOL AND CITY Ask in the office of the international students if the school provides tours. These are quite common. You can also ask if they have self-guided tours and maps if you don't want to go with a group. Ask about safety issues if you school is located in an area with crime. Learn about the local supermarkets and shopping options. Keep and open mind about the food you find. While you may not find foods like those from your country, if you are flexible, you may find that they local food is of good quality and very tasty as well. Many practices that may be common in your country may be different in the US, so be attentive and again, be flexible. Learn about the local restaurants, post office, pharmacy, laundry mat, tailor, dry cleaner, shoe repair, clothing stores, computer, TV, stereo and electronic shops, public library, museums, parks, pools, bars, night clubs, coffee shops, liquor stores, malls, churches, meditation and yoga centers, gyms and fitness centers, hospitals, clinics, doctors, fire department, police station, bicycle shop, recreation and sporting goods equipment shops. Also, if you want to learn about the credibility and quality of service provided by any particular business, read about them on Yelp ( http://www.yelp.com ). You can find excellent reviews by customers, which can give you a quick idea about whether you want to shop in a particular store or eat in a particular restaurant. Yelp can also be used to find any type of business in a particular area, and it can be used to find directions. You can download this app for your phone on iTunes. If your are planning to stay a short time, consider buying slightly used or "second hand" furniture, clothes or other items that you can easily sell or give away when you move back to your country. This will save you lots of money. You can find many good deals on items at Craig's list (craigslist.org), as well as at stores like the GoodWill, and Salvation Army and any general "thrift shop".  Craig's list is a popular place to find jobs, apartments and buy a wide variety of used merchandise at inexpensive prices. You can also find many inexpensive good quality items at "tag sales", "garage sales" or "lawn sales". These are impromptu sales where people place items outside their home in the garage or on their lawn, and the prices are usually quite cheap. You can bargain and ask for a lower price, and often these are run by students who just want to get rid of their things at very low prices. You can also look for "flea markets". Also stores like IKEA, and Target generally have some fairly inexpensive furniture and other items. You can find good deals on furniture, computers, clothes, cars, etc. You can also learn about days that local museums are free, and restaurants that have "all you can eat" for one fee, as other means to save money. Universities will also have many free entertainment venues including music, speakers, movies, etc. You can also contact us at Academic English Solutions if you need to improve your English proficiency or take classes to prepare for the TOEFL, academic writing. We will also help you with communicate with advisers or international student officers in the US, revising your correspondence if needed, and offering practical advice. If any of the 10 items listed above are not clear, we can clarify and advise your further. We can give you basic and practical information related to the most universities in the US, as well as practical tips and suggestions on how to navigate life in the US. Contact us if you are having trouble organizing your visit, we can work as a liaison to connect you with the appropriate person, or explain certain customs or procedures common to the US. We also offer a wide variety of online English classes: http://www.academicenglishsolutions.com/AES/English_Classes.html In addition, we revise the English of scientific manuscripts, dissertations, and other academic texts. Read more about our editing service here: http://www.academicenglishsolutions.com/AES/Editing.html Contact us to schedule a free demonstration class online: http://www.academicenglishsolutions.com/AES/home.html While I have added several of my own tips above, I  wish to gratefully acknowledge that the top ten suggestions mentioned here are primarily a paraphrasing of Succeeding as an International Student in the United States and Canada, by Charles Lipson. I highly recommend this book for anyone planning to do research or study in the US. It has many more practical tips. Here is a link to Amazon where you can find it at a discounted price: http://www.amazon.com/Succeeding-International-Student-Chicago-Academic/dp/0226484793/ref=sr_1_1?ie=UTF8qid=1383320213sr=8-1keywords=succeeding+as+an+international+student+in+the+united+states+and+canada http://www.academicenglishsolutions.com/AES/home.html ]] My book, English for Research Papers: A Handbook for Brazilian Authors, is now available at Amazon, and I just wanted to invite you to review it there. Here is a link to the book: http://www.amazon.com/English-Research-Papers-Handbook-Brazilian-ebook/dp/B00GPT0FXW/ref=sr_1_1?ie=UTF8qid=1384714433sr=8-1keywords=english+for+research+papers+a+handbook+for+brazilian+authors http://www.academicenglishsolutions.com/AES/home.html ]] I would be very grateful if you could repost and share this message with your connections, colleagues and students who might find my book useful. Below is a link to the book, English for Research Papers: A Handbook for Brazilian Authors. Muito obrigado! http://www.amazon.com/ English-Research-Papers-Handboo k-Brazilian-ebook/dp/ B00GPT0FXW/ ref=sr_1_1?ie=UTF8qid=13847953 03sr=8-1keywords=english+for +research+papers+a+handbook+fo r+brazilian+authors http://www.academicenglishsolutions.com/AES/home.html ]] 