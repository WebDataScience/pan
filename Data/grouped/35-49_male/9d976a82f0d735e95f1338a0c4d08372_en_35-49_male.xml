 Slate.com writes about our upcoming study on Internet Research extending our findings presented in " Information Credibility on Twitter " [pdf]. Social media hoaxes: Could machine-learning algorithms help debunk Twitter rumors before they spread? ... In a new paper, to be published in the journal Internet Research next month, the authors of the Chile earthquake studyCarlos Castillo, Marcelo Mendoza, and Barbara Pobletetest out their algorithm on fresh data sets and find that it works pretty well. According to Meier, their machine-learning classifier had an AUC, or area under the curve, of 0.86. That means that, when presented with a random false tweet and a random true tweet, it would assess the true tweet as more credible 86 percent of the time. (An AUC of 1 is perfect; an AUC of 0.5 is no better than random chance.) My guess is that a knowledgeable and experienced human Twitter user could do better than that under most circumstances. And of course, if a given algorithm became widespread, committed trolls like the Hurricane Sandy villain @ComfortablySmug could find ways to game it. Still, an algorithm has the potential to work much faster than a human, and as it improves, it could evolve into an invaluable "first opinion" for flagging news items on Twitter that might not be true. ... Source: Slate.com ]] I am glad to announce the third edition of the Web Quality workshop, to be held on May 13th, 2013 in Rio de Janeiro, Brazil. The workshop is co-located with the World Wide Web conference. This year's theme is the question: Signal or Noise? . The Web and social media keep on growing and playing an ever increasing role in our lives. In this context, finding relevant, timely and trustworthy content in a sea of seemingly irrelevant chatter remains a challenging research issue. The workshop will bring together practitioner and researchers working on key problem areas such as modelling trust and author reputation, detecting abuse and spam, finding high-quality content, uncovering plagiarism, among other topics. Website: WebQuality 2013  ]] New Scientist 2914, 23 April 2013 (free registration required) describes our proyect Veri.ly : ... A big problem with theories floated on social media is that information can go viral simply because it is popular, whether or not it is true. Patrick Meier of the Qatar Computing Research Institute (QCRI) in Doha is building Verily, a system that allows users to submit verification requests for information they are interested in. Each request prompts a crowd of online workers to set off into their networks to figure it out. The system gathers evidence for and against the claim, though it won't pass judgement. ...By training machine learning algorithms on huge data sets, Meier is building up profiles of the classes of digital evidence that tend to be credible, and those that are not. As an example, Meier points to a recent study of misinformation on Twitter after the 2010 Chilean earthquake. Carlos Castillo of the QCRI and colleagues showed that non-credible tweets tend to spark responses that question or rebuke them  a trait software can be trained to recognise. "Non-credible information propagates across the twittersphere leaving very specific ripples behind," says Meier. "You could absolutely start having a probability  a percentage chance that particular tweets are not credible." Full article in New Scientist (free registration required)  ]] Congratulations to my colleagues M. Imran (QCRI), S. Elbassuoni (Beirut University), F. Diaz (Microsoft) and P. Meier (QCRI) for a best paper award at the ISCRAM conference. ISCRAM is the main international conference on systems for crisis response and management. Our work, described in the two papers below (specially on the first one), describes a method to extract information nuggets from tweets related to emergencies. For instance, we can go beyond detecting that a tweet is about a donation to identify which is the item being donated (e.g. clothes, money, etc.). Muhammad Imran, Shady Elbassuoni, Carlos Castillo, Fernando Diaz and Patrick Meier: Practical Extraction of Disaster-Relevant Information from Social Media . In SWDM. Rio de Janeiro, Brazil, 2013. Muhammad Imran, Shady Mamoon Elbassuoni, Carlos Castillo, Fernando Diaz, and Patrick Meier: Extracting Information Nuggets from Disaster-Related Messages in Social Media . In ISCRAM. Baden-Baden, Germany, 2013. Official announcement at QCRI website. ]] Slides from keynote at the Social News on the Web Workshop. Rio de Janeiro, Brazil, May 2013. ]] With Janette Lehmann (UPF), Mounia Lalmas (Yahoo!) and Ethan Zuckerman (MIT Civic Media), we developed an automatic method ( pdf , blog post ) that groups together all the users who tweet a particular news item, and later detects new contents posted by them that are related to the original news item. We call each such group a transient news crowd. The beauty of this approach, in addition to being fully automatic, is that there is no need to pre-define topics and the crowd becomes available immediately, allowing journalists to cover news beats incorporating the shifts of interest of their audiences. Continue reading at crowdresearch.org  ]] QCRI/AJE press release: QCRI and Al Jazeera launch predictive web analytics platform for news New platform developed by QCRI and Al Jazeera can predict visits to news articles by taking cues from social media Try it! http://fast.qcri.org/ News organisations have vast archives of information, as well as a number of web analytic tools that aid in allocating editorial resources to cover different news events, and capitalise on this information. These tools allow editors and media managers to react to shifts in their audiences interest, but what is lacking is a tool to help predict such shifts. Qatar Computing Research Institute (QCRI) and Al Jazeera are announcing the launch of FAST (Forecast and Analytics of Social Media and Traffic), a platform that analyses in real-time the life cycle of news stories on the web and social media, and provides predictive analytics that gauge audience interest. The explosion of big data in the media domain has provided QCRI an excellent research opportunity to develop an innovative way to derive value from the information, said Dr Ahmed Elmagarmid, Executive Director of QCRI. Together with our valued partner, Al Jazeera, the QCRI team has developed a platform that will help shift the way media does business. Al Jazeera Englishs website thrives on good original content in news and features, dynamic ways of creativity through interactive and crowd sourcing methods, and up-to-date social media tools. We welcome working with QCRI in developing FAST as it allows us to understand the consumption of news and what is expected to do well in driving traffic forward. Analytics in predicting the future trend of a web story is a crucial component in understanding web traffic, this initiative is a component we welcome, said Imad Musa, Head of Online for Al Jazeera English. You can test the platform at http://fast.qcri.org/ and read the full press release at the QCRI website . The system is based on research described in the following paper: Carlos Castillo, Mohammed El-Haddad, Jurgen Pfeffer and Matt Stempeck: Characterizing the Life Cycle of Online News Stories Using Social Media Reactions . Submitted for publication. [ arxiv pre-print | review by s.v. ] ]] Wired UK, 30 September 2013. Katie Collins covers part of our work in Social Computing and Social Innovation at QCRI: On 24 September a 7.7-magnitude earthquake struck south-west Pakistan, killing at least 300 people. The following day Patrick Meier at the Qatar Computer Research Institute (QCRI) received a call from the UN Office for the Coordination of Humanitarian Affairs (OCHA) asking him to help deal with the digital fallout -- the thousands of tweets, photos and videos that were being posted on the web containing potentially valuable information about the disaster. [...] AIDR (Artificial Intelligence for Disaster Response) was the second project tested for the first time during the Pakistan floods, and is due to be launched officially at the CrisisMappers conference in Nairobi in November. It's an open-source tool relying on both human and machine computing, allowing human users to train algorithms to automatically classify tweets and determine whether or not they are relevant to a particular disaster. In Pakistan, SBTF volunteers tagged 1,000 tweets, out of which 130 were used to create a classifier and train an algorithm that could be used to recognise relevant tweets with up to 80 percent accuracy ... Full article in Wired UK . ]] I had the privilege to work with Wei Chen (Microsoft Research) and Laks V.S. Lakshmanan (University of British Columbia) on a book for the Synthesis Lectures on Data Management series, edited by M. Tamer Ozsu and published by Morgan and Claypool. This book starts with a detailed description of well-established diffusion models, including the independent cascade model and the linear threshold model, that have been successful at explaining propagation phenomena. We describe their properties as well as numerous extensions to them, introducing aspects such as competition, budget, and time-criticality, among many others. We delve deep into the key problem of influence maximization, which selects key individuals to activate in order to influence a large fraction of a network. Influence maximization in classic diffusion models including both the independent cascade and the linear threshold models is computationally intractable, more precisely #P-hard, and we describe several approximation algorithms and scalable heuristics that have been proposed in the literature. Finally, we also deal with key issues that need to be tackled in order to turn this research into practice, such as learning the strength with which individuals in a network influence each other, as well as the practical aspects of this research including the availability of datasets and software tools for facilitating research. We conclude with a discussion of various research problems that remain open, both from a technical perspective and from the viewpoint of transferring the results of research into industry strength applications The book is available for USD 20 or through many libraries: Wei Chen, Laks V.S. Lakshmanan, Carlos Castillo: Information and Influence Propagation in Social Networks . Synthesis Lectures on Data Management. Morgan and Claypool Publishers, Oct. 2013. [ doi ]. Two chapters are available for free: Chapter 1: Introduction available for free. Chapter 6: Data and Software pre-review version available. ]] Presentation on November 14th, 2013 at the Tow Center , Columbia Journalism School. (New York, USA). Carlos Castillo: Social Media News Mining and Automatic Content Analysis of News . Invited talk at Tow Center, Columbia University. New York City, USA, 2013. [ VIDEO | blogpost | invitation ] ]] 