 In this table you can find: Number of votes for Obama and McCain in 2008. Number of votes for Obama and Romney in 2012. The error made by predicting that Obama would obtain exactly the same percentage in 2012 that in 2008 in each state. The electoral votes obtained in 2008 and in 2012. The % of popular and electoral vote. The MAE (Mean Absolute Error) Data for 2008 Elections was obtained from Wikipedia . Data for 2012 Elections was obtained from politico.com . Please note that I do not fully understand most of the subtleties of the electoral college so the number of electoral votes may be not accurate. Nevertheless, this "groundhog-day" baseline is really good (for the US Presidential Elections): it only missed 2 out of 51 states with an impressive MAE of 2.75% . So, in short, what would be a reasonable MAE for an algorithm to be credible (to me)? A 5% improvement over the baseline, i.e. MAE = 2.61%. Of course, if your algorithm is able to be below a MAE of 2.48% (a 10% improvement) I would be impressed. If your MAE is greater than 2.61%, I'm really sorry but your algorithm is useless :( However, I think that any sensible prediction should take into account data from past elections and, therefore, it would be really difficult to tell the difference between just using the baseline and "icing" that historical data with some extra information from social media. As usual, contact me on Twitter if you please: @PFCdgayo ]] To solve that, a student ( David Moreno ) tried to convince me to use Google+ Hangouts and I went through this thorough explanation on how to use Hangouts to stream a keynote . Nevertheless, I found rather uncomfortable to need two computers in order to share both the slides and the webcam and, besides, the quality of the image was IMHO better with Skype than with the Hangout. On the other hand, using a Hangout On Air I would be able of streaming the conference through YouTube and anyone interested in it could attend the lecture without joining the Hangout. Moreover, the video would be immediately available after the broadcasting. Hence, I prepared my list of "requirements": I wanted to share my screen using Skype and Google+ Hangouts. The screen should show both my slides and my webcam. I didn't want to pay for Skype premium. The solution was to prepare a webpage showing the "slides" (actually images produced from PowerPoint) and the webcam. To that end, the jQuery webcam plugin was invaluable, as it was the help of David. You can check the result in this video (in Spanish) and you can find the current version of Screen Me! (HTML+CSS+JS) prepared by David in github. ]] Lexicon matching is by no means the most accurate way of performing sentiment analysis; however, it is one of the easiest ways of implementing a quick prototype. Needless to say, we need a lexicon to do that and they tend to be scarce, mostly limited to the English language, and small. I've just heard of the corpus by Warriner et al. with almost 14,000 English words and I've tried to prepare a quick "translation" into Spanish. The method I've applied is extremely crude: Translate the list of words from English into Spanish. Translate (again) the list of Spanish words into English. Check the original English word and the English-to-Spanish-to-English word are the same. In addition to that there has been some manual checking but, as I say, it is pretty crude and is provided as is. So, here you are, the Warriner et al. corpus machine-translated into Spanish with a little more than 9,000 words. Enjoy! As usual you can find me at PFCdgayo for any comment regarding this post. ]] English version is below . Descompon tus proyectos en tareas concretas y realizables. Haz un triage de tareas para ver como afrontarlas. Delegar o ponerlas en la cola de pendientes es a veces una posibilidad. Necesitas una lista: te ayuda a no olvidar nada pero, sobre todo, te anima ver el trabajo realizado. Incluye en tu lista todo, incluso cuestiones ajenas al trabajo/estudio. De lo contrario trabajo/estudio agotaran tu tiempo. Date un premio al terminar una tarea de tamano razonable. 3-5 minutos de redes sociales o consola pueden ser ese premio. Descansa cada par de horas. Estira las piernas, toma el aire, pica algo, socializa (con gente de carne y hueso). Diferencia lo urgente de lo importante, no suelen coincidir. Evita por todos los medios afeitar yaks. Para ello tienes que aprender a detectar esas situaciones. Gestionar el tiempo no puede ser complicado: tienes que ser capaz de automatizar tu metodo con el uso. Asegurate de que tienes una vida, comes de forma saludable y duermes un numero de horas razonable. Tweets are ephemeral so I'm writing this post to save these little pieces of advice. Break down your projects into concrete and achievable tasks. Apply task triage to see how to deal with them. Delegating or putting them in a pending queue is a feasible choice. You need a TODO list: it helps you to not forget anything but, above all, it's encouraging to see the already done work. Put everything in your TODO list, even issues outside your job/studying. Otherwise job/studying will exhaust your time. You deserve some reward after completig a reasonable sized task. 3-5 minutes of online social networking or gaming can be that reward. Take a break every couple hours. Stretch your legs, get some fresh air, take a snack, socialize (with people in the flesh). Tell appart urgent from important things, they usually differ. Avoid by all means yak shaving. To do that you need to learn how to detect such situations. Time management cannot be complicated: you have to be able to automate your method through its use. Be sure you have a life, you eat healthily and you sleep your hours. ]] Update December 16: If you are interested in something more "academic" you may find of interest this paper by me that is available for free here . Update August 15: I was wrong when saying that DiGrazia et al. should have cited Morstatter et al. I warned them on April 25 about that paper but by then they should have already submitted to ASA 2013 and, therefore, their final paper could not have made reference to the work by Morstatter. Prior warnings : This is a lenghty post, I'm sorry. I've done my best to structure it properly but it touches plenty of intermingled topics and, besides, it has been written in a hush (paraphrasing Pascal, if I had more time, I would have written a shorter post). I hope none of the people mentioned here will have hard feelings because of this post; you know, it's just academia, nothing personal. The tone is sometimes humorous (or sarcastic depending on your inclination). The post tackles with the following topics: (1) a comment about the paper that DiGrazia et al. presented at the Annual Meeting of the American Sociological Association in New York on August 2013; (2) some personal digressions on the use of "catchy" titles in academic papers and the possible unintended outcomes of such practice; (3) more personal digressions on press releases and handling the press when covering academic research; and (4) a comment to the open editorial published by Fabio Rojas in Washington Post tangentially related to the paper mentioned in (1). Said that, let's the post begin! In case you don't know my work I've earned kind of a reputation as someone that tend to criticize anyone claiming they has been able to predict elections using Twitter data ( i , ii , iii , iv and v ). There is a long and twisted story behind that reputation but, at this moment, let's reduce my role to that cliche: my only line is "No, you cannot predict elections with Twitter" . To clarify that point a little, up to now, nobody has tried to predict elections using Twitter data (except for this person and this people ), everybody has shown that they could have predicted elections. In other words, researchers have collected tweets related to a given election, they have applied to them some method and after the election they have compared their results with the actual outcome and said eureka! Hence, one of the facets of my role in this academic "drama" is that I ask researchers to predict elections before they are held. It seems pretty simple, right? Nevertheless, virtually every paper published up to now is a post-facto prediction which, obviously, cast doubts on such results; not only because authors can inadvertently incur in data dredging but also because those authors not achieving positive results are not publishing their papers and, hence, the public perceives that predictions are possible when they may not. I think such a requirement is sensible for a so-called prediction but anyway it is mostly ignored and some authors argue that their paper is not actually dealing with predictions to skip such a requisite (BTW, in Twitter conversation this argument has been used by one of the authors of the DiGrazia et al. paper but they are not the only ones). However, just in case someone is paying attention: You, Have, To, Predict, In, Advance. If you don't want to follow my advice follow that of Lewis-Beck (2005): "the forecast must be made before the event. The farther in advance [...] the better" . Once clarified my position (you cannot predict) and the worst sin of virtually every paper on predicting elections from Twitter (they make postdictions and not predictions) what's the matter with the work by DiGrazia et al.? Why should I comment it appart of fulfilling my role of Jiminy Cricket? There are a number of reasons for doing it. First, it is a nice paper, with issues as any other paper; however, if such issues could be somewhat solved or reduced, it provides evidence for the hypothesis most of us were working on without even cast a doubt on it; namely, that Twitter actually provides insight (in some distant way) to the position of the public regarding elections. Second, it is a perfect example of how a team of researchers can be involved through misfiring catching titles and unfortunate press releases in an uncomfortable position while trying to balance what they paper really says with what they seemed to have said to some journalist and, hence, the public believes is scientific evidence. It must be said however, that in this case most of the harm is due IMHO to the title of the press release (not the fault of the authors of the paper) and one unfortunate open editorial authored by only one of the authors. In fact, this op-ed is the third reason to discuss the paper. That op-ed promises a lot on the basis of really thin evidence but cites anyway parts of the paper for support, therefore confusing the public by mixing personal opinions with biased excerpts of scientific evidence. From my point of view this op-ed is the most interesting aspect of this story since it's the first time I cannot point a journalist for having misinterpreted a researcher but it is the researcher himself the one "twisting" his work to picture their opinions in a much more attractive light. The paper of interest is "More Tweets, More Votes: Social Media as a Quantitative Indicator of Political Behavior" by Joseph DiGrazia, Karissa McKelvey, Johan Bollen and Fabio Rojas. As aforementioned, the paper was presented at ASA 2013 and you can find a draft of the paper (slightly different from the final version) here . For the final version please ask the authors since the copy I have got was sent to me from someone, not the authors, and haven't found it online. However, before proceeding with my review of the paper I would like to explain how I came to it. Quite simple, I was curious because of the title; in fact, I was curious because the first part of the title: "More Tweets, More Votes" . If you are a connoisseur of academia you know that attention is everything; attention get you cites, coworkers, program committees, etc. You know the proverb, if a paper is written and nobody reads it, does it make any impact? Hence, authors try to get the attention of potential readers from the very beginning, that is, the abstract is important but the title is key. This paper has a really catchy title; indeed, catch and clever since it serves as an abstract, it's memorable and nicely fits a tweet. This paper has got a title which is a "winner". Because of the title I supposed that the authors were using the most simple approach to predict elections: that is, counting the number of tweets for a candidate, take them as votes, compute the vote share, et voila! From that supposition I read the paper (around April) and found it rather different from my initial guess. While wearing my referee hat I found it to have some flaws but with also with potential. The title, however, was a problem: it promised much more than the paper provided and I guessed it was going to be a headline attractor (as it has been). Please do not misunderstand me: I'm a great fan of catchy titles for academic papers. I could be very capable to introduce myself a-la Troy McClure: Hi, I'm Dani Gayo. You may remember be from such papers as "I wanted to predict elections with Twitter and all I got was this lousy paper" or "All liaisons are dangerous when all your friends are known to us" . (BTW, both are real papers by me). The problem with catchy titles is that the paper has to be able to stand for the title. With this paper you have a feeling not entirely unlike what you have after reading "The Neverending Story" : it's nice, but it does not fulfill the title. Before you forget this post to read the paper I'll provide an spoiler : the authors looked for a correlation between number of tweets for a candidate and the margin of difference between that candidate and his or her opponent. They found the correlation to be significant and positive which means that, in general, the larger the number of tweets the larger the difference in votes you should expect. Unfortunately, the truth is that you cannot readily apply that finding as a rule of thumb to compute the expected vote rate or vote difference from the tweets. That's why the paper does not stand for the catchy title. Is that a sin? No way, in fact it was, at least for me, a glad surprise and, indeed, the final version of the paper is quite interesting. Nevertheless, the title is far more sexy than the paper but I'm repeating myself. In the final version of the paper the authors made a great effort to control for any confounding variable such as incumbency, demographic features and so on. The good news is that when controlling for all of those variables the correlation is still significant, the not-so-good news (my point of view) is that the correlation between tweets and votes is really small when compared with other factors (such as incumbency, for instance). As a researcher I'm reasonably excited with the good news: it means that Twitter is no inane chatter, tweets tend to generally serve as a proxy for public opinion. The problem lies in the not-so-god news: it means it is really hard to make any sensible prediction and, let's not forget, we humans want to predict the future and not simply to know that it is predictable in theory. What's the reason why sometimes tweets can reflect the actual vote and sometimes not? I think a plausible explanation is that the Twittersphere is far from monolithic and that depending on the candidate, even on the topic, one tribe or another is going to dominate the Twitter space and you will never know which tribe is responding. You can find much more on this somewhat devastating report from Pew Research . Those are the main worrisome aspects of a paper that is perfectly nice but promises a lot to then fail the reader by not fulfilling its promises. Needless to say, in addition to that, I have the common petty comments any referee would provide (please note that many of these refer to the first draft of the paper, BTW, I exchange some comments with one of the authors, Fabio, and this makes writing this post much more complicated for me from a personal point of view): The paper does not completely cover the prior art which, at this moment, is still reasonably small to be cited with detail. The following material could provide complete references to that prior art: [1] , [2] . Much more detail should be provided regarding the dataset, specially the way in which tweets related to a candidate where obtained (keywords used and date ranges), and how topicality was warranted. Since the paper has been published after the work by Morstatter et al. (2013) the authors should at least acknowledge that the representativeness of their sample is unknown (since they have not used the Firehose). Indeed, the problem of not having access to the Firehose and try to justify the gardenhose as a representative sample is a bummer for all of us from now on. A clarification on this rectification can be found here . The authors take into account both positive and negative tweets under the argument that "all publicity is good publicity" , is a really weak argument and there have been a number of papers showing that tweet-counting is a subpar approach. In the final version of the paper the authors mentioned the Pollyanna effect which, in total honesty, I do not think has been properly interpreted by them to support their argumentation. In other words, there are no actual argumentation to support that negative tweets are a good thing for a candidate. In the first draft there are a couple of scatter plots that, in my opinion, if anything they only help to prove my point (i.e. that the number of tweets does not actually mean anything for a given concrete candidate). In this regard I'm pointing to this post in The Monkey Cage that makes a much better job than me explaining the problems with that scatter plot and the interpretation the authors made of it. In addition to that, performance in electoral prediction cannot be evaluated by telling how many elections one has correctly guessed (even when discussing a highly polarized system with only two major parties like the US). It's much more important to know the share vote, specially in disputed elections and/or systems with more than two parties (e.g. most of Europe and other countries in the world). Again, it's not me who says this but Campbell (2004). Finally, my common warnings about Twitter user base not being representative of the population, self-selection bias, spam, propaganda, lack of geolocation of tweets, etc. apply to this paper. So, in short, the paper is nice, it's a honest approach to the problem with some new ideas. If everything in the dataset is right , and this can be replicated with other comparable datasets, and/or data from other countries, and/or data with better preprocessing (eliminating bots and spam for instance), then their results would be of interest for researchers since it would imply that Twitter data means something (although we worked on that basis all of the time). Unfortunately, the results are no ground breaking and, no, given a race more tweets does not imply always more votes, it's much more complicated than that. That for the paper. Let's talk about the press release. As I said before, the paper had a press release issued by the American Sociological Association which you can find here . I don't know how usual or unusual it is that ASA issues a press release for an article in a conference but it certainly helped to spread the word about the paper making a subtle twist to its main conclusion. The press release was issued with the following title: "Study finds more tweets mean more votes for political candidates" . If you are not used to press releases by research organizations I'll tell you; they are interested in what I call "sexy research". Cures for cancer or AIDS apply for sexy research, "striking" or "bizarre" results are also sexy research, social media is currently sexy research in computer science (famous for being unsexy, unless you work in computer graphics or robots). Having had my fair deal of interviews with journalists and issuing press releases I think I know a little about them. First, press releases (and articles about science) are usually written in a rush, they must be brief, they must be catchy, they must be to the point, and they must show that the research is useful for the public (aka the tax payers, aka your funding source). If you read the press release regarding this paper you can find that it describes the main findings of the paper but from a rather optimistic point of view. This applies not only to this press release but to any press release regarding a piece of research. It is somewhat comparable to listing a house, you don't say "small", you say "cute"; you don't say "falling apart", you say "great potential"; you don't say "old", you say "with character". All of this make up is usually done by the people in the press office who know that we, researchers/scientists, are "too shy to sell our work"; they are there to simplify our explanations, make them more mundane, more close to the public... If you compare the writing of the release with the quotes from the authors you can see that the authors are much rather cautious in their statements. This is also common since we, researchers, do not like to make bold statements (except in catchy titles) and usually this quotations are obtained by the people in the press office by making questions (which are trying to obtain a catchy claiming in line with their prejudgment of the piece of research). If you have gone through this you probably understand the nightmare it is, the jokes from your colleagues once the press release is issued, the rants in the comments in the online newspaper... If you haven't you are lucky (lucky but with unsexy research, sorry). Now, let's suppose you are a journalist going through a number of press releases. What would you think of this one? "Wow, Twitter can predict elections!" By now the snowball is rolling and, therefore, the paper is covered in plenty of newspapers without even contacting the authors most of the time, they will publish the press release almost verbatim and the mantra "Twitter can predict elections" is again on track... At this moment, I should enter scene and do my routine. However, this time was different (I had hoped): (1) I had already tell the authors my point of view, (2) I'm really tired of telling once and again what I see as obvious (that predicting elections is NOT that easy) and, (3) there is no point in trolling thousands of journalist worldwide plus hundreds of thousands of tweeters wowing "Hey, Twitter can predict elections" (BTW, I have not ever trolled journalists). Nope, I'm water my friend, I'm flowing... I was flowing until I reached an open-editorial by Fabio Rojas, one of the co-authors of the paper, published in Washington Post and titled "How Twitter can help predict an election" . After reading it I simply got frozen. I got frozen because I profoundly disagree with that piece, not actually because of the content but because of the unfortunate timing and (mis)use of a piece of research to support personal opinions about a future that may simply not occur. I also got frozen because I had a gentle exchange of e-mails with Fabio regarding their paper and I'm not a bully nor a troll. Hence, Fabio, this is no personal, just business. As I see it the op-ed is a piece of opinion about a plausible (albeit rather distant) future usage of social media. If it simply was that, I would not have any problem with it but Fabio cites the paper where he's coauthor to provide support for those opinions and, in fact, although some facts are extracted from the paper, the paper as a whole does not provide any evidence to support the claims in the op-ed. Those facts taken from the paper are provided without context and cherry-picked so the reader (the public, the tax payers, the source of funding) are mislead to think that performance is much better than it really is and that predictions are easy to made. The terrible "all publicity is good publicity" mantra is used again. The problem with that is that virtually every reader can think of counterexamples against that line of argumentation and, hence, put the author (and the research he undertakes) under a poor light. It is because of all this that I think that this op-ed has been unfortunate at least and potentially harmful for the field of computational social science in general and electoral prediction based on user generated content in particular. Finally, if you have reached the end of this post you may wonder what's my opinion on this matter. Can or cannot elections be predicted from Twitter data? If you aim to use the methods reported in the literature up to now, no, you cannot. However, I'm mildly optimistic and more work is needed (specially regarding adversarial scenarios, i.e. those where candidates are aware that Twitter data can be used to measure their potential). Needless to say, "I dunno, we need more work" is not sexy and does not make headlines. To conclude, I'd like to provide a short summary for all of the topics I covered in this post: I rather liked the paper by DiGrazia et al. It has issues but which paper hasn't. The title is sexy and otherwise I would not have read it so, although problematic, it was an overall good decision. Use catchy titles with caution. Think of them attached to your resume before using them. Think of potential consequences, specially when read by press offices and journalists. Be extremely cautious when writing a press release or working with people from the press office to issue one. Be sure they do not distort your work. The op-ed by Fabio was, from my point of view, a misfortunate event because of timing and because of mixing personal opinion with cherry-picked scientific evidence. It is hard for the non expert telling apart a scientist's opinions from hard facts, everything is "scientists say". Hence, when writing for the general public be even more cautious and always be clear and make distinction between facts accepted by the whole community, findings in your work (which may be disputed by other authors) and your personal opinions (driven or not by your findings). As usual you can find me at Twitter: @PFCdgayo . References Campbell, J.E. (2004). Introduction - The 2004 presidential election forecasts. Political Science Politics, 37, 733?736. Lewis-Beck, M.S. (2005). Election forecasting: Principles and practice. The British Journal of Politics International Relations, 7, 145?164. Morstatter, Fred, et al. "Is the sample good enough? comparing data from Twitter?s streaming API with Twitter?s Firehose." Proceedings of ICWSM (2013). ]] Warning: This post is a follow up to this one . Unless you read that one before, you'll probably won't get the point of this one. In my previous post I said that one of the issues with the DiGrazia et al. (2013) paper is that they did not mention the work by Morstatter et al. (2013). The later work is crucial because it shows that when using the public Streaming API gardenhose (i.e. the randomly sampled 1% stream of tweets that most researchers use) results are quite different from those obtained when using the firehose (the whole stream of tweets). In my previous post I said that DiGrazia et al. should have at least acknowledged that they didn't known how representative their data was on the basis of the work by Morstatter et al. [ Addendum, August 18: However, as Alex Hanna accurately pointed out , this would not exactly apply to the study by DiGrazia et al. since they used the gardenhose (10% sample) and not the 1% public Streaming API.] To justify that I simply said that the work by Morstatter et al. preceded that by DiGrazia et al. However, Emilio Ferrara told me that I was wrong on that since the work by Morstatter et al. was published in July and the first draft by DiGrazia et al. was published in February. I was sure, however, that DiGrazia et al. were aware of that work because I had addressed them to a preprint on April 25. That was the reason for me still pointing that flaw in the paper. However, I was not aware of the deadlines for the annual meeting of the ASA: on January 9, 2013 papers should have been submitted, and March 18, 2013 was the date decisions letters should be sent to authors. Besides, on April 30, 2013 the final program for the conference was to be announced so I assume that between late March and early April authors should have submitted their camera ready version of the paper. So, in short. The work by Morstatter et al. was available online at least on April 25 and DiGrazia et al. should know of it at least from that date because of my e-mail. However, it's very likely that by that day they had already submitted their camera ready version of the paper and, hence, that would explain the lack of that reference in their paper. [Addendum August 18] Besides, as aforementioned, it's debatable whether the findings by Morstatter et al. when comparing the public Streaming API with the firehose could apply or not to the gardenhose employed by DiGrazia et al. Because of this I have striken through that concrete piece of criticism in my previous post. Nevertheless, the findings by Morstatter et al. are a source of concern for all of us working with Twitter public Streaming API (the 1% sample) and even maybe those working with the gardenhose (the 10% sample) since we simply don't know the biases they can exhibit when compared with the whole firehose.]] "Jonathon Fletcher: forgotten father of the search engine". The article is quite interesting since it vindicates one pioneer of the now essential field of Web-IR (Web Information Retrieval) but it has a number of "flaws" that I, as an academic, would like to elaborate. Certainly, the work by Fletcher describing his search engine is very little known ( 4 cites [1], according to Google Scholar). Maybe one reason for this is that his original paper vanished with his homepage at the University of Stirling. Fortunately, we have the Internet Archive so you can read it . Another reason for Fletcher work to remain mostly unnoticed is that his search engine (Jumpstation) only operated from December 1993 to April 1994. In other words, it was a proof of concept rather than a full-fledge system. Finally, Fletcher was not the only one making search engines around 1993-1994. As I like to say "It steam-engines when it's steam-engine-time": just in WWW94 (aka The First International Conference on the World-Wide-Web) nine papers about Web indexing and Web-IR were presented. Needless to say, they did not call it Web-IR at the time. Martijn Koster , Oliver A. McBryan , Pinkerton or Mauldin Leavitt were some of the researchers that were working in the same problem that Fletcher did and producing virtually the same architecture: that is, a robot to crawl the Web, in order to feed a database that was, in turn, indexed to be eventually queried by users of the search engine. So, is Fletcher the parent of the search engine? Not really. Who is then? None of the aforementioned researchers or, better, all of them! That's why I much prefer talking about pioneers in a field than about "parents". I highly encourage you to read all of their papers, specially if you are pursuing a PhD in CS or if you are still an undergraduate in CS. After reading those papers you'll feel that it's something you could do in a couple of days, maybe a week. The systems they describe are crude, simplistic and primitive. However, the architecture behind those toys has been powering the search engine industry for the last 20 years. That's what I call an influential idea and that's why those papers are relevant and highly cited. The BBC piece obviously talks about Google as a counterpoint to Fletcher's story. You know, you have this guy who invented something great but some other people get all the credit (and the money). Unfortunately, there are other stories from the early search engine pioneers. Indeed, some of the aforementioned researchers got some credit (and probably some money too). For instance, Pinkerton created WebCrawler that was sold to America Online and then Excite. And if you are old enough you must remember Lycos, right? Well, it was the product of Mauldin Leavitt. I would like to talk also about the impact of the work by Brin Page (Google), the way in which they were virtually forced to found a company to monetize their idea, and the way in which all this search engine industry is in debt with IBM in 1950s but that will be another post. As usual you can find me at Twitter: @PFCdgayo . [1] Although not appearing in the citations to Fletcher's work I mentioned him as a search engine pioneer in my PhD dissertation (2005). ]] previous post I argued that there is no single parent of Web search engines but, instead, a number of pioneers that worked unaware of each other and eventually produced virtually identical achitectures. Nevertheless, the researchers I surveyed in that post pioneered the first age of search engines; that is, the now (literally or de facto) defunct Altavista, Lycos or Excite. Google, in contrast, is an example of the second age of search engines to which Bing, Yandex or Baidu also belong. It's not my aim to provide a lengthy explanation of the differences between Google (and family) and prior search engines since, first of all, Google is today very different from Google in 1998. Instead, what I'll try to do is to briefly explain why the idea by Brin Page was both brilliant and it has had an enormous impact. To start with, it's extremely difficult to explain in words how broken Web searching was back in 1997. If you are old enough (i.e. you were a user of Altavista or Lycos or Excite) you surely remember it. If you are not that old (i.e. Google has been your search engine since you can remember) I'll provide you a couple of examples (taken from the original Brin Page papers). First example: most major search engines of the time were unable to find themselves. That is, the first result you obtained when issuing the query "excite" in Excite, or "altavista" in Altavista was quite unlikely to be www.excite.com or www.altavista.com. Second example: when submitting the query "bill clinton" (you know, the husband of Hillary Clinton, the POTUS at the time) a major search engine produced as one top result a webpage like this one . So, we had search engines incapable to find themselves and that, at the same time, ignored that maybe www.whitehouse.gov was a better result for "bill clinton" than a crappy page mocking on him. What was the problem with those search engines? The short explanation is that they were trying to apply Information Retrieval to the Web. That had plenty of sense since, at the time, IR was very mature. In fact, the discipline was almost 40 years old and, therefore, all that know-how should apply nicely to the Web. Unfortunately, it simply did not work. Hence, Web search engines devoted tons of energy to refine their respective "secret sauces" (I mean, ranking algorithms). Those "sauces" involved exploiting features from webpages ranging from size, to usage of headings, appearance of keywords in links, headings and other parts of the page, etc. Needless to say, secret sauces improved little (if anything at all). So, again, what was the problem with mid-1990s search engines? I hate to repeat myself but it was applying IR to the Web because, unfortunately, the Web is not a text collection. As Marchiori said , the Web is not made of text documents but of hypertext documents and, therefore, search engines should focus not on the textual features but in the "hyper" features. Indeed, Marchiori suggested that webpages should be ranked on the basis the links they received and the rankings of the pages linking to them while mostly obviating the textual content. Such an idea sounds crazy but it's simply mindblowing. Unfortunately, no matter the greatness of this idea Marchiori is not the parent of the modern search engine since he stated that such an approach was unfeasible in practice. If you are a PhD student (or even a grown-up researcher) you should learn something from that: don't, make, bold, claims. A huge problem with bold claims is that sooner or later someone is going to make a fool on you. In this particular case it was rather soon. Marchiori suggested in 1997 that search engines should focus on links while avoiding text while, at the same time, saying that such a thing was unatainable. In 1998 Kleinberg and Brin Page described two different algorithms that working solely from links were able to rank webpages much better than state-of-the-art search engines. The paper by Kleinberg describing HITS algorithms slightly predates the work by Brin Page and, in fact, they cite it. However, there are a number of differences between HITS and PageRank. First of all, Kleinberg did not intend to build a search engine but, instead, to produce a better ranking of a somewhat small set of pages obtained from a given search engine. Secondly, Kleinberg algorithm relied on two characterizing features of webpages represented by two different (albeit mutually reinforcing) scores. Pages could be "authorities" (i.e. pages with highly relevant contents given a topic) or they could be "hubs" (i.e. pages with plenty of links to "authorities" in a given topic). Simply stated, HITS is an iterative algorithm that assign to pages in a graph (not the whole Web graph) both a hub and an authority score. The eventual ranking would be made on the basis of the authority score and, hence, a selection of highly relevant pages would be provided to the user. In contrast, PageRank assumes that web pages are described by just one score which, quite annoyingly, is also named PageRank. The PageRank of a webpage dependes on the number of incoming links but also on the PageRank of web pages issuing those links. Besides, a webpage's PageRank is equally distributed among the outgoing links. This algorithm is, as HITS, iterative and it has got a number of nice properties: It's quite fast (i.e. the number of iterations is relatively small even for large graphs). Second, the initial scores are mostly irrelevant and the eventual ranking is virtually the same without regard to those initial values. Third, the global PageRank in the graph does not change accross iterations but, instead, it is distributed among webpages. A webpage PageRank can be considered a nice proxy for the chance of a user reaching that webpage by randomly visiting links. In other words, pages with larger PageRank values are "central" in the Web. Certainly, as of today PageRank is just one component of Google's ranking methods but back in 1998 it was the core of Google's search engine and it was a huge improvement when compared against state-of-the-art products. However, according to legend , none of major search engines of the time were interested in Brin Page method, neither Yahoo! that at the time was not in the search business. Therefore, Brin Page were "forced" to start their own bussiness, the rest is history . I would not like to close this post, however, without referring to some of the people that made Information Retrieval a mature area before the advent of Web-IR: Luhn, who is the solely inventor of information retrieval systems and automatic summarization , both achievements occuring in IBM during the 1950s; Maron Kuhns, who invented the list of relevant documents in the 1960s; and, of course, the demigods and demigoddess of IR Gerard Salton , Stephen Robertson and Karen Sparck-Jones . As Newton nicely put it, If I have seen further it is by standing on ye sholders of Giants. As usual you can find me at Twitter: PFCdgayo ]] @odo asked my opinion about this opinion piece (in Spanish) about the new Tweets-per-second (TPS) record and the purported uselessness of such a metric. In that piece the journalist argues that TPS is a mostly meaningless metric for two main reasons: on one hand the chat can come from a very vocal minority and, on another hand, there surely must exist (the journalist argument) events with much wider impact than the broadcast of "Castle in the Sky" . He cites as examples bin Laden's death or an iPhone presentation. I have nothing to argue about the vocal minorities argument; it's a real problem when trying to use Twitter to "pulse" public opinion and, in fact, there are some literature about this (see Mustafaraj et al. 2011 ) However, the second argument should be justified and to that end I conducted some back-of-the-envelope research using Google Trends. As you probably know, that tool allows you to compare search volumes for different queries filtering by country (and even region), and also at different time slices. Therefore, I compared queries for "Castle in the Sky", Osama bin Laden, Superbowl, Barack Obama and iPhone. Neither Superbowl or Obama were mentioned in the original newspaper piece but I think they are fair examples. I've limited my research to Japan since, as explained in Twitter's blog, the new TPS record was due to Japanese users. Hence, I used queries in Japanese. After some Wikipedia research and trying different approaches (e.g. Osama bin Laden, Laden, and bin Laden) I eventually employed the following queries:  (Castle in the Sky)  (bin Laden)  (Superbowl)  (Obama) iphone Given that bin Laden's death took place on May 2011 and the TPS record took place on August 2013, I took as time frame from April 2011 to September 2013. In the following graphs you can check how "Castle in the Sky" is extremely popular in Japan, greatly surpassing bin Laden's death (a meager 7% of the peak number of queries for CitS) and Superbowl (52% of the peak number of queries for CitS). Certainly, Obama's victory in 2012 was much more important: the CitS broadcasting responsible for the TPS record achieved just 69% of the queries that got Obama. However, there is something that seems to be really important in Japan, much more than Obama: the iPhone. CitS peaks at 3% of iPhone queries! So, in short, impactful events may be uninteresting for Internet users (either Googlers or Twitterers) and, of course, number of queries or TPS do not reflect the importance of the event but the interest or attention that event got from the Internet population. Now, going back to the usefulness or uselessness of TPS as a metric, many of you may remember past TPS records ( this link is for those of you that cannot remember them) and are wondering how this new record compares to previous ones. The truth is that they are simply not comparable for a number of reasons: Twitter user base was different at each record. First and foremost, larger each time. Events driving to each TPS record were pretty different even when being of worldwide interest. For instance, Superbowl and FIFA World Cup are followed by different populations with a different approach to Twitter. Twitter architecture and infrastructure was different each time. In other words, TPS records are IMHO a signal of Twitter's performance at peak time more than a way to compare moments of worldwide-awe. So, to sum up. TPS provide clues but not the whole story, specially when you compare apples to pears (i.e. different events); however, I think it can be an interesting metric to use when evaluating the "performance" of a single event. As usual you can reach me at @PFCdgayo . ]] "To publicly announce the actual results of the election before the election takes place. Those results should be the actual vote share (i.e. percentage) received by any party concurring to the election." I feel urged to clarify that because I've been told by a few people that I'm confusing "predicting" with "forecasting", and that talking about "predictions" in social science is not actually about making forecasts. I can, however, accept I'm wrong in that regard; after all, Campbell (2004) or Lewis-Beck (2005) used "forecast" instead of "prediction". Nevertheless, since in common English both "predict" and "forecast" are used almost interchangeably I cannot simply accept the use of "prediction" in the sense of post-facto explanation but only as a statement about a future outcome. That said, we have to establish a way to determine if the forecast/prediction was accurate or not. Forecasting/predicting the winner is tempting but you just cannot do that. If you don't accept my word please accept that by Campbell (2004). Hence, you must compare your forecasts for vote shares against the actual results. To that end, MAE (Mean Absolute Error) is commonly used although you can try a number of different measures (see Lewis-Beck 2005 for alternatives). However, MAE is just a number and you cannot tell if your MAE is small (which is good) or large (which is bad). Indeed, you have to compare your MAE against other MAE to tell if your forecasting/predictive method is good enough or not. Certainly, you can try to compare against traditional polls but it could be unfair to compare a method to predict/forecast the future with a method to ask people about their future behavior. One option is to compare your method against traditional electoral forecasting methods [Oh, you didn't know? Figure it... Political scientists have been trying to forecast/predict elections well before Twitter and Facebook, in fact from the early 1990s ?-cf. Lewis-Beck Rice (1992) and Campbell Garand (2000)] but they can be quite cumbersome. Another option is choosing a reasonable baseline. Here I must clarify what a baseline is. A baseline is not a straw-man method so simplistic that it's almost impossible not to beat it. Nope. A baseline is a reasonably simple (but not simplistic) method to produce acceptable results for the problem at hand if no other method is available. Which could be a reasonable baseline for predicting/forecasting elections? Assuming random results? Obviously not. The most reasonable baseline is assuming the future will repeat the past. That is, that in every election in every place the results will be exactly the same as the prior election. And here we are. I guess that some teams worldwide are preparing for the many elections that are going to take place in different countries. Since I cannot prepare baselines for all of them I'm guessing (not predicting nor forecasting) that the US elections are going to attract some interest. Therefore, I've collected (from Wikipedia) results for gubernatorial, senate and house of representative elections and they are my prediction/forecast for 2014. You can access them here . Gubernatorial and senate are rather simple. I provide there the winner (party) for each race, the vote share for the winner and, as a summary, the number of races won by each party. The House of Representatives is trickier since most of the states are divided in many districts. Therefore, I've provided the "winner" in each state, the vote share in each state and the number of seats for each party in each state. Please note, that forecasting seats is not a good choice and, hence, vote share should be predicted for each district. However, to be sincere, it is too much effort for me at this point to prepare all of this one year before elections and almost two years before papers predicting/forecasting those elections are published ;) What's the plan ahead? November 5, 2014: I'll fill the actual results in the spreadsheet and I'll compute MAE for each kind of election. I'll update the post with comments regarding any forecast/prediction made around that date using social media data. Feel free to comment anything about the data or the post itself. You can find me in Twitter: @PFCdgayo . References Campbell, James E. "Introduction ? The 2004 presidential election forecasts." Political Science and Politics 37.04 (2004): 733-735. Campbell, James E., and James C. Garand. "Forecasting US National Elections." Before the Vote: Forecasting American National Elections (2000): 3-16. Lewis-Beck, Michael S. "Election forecasting: principles and practice." The British Journal of Politics International Relations 7.2 (2005): 145-164. Lewis-Beck, Michael S., and Tom W. Rice. Forecasting elections. CQ Press, 1992. ]] 