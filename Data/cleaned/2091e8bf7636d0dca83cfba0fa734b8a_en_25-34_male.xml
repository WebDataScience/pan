 Today I got an external 1TB  drive (made by Western Digital). I needed it to back up my system periodically. When I tried to change partition and file system to NTFS from FAT 32 I ran into a problem. In Administrative Tools - Computer Management - Disk Management snap-in panel hanged on "Connecting to Virtual Disk Service..." After wasting some time searching for answers on the Internet, I decided to run Computer Management as Administrator... Duh... I should have done it right away. So Disk Management snap-in panel opened up fine and I was able to configure partitions on my external hard drive. In order to run as Administrator right click on the program icon and select "Run as administrator". Have a good new year. This problem was conformant to ISolvable interface ;) ]] Well today is my first post in here. I am just trying blog's features. Take Care, Ivan ]] This is a brief summary of SQL Server 2005 symmetric encryption, encryption keys, their hierarchy and usage. At the end of this blog you may find an example script on how to use symmetric key encryption. Symmetric encryption Maximum number of characters which can be encrypted in one function is 7943. Symmetric encryption decrypts 1000 times faster than asymmetric encryption. Symmetric key is stored in the database and could be encrypted by database master key, by certificate and by password. If symmetric key was created using the same KEY_SOURCE, ALGORITHM and IDENTITY_VALUE it would be the same, even if created in different databases. When database backed up or detached symmetric key is kept in the database. When symmetric key is encrypted by password, Triple DES algorithm is used to encrypt symmetric key. Triple DES is weaker than AES. So a key for a stronger encryption is encrypted with a weaker encryption algorithm. It is advised to use certificate to encrypt symmetric key and to use AES 256 algorithm for symmetric encryption. AES encryption works only on Windows Server 2003 and later. (I have not tried it on Win XP SP 3, which came out in May 2008, may it has this encryption already.) Certificate can be encrypted/decrypted by database master key. If database master key is OPEN, then certificate decryption/encryption is applied automatically when you OPEN symmetric key. To OPEN database master key a user must have CONTROL permissions in the database. Database Master Keys. (This is where different articles say different things, so I was trying to give priority to the article with the later date and my experience.) Here is an MSDN article about SQL Server 2005 encryption hierarchy. And below is an overview of the encryption hierarchy. Here is a similar MSDN article only for SQL Server 2008 . And an accompanying MSDN diagram for permissions hierarchy. Here is original MSDN article. When I was writing encryption script for our database I put these two diagrams on the wall in front of me, it helps :). Database Master Key (DMK) can be created with encryption by PASSWORD only. DMK is a symmetric key (according to may 2008 article and public/private key according to 2007 article, so I would go with 2008 J). DMK can be backed up. Other encryption can be added to DMK like so ALTER MASTER KEY ADD ENCRYPTION BY SERVICE MASTER KEY; it is recommended to drop encryption by password and back up the key immediately before using it. Like so : ALTER MASTER KEY DROP ENCRYPTION BY PASSWORD = 'whatever the original password is'; the only problem with that is, IT DOES NOT WORK. MSDN says that a key can not be without any encryption, that is why the error message is thrown, but even when I added encryption by Service Master Key it did not help. Because SMK encryption is in master database only and thus when dropping password encryption it tries to drop it from the original database leaving key unencrypted, which is not allowed. So I have to add some other encryption to DMK before dropping password encryption. What it means is that the key can be accessed in the following 3 ways (assuming the database is restored from FULL database back up or original MDF file is attached.) When database is restored on the computer and SQL instance where the original key was created. When database is restored under the same SQL Server service account which was used when creating original key. When database is restored on a completely different computer and under very different service account, but a correct password for DMK is provided. When you ADD ENCRYPTION BY SERVICE MASTER KEY to DMK it creates a copy of DMK in master database and SQL Server Service account and SQL Server computer instance identity is used to encrypt the key. Thus there are 2 copies of DMK, one in the database itself and encrypted by password and another in master database and encrypted by SMK. When SMK encryption is added for DMK, there is no need to open DMK by password, in fact the whole command can be omitted, DMK gets unencrypted automatically. Which is what we want, because we dont want to provide DMK password in our scripts and if it changes we would not want to update password in all T-SQL code (in a wrapper function in our case, but still we dont want to make it visible even there). No matter how easy it is to decrypt the data I would still create a back up for database master key and put in a safe somewhere. Or Print and store securely all the parameters required to recreate symmetric key. The is_master_key_encrypted_by_server column of the sys.databases catalog view in master database indicates whether the database master key is encrypted by the service master key. Information about the database master key is visible in the sys.symmetric_keys catalog view. Please check that before dropping any other encryptions for DMK. If DMK is not encrypted by server (SMK) then it is better to know how to open DMK manually or add SMK encryption to it. I looked up the syntax for ALTER MASTER KEY and the only encryptions which can be added are SMK and password: ALTER MASTER KEY alter_option alter_option ::= regenerate_option encryption_option regenerate_option ::= [ FORCE ] REGENERATE WITH ENCRYPTION BY PASSWORD = 'password' encryption_option ::= ADD ENCRYPTION BY [ SERVICE MASTER KEY PASSWORD = 'password' ] DROP ENCRYPTION BY [ SERVICE MASTER KEY PASSWORD = 'password' ] But I still dont get why they have DROP password if it does not work? Some articles say that SMK always has password encryption in case database gets detached and needs to be restored. May be this syntax is for the future release. Who knows, it was in SQL Server 2005 books online. The REGENERATE option re-creates the database master key and all the keys it protects. The keys are first decrypted with the old master key, and then encrypted with the new master key. This resource-intensive operation should be scheduled during a period of low demand, unless the master key has been compromised. So for now we are going to have a password for DMK, unless something new comes up. Below are the encryption script and examples on how to use it. Encryption script. Please test variables' sizes before going into production to make sure that data is not truncated. -- ============================================= -- Author: Ivan A -- Copyright 2009 by ISolvable - http://isolved.spaces.live.com/ -- Create date: 07/21/2008 -- Description: PLEASE DO NOT STORE THIS SCRIPT IN PRODUCTION DATABASE. -- This is a script which creates database keys. -- Passwords need to be changed in production. -- ============================================= ALTER PROCEDURE [dbo].[CREATE_KEYS] -- Add the parameters for the stored procedure here AS BEGIN -- this to prevent accidental executions of this stored procedure and to inform a user. RAISERROR (N 'This procedure cannot be executed. Please examine the procedure code first!' ,16,1) /*DO_NOT_EXECUTE,,*/--THIS STORPROC -- BACKUP KEYS. -- DO NOT STORE THIS CODE IN PRODUCTION. -- CODE WITH PASSWORDS TO BE STORED IN A SAFE(TBD). -- BAKUP KEYS TO BE STORED IN A SAFE(TBD). -- THIS CODE HAS ONLY MOCK UP PASSWORD. -- CHANGE PASSWORDS BEFORE USE IN PRODUCTION. -- DO NOT RECREATE KEYS IF SOME DATA IS ALREADY ENCRYPTED. -- DECRYPT DATA FIRST. RECREATE KEYS. THEN ENCRYPT DATA WITH NEW KEYS. -- BACKUP NEW KEYS. -- THE CODE FOR THIS PROCEDURE WOULD BE COMMENTED OUT, TO PREVENT ACCIDENTAL EXECUTION. -- PRESS CTRL-SHIFT-M and fill in the appropriate values. -- THEN EXECUTE CODE SEPARATELY FROM STORED PROCEDURE STATEMENTS. -- ============================================= -- Author: Author,,Name -- Create date: Create Date,, -- ============================================= /* -------------------------------------------------------------------------------------------- -- -- this creates a Database Master Key (DMK) in the original database -- and encrypts MASTER KEY by password. -- MUST MEET WINDOWS PASSWORD POLICY. CREATE MASTER KEY ENCRYPTION BY PASSWORD = 'master_key_password,,password' -- this copies DMK into master database -- and encrypts DMK using SQL Server Service Master Key. -- After this statement is executed DMK would be opened/decrypted automatically. ALTER MASTER KEY ADD ENCRYPTION BY SERVICE MASTER KEY -- this exports DMK into a file (ON THE SERVER NOT ON LOCAL COMPUTER) -- and encrypts it using a password. -- store this file in a safe and secure location. BACKUP MASTER KEY TO FILE = 'Back_up_file_path_on_server,,c:emp' ENCRYPTION BY PASSWORD = 'master_key_password,,password' -- this creates certificates which protects database encryption keys (symmetric or asymmetric). CREATE CERTIFICATE [Cert_Name,,CTrack_Cert] WITH SUBJECT = 'Certificate_Subject,,Key Protection' -- this creates symmetric key and ecnrypts it with a certificate. CREATE SYMMETRIC KEY [Symmetric_Key_Name,,CTrack_Sym_Key] WITH ALGORITHM = Symmetric_Algorithm,,AES_256 ENCRYPTION BY CERTIFICATE [Cert_Name,,CTrack_Cert]; -------------------------------------------------------------------------------------------- -- */ /* -------------------------------------------------------------------------------------------- -- -- Stored Procedures to be created. -------------------------------------------------------------------------------------------- -- SET ANSI_NULLS ON GO SET QUOTED_IDENTIFIER ON GO -- ============================================= -- Author: Author,,Name -- Create date: Create Date,, -- Description: This procedure opens current symmetric key. -- ============================================= CREATE PROCEDURE [dbo].[usp_Sys_Open_Key] -- Add the parameters for the stored procedure here AS BEGIN -- This is to be called subsequently with other stored procedures -- within the same connection session. -- As soon as connection session is closed the key is also closed. -- This opens a key defined in usp_CREATE_KEYS script (not in production) OPEN SYMMETRIC KEY [Symmetric_Key_Name,,CTrack_Sym_Key] DECRYPTION BY CERTIFICATE [Cert_Name,,CTrack_Cert] END GO SET ANSI_NULLS ON GO SET QUOTED_IDENTIFIER ON GO -- ============================================= -- Author: Author,,Name -- Create date: Create Date,, -- Description: This procedure closes current symmetric key. -- ============================================= CREATE PROCEDURE [dbo].[usp_Sys_Close_Key] -- Add the parameters for the stored procedure here AS BEGIN -- This is to be called subsequently with other stored procedures -- within the same connection session. -- As soon as connection session is closed the key is also closed. -- This opens a key defined in usp_CREATE_KEYS script (not in production) CLOSE SYMMETRIC KEY [Symmetric_Key_Name,,CTrack_Sym_Key] END GO -------------------------------------------------------------------------------------------- -- */ /* -------------------------------------------------------------------------------------------- -- -- Functions to be created. -------------------------------------------------------------------------------------------- -- SET ANSI_NULLS ON GO SET QUOTED_IDENTIFIER ON GO -- ============================================= -- Author: Author,,Name -- Create date: Create Date,, -- Description: This function encrypts a string using database symmetric key -- ============================================= CREATE FUNCTION [dbo].[udf_Encrypt] ( -- Add the parameters for the function here @text varchar (7944) ) RETURNS varchar (7988) AS BEGIN -- if provided text length is greater than what can be encrypted than -- an error message is thrown to prevent truncation. if len(@text) 7943 begin DECLARE @temp as int RETURN (1 + 'Argument text should be less than or equal to 7943' ) end -- Declare the return variable here DECLARE @ Result varchar (7988) -- opening and closing key is resource intensive operation -- if encryption needs to be applied multiple times please -- use usp_Sys_Open_Key and usp_Sys_Close_Key stored procedures -- and call encryptbykey function directly from your code. -- usp_Sys_Open_Key procedure does the following. -- OPEN SYMMETRIC KEY [Symmetric_Key_Name,,CTrack_Sym_Key] --DECRYPTION BY CERTIFICATE [Cert_Name,,CTrack_Cert]; -- use encrypt function SELECT @ Result = encryptbykey(key_guid( 'Symmetric_Key_Name,,CTrack_Sym_Key' ),@text) -- close symmetric key using usp_Sys_Open_Key procedure, -- which does the following. -- CLOSE SYMMETRIC KEY [Symmetric_Key_Name,,CTrack_Sym_Key] -- Return the result of the function RETURN @ Result END GO SET ANSI_NULLS ON GO SET QUOTED_IDENTIFIER ON GO -- ============================================= -- Author: Author,,Name -- Create date: Create Date,, -- Description: This function decrypts a string using database symmetric key -- ============================================= CREATE FUNCTION udf_Decrypt ( -- Add the parameters for the function here @text varchar (7988) ) RETURNS varchar (7943) AS BEGIN -- Declare the return variable here DECLARE @ Result varchar (7943) -- opening and closing key is resource intensive operation -- if encryption needs to be applied multiple times please -- use usp_Sys_Open_Key and usp_Sys_Close_Key stored procedures -- and call encryptbykey function directly from your code. -- usp_Sys_Open_Key procedure does the following. -- OPEN SYMMETRIC KEY [Symmetric_Key_Name,,CTrack_Sym_Key] --DECRYPTION BY CERTIFICATE [Cert_Name,,CTrack_Cert]; -- use encrypt function SELECT @ Result = decryptbykey(@text) -- close symmetric key using usp_Sys_Open_Key procedure, -- which does the following. -- CLOSE SYMMETRIC KEY [Symmetric_Key_Name,,CTrack_Sym_Key] -- Return the result of the function RETURN @ Result END GO -------------------------------------------------------------------------------------------- -- */ /* -------------------------------------------------------------------------------------------- -- -- Other ways to use -------------------------------------------------------------------------------------------- -- -- you may use the following command to open the key, -- however the wrapper procedure would be provided, to avoid -- changing key names in multiple code files. OPEN SYMMETRIC KEY [Symmetric_Key_Name,,CTrack_Sym_Key] DECRYPTION BY CERTIFICATE [Cert_Name,,CTrack_Cert]; -- you may use the following command to close the key. CLOSE SYMMETRIC KEY [Symmetric_Key_Name,,CTrack_Sym_Key] -------------------------------------------------------------------------------------------- -- */ /* -------------------------------------------------------------------------------------------- -- -- Procedures to be called from .NET -------------------------------------------------------------------------------------------- -- SET ANSI_NULLS ON GO SET QUOTED_IDENTIFIER ON GO -- ============================================= -- Author: Author,,Name -- Create date: Create Date,, -- Description: This procedure is for use from .NET, -- it encrypts plain text using database symmetric key. -- ============================================= CREATE PROCEDURE [dbo].[usp_Encrypt_BySymKey] -- Add the parameters for the stored procedure here @plaintext as varchar (1000) AS BEGIN -- the size of the encrypted string -- should be at least 60 bytes larger than the original plain text. DECLARE @encrypted as varchar (1060) -- Open symmetric key Exec dbo.usp_Sys_Open_Key -- encrypt plaint text SET @encrypted = dbo.udf_Encrypt(@plaintext) -- close symmetric key Exec dbo.usp_Sys_Close_Key select @encrypted END GO SET ANSI_NULLS ON GO SET QUOTED_IDENTIFIER ON GO -- ============================================= -- Author: Author,,Name -- Create date: Create Date,, -- Description: This procedure is for use from .NET, -- it decrypts into plain text using database symmetric key. -- ============================================= CREATE PROCEDURE [dbo].[usp_Decrypt_BySymKey] -- Add the parameters for the stored procedure here @encrypted as varchar (1060) AS BEGIN -- the size of the encrypted string -- should be at least 60 bytes larger than the original plain text. DECLARE @decrypted as varchar (1060) -- Open symmetric key Exec dbo.usp_Sys_Open_Key -- encrypt plaint text SET @decrypted = dbo.udf_Decrypt(@encrypted) -- close symmetric key Exec dbo.usp_Sys_Close_Key select @decrypted END GO -------------------------------------------------------------------------------------------- -- */ END Summary of instructions on how to use procedures and functions created by the above script. Detailed instructions are in the script comments above. The script creates 2 stored procedures and 2 functions. usp_Sys_Open_Key usp_Sys_Close_Key udf_Encrypt udf_Decrypt When setting up a database, you would need: to open this script. Press CTRL-SHIFT-M Fill in the required parameters. Make sure data is NOT encrypted before recreating the keys. Run code to create stored procedures and functions. Here is an example on how to use these things: DECLARE @original as varchar (100) -- make sure variable for encrypted text is at least 64 bytes larger, or better double it. , @encrypted as varchar (200) , @decrypted as varchar (100) -- creating original text SET @original = 'some text to encrypt' SELECT @original, len(@original) -- open symmetric key Exec usp_Sys_Open_Key -- Encrypt text, make sure you do not trim. -- Do not cast without specifying a size. SET @encrypted = dbo.usp_Encrypt(@original) SELECT len( cast (@encrypted as varbinary(68))), len(@encrypted) -- Do casting like so. This would be correctly decrypted. SET @decrypted = dbo.usp_Decrypt( cast ( cast (@encrypted as varbinary(68)) as varchar (68))) SELECT @decrypted -- always close key or close connection session as soon as key is not needed. Exec usp_Sys_Close_Key -- the return value of the decrypt function is 7943, you may do casting, make sure to specify the size -- like in the following example. drop table temp2 Exec usp_Sys_Open_Key select * , cast (dbo.usp_Decrypt(accnum_enc) as varchar (20)) as accnum_dec into temp2 from temp1 Exec usp_Sys_Close_Key Again this problem turned out to be compatible with ISolvable interface. Ok, I think this is enough for today. It is 11:00 PM on January 15, 2009, and tomorrow I need to wake up at 4:40 am to go to work at 6:am. Need to get some sleep so I can actually work tomorrow :). P.S. The post could have some errors, since I wrote most of it in July of 2008, please write me if you find some problems or have answers to my questions. ]] Have you ever wondered how to grab pictures from a web cam into your .NET application. What if that web cam is remote and password protected. What if it is mobile web cam such as Rovio . What if you like to detect motion events and fire some logic to notify you of such motion. Well, today I would like to walk you through on how to create such application. Before we begin you need to download AForge if you like to have motion sensing capabilities - http://code.google.com/p/aforge/ . "AForge.NET is a C# framework designed for developers and researchers in the fields of Computer Vision and Artificial Intelligence - image processing, neural networks, genetic algorithms, machine learning, etc." You would not need the whole package just several classes. I had to modify some of the original AForge classes to suit my needs, but you are welcome to use original provided you comply with licensing requirements. Part 1 - Figuring out where web cams store their pictures. In my case I had two types of web cams. TRENDnet Wireless Internet Camera Server TV-IP201W and Rovio which I mentioned earlier. By logging in to my web cam server I could see the following page. This web page is generated by a camera embedded web server. My guess that it is running some sort of windows embedded OS since it is rendering aspx pages, but leave this to the reader to figure out since it is irrelevant for this post. This front page image is updated every time you request a page. So my guess was that camera stores temporary images internally and provides a link to those images. The next step is to figure out the exact uri of the image. This was easily done by opening page source and examining html code. Please see the screen shot below. Yes, that weird number is a relative reference to server's folder. Convert it to full uri and you would get something like http://MyCameraServer.com/goform/capture?12378465237865336 . This is our first type of cameras. Now lets explore Rovio, which has a very simple to use set of APIs . There I found that I can just go to something like http://MyCameraServer:MyPort/Jpeg/CamImg0000.jpg , where MyCameraServer:Myport is an address where you set up your Rovio. That's it. Easy. Part 2 - Connecting to remote server. To connect to remote web server we would utilize classes WebRequest and WebResponce from System.Net namespace. It is very simple. First we need to create web request and provide URI, then we need to add credentials, then execute request, which returns response as instance of WebResponce class. Later we can access stream reader of web response and read our data. Below is an example. Dim request As WebRequest Dim response As WebResponse Dim reader As Stream Dim data(8191) As Byte Dim count As Integer Dim total As Integer request = WebRequest.Create(url) request.Credentials = New NetworkCredential( "login name" , "password" ) response = request.GetResponse() reader = response.GetResponseStream() total = 0 Dim myBitmap As Bitmap = New Bitmap(reader, True ) While True count = reader.Read(data, 0, 8192) If count = 0 Then Exit While End If total += 8192 End While reader.Close() Part 3 - Automatic refresh You probably would not want to refresh this picture every time. So here is what I did, there are many ways, I prefer the quickest one :). Set the timer and refresh the picture by timer. It works great, except now if you set it to refresh too often your UI will freeze. So the idea is to load pictures on a separate thread. And for that I have created this simple class ThreadingHelper. Imports System.Threading Public Class ThreadingHelper Private Shared _currentForm As Form ''---------------------------------------------------------------------------- '' methods related to threading ''---------------------------------------------------------------------------- Public Delegate Function DelegateBegin( ByVal endCode As DelegateEnd) As DelegateEnd Public Delegate Sub DelegateEnd() Public Sub BeginCode( ByVal currentForm As Form, _ ByVal _beginCode As DelegateBegin, _ ByVal _endCode As DelegateEnd) _currentForm = currentForm Dim callbackTransfer As AsyncCallback = New AsyncCallback( AddressOf TransferContext) _beginCode.BeginInvoke(_endCode, callbackTransfer, Nothing ) End Sub Public Sub TransferContext( ByVal asyncResult As System.IAsyncResult) Dim asyncResultBegin As Runtime.Remoting.Messaging.AsyncResult Dim delegateBegin As DelegateBegin Dim endCode As DelegateEnd asyncResultBegin = CType (asyncResult, Runtime.Remoting.Messaging.AsyncResult) delegateBegin = asyncResultBegin.AsyncDelegate endCode = delegateBegin.EndInvoke(asyncResult) Try If _currentForm.InvokeRequired Then _currentForm.Invoke(endCode) Else endCode() End If Catch ex As NullReferenceException ''if error occured on a different thread for the purpose of this streaming application we are just going to swallow it. '' write youe handler if you like Catch ex As ObjectDisposedException Catch ex As Exception End Try End Sub End Class Here is how to use it. myThreading = New ThreadingHelper() myThreading.BeginCode( Me , AddressOf GetPicture, AddressOf UpdatePicture) Part 4 - Rest of the code. Imports System.Net Imports System.IO Imports MotionDetection Public Class Form1 Private Const url1 As String = "http://myserver/goform/capture?1078432126434196" Private Const url2 As String = "http://myserver/goform/capture?1073114869312048" Private Const url3 As String = "http://myserver/goform/capture?1225387068534147" Private Const url4 As String = "http://myserver/goform/capture?1225387347966623" Private Const url5 As String = "http://myserver/goform/capture?1082672364580373" Private Const url6 As String = "http://myserver/Jpeg/CamImg0000.jpg" Private cameras(,) As String = { _ { "camera 1" , url1}, _ { "camera 2" , url2}, _ { "camera 3" , url3}, _ { "camera 4" , url4}, _ { "camera 5" , url5}, _ { "rovio 1" , url6} _ } Private Shared _cameraIndex As Integer = 0 Private Shared _motionLevel As Double = 10.0 Private Shared _detectMotion As Boolean = False Private Shared globalBitmap As Bitmap Private Shared _motionDetector As MotionDetector3 Private myThreading As ThreadingHelper Private Shared urlCurr As String = url1 Private Sub Form1_Load( ByVal sender As System. Object , ByVal e As System.EventArgs) Handles MyBase .Load For i As Integer = 0 To (cameras.Length / 2) - 1 ComboBoxCameras.Items.Add(cameras(i, 0)) Next ComboBoxCameras.SelectedIndex = 0 urlCurr = cameras(ComboBoxCameras.SelectedIndex, 1) myThreading = New ThreadingHelper() _motionDetector = New MotionDetector3() _motionDetector.MotionLevelCalculation = True Timer1.Start() End Sub Private Sub LoadFile( ByVal url As String ) Try Dim request As WebRequest Dim response As WebResponse Dim reader As Stream Dim data(8191) As Byte Dim count As Integer Dim total As Integer request = WebRequest.Create(url) request.Credentials = New NetworkCredential( "login name" , "password" ) response = request.GetResponse() reader = response.GetResponseStream() total = 0 Dim myBitmap As Bitmap = New Bitmap(reader, True ) While True count = reader.Read(data, 0, 8192) If count = 0 Then Exit While End If total += 8192 End While reader.Close() If Not globalBitmap Is Nothing Then globalBitmap.Dispose() End If ''to prevent cross threading access to a shared memeber you would need to clone image since it would be stored on the local stack and you would need to update it globalBitmap = myBitmap.Clone( New Rectangle(0, 0, myBitmap.Width, myBitmap.Height), Imaging.PixelFormat.Format32bppRgb) If Not myBitmap Is Nothing Then myBitmap.Dispose() End If response.Close() Catch ex As Exception 'MessageBox.Show(ex.Message) End Try End Sub Private Sub Timer1_Tick( ByVal sender As System. Object , ByVal e As System.EventArgs) Handles Timer1.Tick Try Timer1. Stop () myThreading.BeginCode( Me , AddressOf GetPicture, AddressOf UpdatePicture) Catch ex As Exception MessageBox.Show(ex.Message) Timer1.Start() End Try End Sub Private Function GetPicture( ByVal endCode As ThreadingHelper.DelegateEnd) As ThreadingHelper.DelegateEnd Try LoadFile(urlCurr) Catch ex As Exception End Try Return endCode End Function Private Sub UpdatePicture() Try If Not PictureBox1.Image Is Nothing Then PictureBox1.Image.Dispose() End If Dim myBitmap As Bitmap myBitmap = globalBitmap.Clone( New Rectangle(0, 0, globalBitmap.Width, globalBitmap.Height), Imaging.PixelFormat.Format32bppRgb) If _detectMotion Then _motionDetector.ProcessFrame(myBitmap) If (_motionDetector.MotionLevel * 100 _motionLevel) Then _motionLevel += 10 TextBoxMotionLevel.Text = _motionLevel.ToString() MessageBox.Show( "Motion detected, here we can automatically send e-mail and deliver files over network somehow." ) _motionDetector.Reset() End If End If PictureBox1.Image = myBitmap Timer1.Start() Catch ex As Exception MessageBox.Show(ex.Message) Timer1.Start() End Try End Sub Private Sub Form1_FormClosing( ByVal sender As System. Object , ByVal e As System.Windows.Forms.FormClosingEventArgs) Handles MyBase .FormClosing Timer1. Stop () If Not PictureBox1.Image Is Nothing Then PictureBox1.Image.Dispose() End If End Sub Private Sub ComboBoxCameras_SelectedIndexChanged( ByVal sender As System. Object , ByVal e As System.EventArgs) Handles ComboBoxCameras.SelectedIndexChanged urlCurr = cameras(ComboBoxCameras.SelectedIndex, 1) _cameraIndex = ComboBoxCameras.SelectedIndex End Sub Private Sub CheckBoxDetectMotion_CheckedChanged( ByVal sender As System. Object , ByVal e As System.EventArgs) Handles CheckBoxDetectMotion.CheckedChanged _detectMotion = CheckBoxDetectMotion.Checked End Sub Private Sub ButtonSetMotionLevel_Click( ByVal sender As System. Object , ByVal e As System.EventArgs) Handles ButtonSetMotionLevel.Click Dim newLevel As Double If ( Double .TryParse(TextBoxMotionLevel.Text, newLevel)) Then _motionLevel = newLevel End If End Sub Private Sub ButtonResetDetector_Click( ByVal sender As System. Object , ByVal e As System.EventArgs) Handles ButtonResetDetector.Click _motionDetector.Reset() End Sub End Class And here is the result: We could have almost streaming video :). The picture appears dark, because I have no control over the light switch in a remote facility. but hopefully you get the idea. And finally with motion detector. You could set level of the motion in % and then write your custom code and what to do about detected motion. For example you could send an e-mail with picture attached or using Rovio you could send a specific sound command and alert an "intruder" :). In my next post I will provide a little more explanation on what modifications I made to AForge library. This was a very nice ISolvable problem :) ]] Have you ever tried to inherit a form and then in Visual Studio Designer you got an error message? Something like this: or like this: That is because Visual Studio Designer in order to render base form controls would create an instance of the base form and apparently would try to execute handlers for base form events. If events contain some logic where you call middle tier and connect to a database or make calls that could fail at design time, such as making a request for Principal Permission, this could generate error messages like the ones above, and in turn Designer would not be able to properly render inherited form. There are several principles and ways to avoid such behavior.  Dont forget to recompile your project every time you make changes to base forms. It also helps to close designer window of the inherited form and re open it, that is when new instance of the base form is created and reloaded in the designer. Basic Principles Since now we are aware that code on the base form could be executed in the designer we could design our parent class in a way which would avoid such execution. - In the parameterless constructor for the base (parent) form do not add any logic besides InitializeComponent() - Or create an overloaded constructor with Protected access modifier, which would be executed only by a child form. - Avoid adding logic into Load event of the base form. - And for that matter avoid adding logic into any event handler for the base form... Dont you think this is too much? I do. Ways to prevent base form to execute logic at design time. There is however a useful property that Form has. It is called DesignMode. You may evaluate this property after instance is created and handle is passed to the Designer. Which means do not check this property in constructor, since it would return false. If this property evaluates to True than you could stop your code from executing. Now what if you have multiple base form event handlers and you dont want to spend time modifying these event handlers by adding this condition. For this purpose there is another neat property on the Form, which is called Events. You could dispose all the events on the form if your base form is in the Designer. So you could add the following code in your base form Load event handler. Private Sub MyForm_Load( ByVal sender As System.Object, ByVal e As System.EventArgs) Handles MyBase .Load If Me .DesignMode Then Me .Events.Dispose() Exit Sub End If '' your logic End Sub So this was another ISolvable problem. :) ]] If you happen to have some sort of ActiveX control on your page in an IFrame, such as PDF viewer, then some of the AJAX controls which use PopupExtender will be obscured by the ActiveX or any other browser plug-in which rendering happens out of the main page context. For example menus, and CalendarExtender suffer this problem, since they inherit PopupBehavior. What happens is Ajax PopupExtender behavior explicitly checks for browser version, in particular IE 6. In IE 6 there was a known problem with z-index so you had to use IFrames to render some popup controls, so PopupBehavior checks whether browser is IE 6 and creates an IFrame around control, otherwise no IFrame created, hence the problem in newer IEs and FireFox with browser plug-ins. To fix this problem, first download source code for AJAX Control Toolkit from CodePlex . And find PopupExtender folder in VS project. Then find this function: addBackgroundIFrame : function () Remember to duplicate your effort for debug and release versions of code, since there are two separate files. Now lets take a look at the original code: addBackgroundIFrame : function () { /// summary /// Add an empty IFRAME behind the popup (for IE6 only) so that SELECT, etc., won't /// show through the popup. /// /summary // Get the child frame var element = this .get_element(); if ((Sys.Browser.agent === Sys.Browser.InternetExplorer) (Sys.Browser.version 7)) { var childFrame = element._hideWindowedElementsIFrame; // Create the child frame if it wasn't found if (!childFrame) { childFrame = document.createElement( "iframe" ); childFrame.src = "javascript:'html/html';" ; childFrame.style.position = "absolute" ; childFrame.style.display = "none" ; childFrame.scrolling = "no" ; childFrame.frameBorder = "0" ; childFrame.tabIndex = "-1" ; childFrame.style.filter = "progid:DXImageTransform.Microsoft.Alpha(style=0,opacity=0)" ; element.parentNode.insertBefore(childFrame, element); element._hideWindowedElementsIFrame = childFrame; this ._moveHandler = Function.createDelegate( this , this ._onMove); Sys.UI.DomEvent.addHandler(element, "move" , this ._moveHandler); } // Position the frame exactly behind the element $common.setBounds(childFrame, $common.getBounds(element)); childFrame.style.left = element.style.left; childFrame.style.top = element.style.top; childFrame.style.display = element.style.display; if (element.currentStyle element.currentStyle.zIndex) { childFrame.style.zIndex = element.currentStyle.zIndex; } else if (element.style.zIndex) { childFrame.style.zIndex = element.style.zIndex; } } }, Notice the section above, which has IF statement. Remove that IF statement completely. Recompile your AjaxControlToolKit project and reference this new dll in your ASP.NET project. Drum roll... it works! ActiveX does not render over your popup control anymore. Keep in mind, if you have many IFrames on your page it would slower rendering, so try to keep number of IFrames to a minimum. Another ISolvable problem :). ]] Recently I was working on a WPF single instance application, which hides its main window and shows system tray icon. My task was to restore main window whenever user tries to open another instance of the application. The good thing I have control over source code, which means I can do anything to the target app. However there are 3 bad things related to WPF WPF shortfalls no out of the box support for single instance applications (in WinForms it is just a flag in project properties tab) it is hard to get a handle of main window of another process, if that window is hidden. Process.MainWindowHandle would return 0. when restoring window of a different process WPF does not listen for window events and thus WPF thread does not start rendering, as a result you get black window with XP blue frame around it. 1st problem First problem is easily ISolvable either using Process.GetProcessByName(yourAppName) when returns true and process id is different from current process, then most likely there is another instance of the app is running, unless name of your app for some reason, is the same as some other app running on the box. In that case you can use mutex to solve the problem. In fact using Mutex is a more robust approach. You can find implementations in here . 2nd Problem Second problem turned out to be lengthy and requires use of SharedMemoryFile and a bunch of other APIs. You can see how it is solved in the same article . This article however does not address WPF issue, since it was written prior WPF release. 3d Problem While easily solvable, it took me some time to figure it out. When you have a code like this: '' if mutex was not created that means other instance is running, '' so we need to restore window of other application. If Not IsMutexCreated Then Try Dim mainWindowHanle As IntPtr = System.IntPtr.Zero SyncLock GetType (FilesView) mainWindowHanle = MemoryMappedFile.ReadHandle( "LocalsharedMemoryFilesView" ) End SyncLock If mainWindowHanle IntPtr.Zero Then Dim result As Boolean result = ShowWindowAsync(mainWindowHanle, SW_SHOWDEFAULT) result = SetForgroundWindow(mainWindowHanle) result = UpdateWindow(mainWindowHanle) SetFocus(mainWindowHanle) End If Catch ex As Exception End Try Application.Current.Shutdown() Try _mutex.ReleaseMutex() Catch ex As Exception End Try End If the ShowWindow or ShowWindowAsync function will indeed restore the window, only with a little problem. It is going to be black. As WPF rendering runs on a separate thread and apparently not listening for main window events :(. Notice that I am not using GC.KeepAlive and GC.Collect like in the original article, but I declared mutex as a class member of Application class, which is a main class in WPF applications. In my case reference to mutex object is kept until Application class is disposed, which is when application shuts down. So GC (Garbage Collector) will not reclaim memory occupied by object mutex because it would have active reference. So WPF thread is silent when some other process calls window restore or window show. Well, here is a good thing I mentioned in the beginning. I have complete control of the source code. And it means I can include event listener in the application main message loop and from there restore WPF window. Since I know I am calling ShowWindow and SetFocus I can concentrate on the events which are fired in those two cases. It is most likely would be GotFocus, Activated or IsVisibleChanged events for WPF window. In there I call Show method, and ... it works! Private Sub FilesView_Activated( ByVal sender As System. Object , ByVal e As System.EventArgs) Handles MyBase .Activated Application.Current.MainWindow.Show() End Sub Private Sub FileView_GotFocus( ByVal sender As System. Object , ByVal e As System.Windows.RoutedEventArgs) Handles MyBase .GotFocus Application.Current.MainWindow.Show() ' or simply ' Me.Show() 'if you are already inside main window class End Sub ' this should also work Private Sub FilesView_IsVisibleChanged( ByVal sender As System. Object , ByVal e As System.Windows.DependencyPropertyChangedEventArgs) Handles MyBase .IsVisibleChanged Me .Show() End Sub Thats it. A lot of sweat for a simple problem, but hey it is ISolvable :). ]] Now this was an interesting bug to find. May be it is not a bug, but I cant explain this behavior otherwise. When working with databases and trying to cover impedance mismatch cases it is common to use the following logic: ... Dim primaryUserId As Integer ? ... Using reader As IDataReader = dataAccess.ExecuteReader( "usp_CostCenter_Get" , params) With reader While .Read() returnResult = True costCenter = If (.IsDBNull(0), "" , .GetString(0)) description = If (.IsDBNull(1), "" , .GetString(1)) primaryUserId = If (.IsDBNull(2), Nothing , .GetInt32(2)) '' -- incorrect behavior, '' if condition is true primaryUserId will get 0 instead of Nothing. '' 0 - is default value for type Integer, but not for type Integer? or Nullable(of Integer) '' Nothing - should be the correct value in this case. End While End With End Using ... In case when condition is true you would expect true part of IF statement to execute, while something else happens and primaryUserId receives default value for type Integer not for type Nullable(of Integer) or Integer? . This is how to correct such behavior: ... Using reader As IDataReader = dataAccess.ExecuteReader( "usp_CostCenter_Get" , params) With reader While .Read() returnResult = True costCenter = If (.IsDBNull(0), "" , .GetString(0)) description = If (.IsDBNull(1), "" , .GetString(1)) '' expanding IF statement If .IsDBNull(2) Then primaryUserId = Nothing Else primaryUserId = .GetInt32(2) End If End While End With End Using ... Please let me know if you had experienced this before and agree or disagree with me. Thank you! ]] There are several reasons why Visual Studio can crash. - http://code.msdn.microsoft.com/KB963035 - http://code.msdn.microsoft.com/KB963676 and some other which I dont remember now. But I am not going to talk about the above issues, rather about some other case when Visual Studio could crash. Here is what happens. Visual Studio Designer could create instances of some of the controls if for example you placed a child control on a form the base class for that child would be instantiated, similar if you have referenced external assemblies which have controls that are on a form, those controls could be instantiated, it depends on a control logic. If you are creating a control you have to check for Design time compilation versus run-time. See my previous blog on how to handle it in Win Forms and there is plenty of topics on the web how to handle a similar issue for WPF. Now if control resides in an assembly which is located on a shared or networking folder, then logic of that control would be executed in a different security context, and AccessDenied exception could be thrown. In my case Visual Studio was not handling this exception properly and was crashing. DWatson was executing and collecting crash data but that did help. What should happen is VS IDE should handle such exception and cancel rendering with a proper message and type of the exception. I solved it by copying all referenced assemblies into local project folder, then recompiling project and restarting Visual Studio IDE. I usually copy assemblies for real projects. I ran into this problem by trying to create real quick prototype and was lazy to transfer dlls locally. Try and let me know. This was another ISolvable problem. I am sorry for being out of touch, but I will try to get back and post since I have a lot of things to share. ]] ClickOnce deployment model has many benefits, but it also has some deficiencies. One of these deficiencies is inability to uninstall ClickOnce applications for a different user. Lets take a look at the following scenario. You are an departmental computer admin in a large enterprise where regular users do not have the ability to open Add/Remove Programs, but they can however install ClickOnce applications. When ClickOnce application is installed a folder is created in %USERPROFILE%Local SettingsApps for that application. There is also a registry entry added under HKEY_CURRENT_USERSoftwareMicrosoftWindowsCurrentVersionUninstall . This entry will have an UninstallString for a specific application. Something like this: rundll32.exe dfshim.dll,ShArpMaintain MyClickOnceApplicationName.application, Culture=neutral, PublicKeyToken=9999999999999999, processorArchitecture=msil Lets say there are some obsolete ClickOnce applications which need to be removed from a regular user profile. If you try logging in under regular user account and SHIFT+RightClick on Add/Remove Programs you would be able to invoke Run as..., in this case you would attempt to use your admin credentials. But the list of installed applications will not have the one you are looking for, because Add/Remove Programs will list apps from CURRENT_MACHINE and admin user profile. Now there is a problem. You could of course give necessary privileges to that regular user and let him do it. But what if you cant give such privileges to a user, because of a security concern you have or a company policy, or you lack required permissions? Then there is a better way. You may create an uninstall.cmd script and place it into original application installation folder, like a shared drive or your IIS. Below is script content. Replace MyClickOnceApplication with the name of your application. Use uninstall command line from the registry (see above on how to get that command). 1: echo off 2: 3: cls 4: 5: Echo MyClickOnceApplication... 6: 7: cd c: 8: 9: taskkill /F /IM "MyClickOnceApplication.exe" 10: 11: cls 12: 13: Echo Uninstalling MyClickOnceApplication... 14: 15: cd %USERPROFILE%Start MenuProgramsStartup 16: 17: if exist "MyClickOnceApplication.appref-ms" del "MyClickOnceApplication.appref-ms" 18: 19: cd c:windows 20: rundll32.exe dfshim.dll,ShArpMaintain MyClickOnceApplicationName.application, Culture=neutral, PublicKeyToken=0000000000000000, processorArchitecture=msil This is using VB script and SendKey to auto press OK button during uninstall. 1: On Error Resume Next 2: Set objShell = WScript.CreateObject( "WScript.Shell" ) 3: objShell.Run "taskkill /f /im MyClickOnceApplication*" 4: Dim returnCode 5: returnCode = objShell.Run( "cmd /K CD " + Chr(34) + "%USERPROFILE%Start MenuProgramsStartup" + Chr(34)+ " del " + Chr(34)+ "MyClickOnceApplication.appref-ms" + Chr(34),0, false ) 6: objShell.Run "rundll32.exe dfshim.dll,ShArpMaintain MyClickOnceApplication.application, Culture=neutral, PublicKeyToken=0000000000000000, processorArchitecture=msil" 7: Do Until Success = True 8: Success = objShell.AppActivate( "MyClickOnceApplication" ) 9: Wscript.Sleep 200 10: Loop 11: objShell.SendKeys "OK" Then you may either send a link to the user, or issue required update which in turn calls one of the above scripts. If you are using script without SendKey command then user will see the following window and you would need to instruct user to click OK button. Conclusion Now you know how to remove ClickOnce apps for a different user. Although many of these commands are exposed via APIs you would still need to run ProcMon to monitor registry changes. Simply removing HKEY_USERSRetrieved User SIDSoftwareMicrosoftWindowsCurrentVersionUninstall and related entries in  USERPROFILELocal SettingsApps..  folder corrupts ClickOnce cache. Removing the whole Apps store fixes it, but user looses all ClickOnce apps in that case and related settings. This problem was somewhat ISolvable problem :). ]] Technorati Tags: WPF , Attached Behaviors , XAML , DataGrid , SelectionChanged , ScrollIntoView It is kind of late in the game, but I thought I would cover how to bring WPF DataGrid selected item into view using Attached Behaviors. There is a nice article by Josh Smith which covers Attached Behaviors in more details. I will just cover a specific case with DataGrid. When you are using MVVM sometimes there is a business need to change selected item from ViewModel and when it happens occasionally users will need to manually scroll to that item. This could be very confusing to the user. The desired behavior would be to scroll selected item into the view automatically. There are multiple ways to solve this problem: - Create an event handler for DataGrid.SelectionChanged event. If you however have multiple DataGrids in your project your code behind file will be polluted with these handlers. This is exactly the case why we are using MVVM to avoid code in code behind files and have a clear separation of concerns. - Second approach will require extending DataGrid class and adding desired behavior. This is an overkill, since now everybody on the project will need to remember to use custom DataGrid. And if there are many of them it requires changes. - Third approach using Attached Behaviors is very lightweight, XAML friendly and preserves MVVM separation of concerns.  All you need is to create a separate class which can be used sparingly as developers see fit, it can sit in your project and be handy when need arises. Below is an example of such class I used for DataGrid. namespace MyProject.AttachedBehaviors { public class DataGridBehavior { #region AutoScrollIntoView public static bool GetAutoScrollIntoView(DataGrid dataGrid) { return ( bool )dataGrid.GetValue(AutoScrollIntoViewProperty); } public static void SetAutoScrollIntoView( DataGrid dataGrid, bool value ) { dataGrid.SetValue(AutoScrollIntoViewProperty, value ); } public static readonly DependencyProperty AutoScrollIntoViewProperty = DependencyProperty.RegisterAttached( "AutoScrollIntoView" , typeof ( bool ), typeof (DataGridBehavior), new UIPropertyMetadata( false , OnAutoScrollIntoViewWhenSelectionChanged)); static void OnAutoScrollIntoViewWhenSelectionChanged( DependencyObject depObj, DependencyPropertyChangedEventArgs e) { DataGrid dataGrid = depObj as DataGrid; if (dataGrid == null ) return ; if (!(e.NewValue is bool )) return ; if (( bool )e.NewValue) dataGrid.SelectionChanged += OnDataGridSelectionChanged; else dataGrid.SelectionChanged -= OnDataGridSelectionChanged; } static void OnDataGridSelectionChanged( object sender, RoutedEventArgs e) { // Only react to the SelectionChanged event raised by the DataGrid // Ignore all ancestors. if (!Object.ReferenceEquals(sender, e.OriginalSource)) return ; DataGrid dataGrid = e.OriginalSource as DataGrid; if (dataGrid != null dataGrid.SelectedItem != null ) dataGrid.ScrollIntoView(dataGrid.SelectedItem); } #endregion // AutoScrollIntoView } } Now is XAML you will need to reference the above namespace: xmlns:localBehaviors="clr-namespace:MyProject.AttachedBehaviors" And use it in your DataGrid in the following manner whenever you like to see such behavior. wpfToolkit:DataGrid EnableColumnVirtualization ="True" VirtualizingStackPanel . VirtualizationMode ="Recycling" Grid . Row ="0" DataContext ="{Binding}" ItemsSource ="{Binding Path=Entities}" SelectedItem ="{Binding Path=EntityNavigation.CurrentEntity, UpdateSourceTrigger=PropertyChanged, Mode=TwoWay, Converter={StaticResource ChildToParentEntityViewModelConverter}}" localBehaviors:DataGridBehavior . AutoScrollIntoView ="True" ... / This ( localBehaviors:DataGridBehavior . AutoScrollIntoView ="True" ) last line in the XAML markup above will do it. This was another ISolvable TProblem . Happy Coding! ]] There is enough said about memory leaks in .NET when using events. BING or Google for "weak event". The problem is that when you have an observer for some object's event myObj.MyEvent when observer is no longer needed myObj.MyEvent still keeps reference to observer's event handling delegate. Thus preventing observer from being garbage collected. The solution is to always unsubscribe observer from any events it might listen to before disposing it. Well, this is easier said than done. One way to deal with this issue is to create weak event references. There are many patterns and frameworks which support weak events such as EventAggregator in Prism or in WPF MVVM app template there is WeakEventReference. The other way is to keep reference to event handler and always unsubscribe before disposing. This could be tricky. The third way is very simple but covers only a subset of all possible event scenarios. This is what I will show in here. Suppose you have a handler which is no longer needed after event is fired. Then you can use lambda expression or anonymous delegate and keep reference to it only until it is in the scope. Here is an example. In this example I actually have two handlers which are not needed after either one of them is executed. 1: // declare a reference to event handler 2: EventHandler myEvent1Result = null ; 3: EventHandler myEvent2Result = null ; 4: 5: // when MyEvent1 fires we handle it like so 6: myEvent1Result = (s, ea) = 7: { 8: // unsubscribe from events immedeately 9: (s as MyObject).MyEvent1Event -= myEvent1Result; 10: (s as MyObject).MyEvent2Event -= myEvent2Result; 11: 12: // do some other processing 13: //.... 14: }; 15: 16: // when MyEvent2 fires we handle it like so 17: myEvent2Result = (s, ea) = 18: { 19: // unsubscribe from events immedeately 20: (s as MyObject).MyEvent1Event -= myEvent1Result; 21: (s as MyObject).MyEvent2Event -= myEvent2Result; 22: 23: // do some other processing 24: //.... 25: }; 26: 27: // once we created event handlers now we can subscribe them. 28: myObject.MyEvent1Event += myEvent1Result; 29: myObject.MyEvent2Event += myEvent2Result; Happy coding! ]] Technorati Tags: WPF , DataGrid , ScrollIntoView , bug In my previous post I talked about using Attached Behaviors to scroll selected item into view. It turns out there is a bug in WPF DataGrid and ScrollIntoView could sometimes throw NullReferenceException when VirtualizingStackPanel.IsVirtualizing ="True" . To avoid this exception there was a solution suggested on this forum http://wpf.codeplex.com/Thread/View.aspx?ThreadId=39458 which basically executes ScrollIntoView on a thread with a very low priority. Here is my previous solution with suggested work around. public class DataGridBehavior { #region AutoScrollIntoView public static bool GetAutoScrollIntoView(DataGrid dataGrid) { return ( bool )dataGrid.GetValue(AutoScrollIntoViewProperty); } public static void SetAutoScrollIntoView( DataGrid dataGrid, bool value ) { dataGrid.SetValue(AutoScrollIntoViewProperty, value ); } public static readonly DependencyProperty AutoScrollIntoViewProperty = DependencyProperty.RegisterAttached( "AutoScrollIntoView" , typeof ( bool ), typeof (DataGridBehavior), new UIPropertyMetadata( false , OnAutoScrollIntoViewWhenSelectionChanged)); static void OnAutoScrollIntoViewWhenSelectionChanged( DependencyObject depObj, DependencyPropertyChangedEventArgs e) { DataGrid dataGrid = depObj as DataGrid; if (dataGrid == null ) return ; if (!(e.NewValue is bool )) return ; if (( bool )e.NewValue) dataGrid.SelectionChanged += OnDataGridSelectionChanged; else dataGrid.SelectionChanged -= OnDataGridSelectionChanged; } static void OnDataGridSelectionChanged( object sender, RoutedEventArgs e) { // Only react to the SelectionChanged event raised by the DataGrid // Ignore all ancestors. if (!Object.ReferenceEquals(sender, e.OriginalSource)) return ; DataGrid dataGrid = e.OriginalSource as DataGrid; if (dataGrid != null dataGrid.SelectedItem != null ) { // this is a workaround to fix the layout issue. // otherwise ScrollIntoView should work directly. dataGrid.Dispatcher.BeginInvoke(DispatcherPriority.Loaded, new DispatcherOperationCallback(ScrollItemIntoView),dataGrid); } } static object ScrollItemIntoView( object sender) { DataGrid dataGrid = sender as DataGrid; if (dataGrid != null dataGrid.SelectedItem != null ) { dataGrid.ScrollIntoView(dataGrid.SelectedItem); } return null ; } #endregion // AutoScrollIntoView Happy Coding! ]] This is continuation of my previous post on MVVM project I recently completed. I decided to analyze some of the code metrics. I really like NDepend add-in for Visual Studio, but it is not a free tool. So I had to resort to .NET Reflector and multiple community developed add-ins. Here is a screen shot from CodeMetrics add-in for .NET Reflector . This tool also provides nice statistics for classes, methods  or modules from IL assemblies. Some people like to talk a lot about Cyclomatic Complexity , while it is useful only on a per method basis. Cyclomatic Complexity of the whole project grows dramatically as your project grows and becomes inadequate in such cases. The rule of thumb is if Cyclomatic Complexity greater than 15 the project will have increased difficulty in knowledge transfer, testing, debugging and maintenance. Additionally such code tends to be more rigid which means changes are more difficult to implement. Below is a recommendation given by NDepend: Recommendations : Methods where CC is higher than 15 are hard to understand and maintain. Methods where CC is higher than 30 are extremely complex and should be split in smaller methods (except if they are automatically generated by a tool). In my project there are 14 193 methods, most of the ones with higher cyclomatic complexity were generated and the rest 13 923 have cyclomatic complexity of 9 or less, which is about 98% of all code base. Another useful metrics for types (classes) is  Depth of Inheritance Tree . Here is a recommendation given by NDepend about depth of inheritance tree: Depth of Inheritance Tree (DIT): The Depth of Inheritance Tree for a class or a structure is its number of base classes (including the System.Object class thus DIT = 1). Recommendations : Types where DepthOfInheritance is higher or equal than 6 might be hard to maintain. However it is not a rule since sometime your classes might inherit from tier classes which have a high value for depth of inheritance. For example, the average depth of inheritance for framework classes which derive from System.Windows.Forms.Control is 5.3. In SHMMP Manager Average Depth of Inheritance Tree is 4.08, which is very good. There are other numbers CodeMetrics add-in provides, some you may use to impress some people, usually those unfamiliar with programming :). Such as number of lines of code. In SHMMP Manager there are about 200 000 lines of code. This number is even greater if taken straight from IL disassembler and is 1 295 965, which, if printed, translates to roughly 43 000 pages. Of course 90% of these lines were generated by the compiler :) NDepend can also analyze code for testability and compositionality and provides nice dependency graphs and matrices. Based on Dependency Matrix, for an example, one may start refactoring process. Keeping track of dependencies especially important when building common class libraries or frameworks. So try these tools and see if they are helpful. ]] In this post I will write about some problems I faced with INotifyPropertyChanged while using MVVM and some solutions available. Lets first examine the problem. Is INotifyPropertyChanged an anti-pattern or not? The short answer is yes ? But it really depends on how big is the project. It is very common to use PropertyChanged events inside ViewModels of an MVVM application to propagate changes occurring in your classes back to presentation layer via WPF data binding mechanism. So why the size of the project matters. Well, it all comes down to maintainability of the code base.  PropertyChange events are usually raised by passing property names as string constants (thats a bummer). If the number of view models and their properties is large, the project will have too many hard coded strings. Yes, it is possible to move these strings at the beginning of the class file, and there are very nice tools which can do that for you, such as Refactor! from DevExpress or even Visual Studio itself. But still if you are changing schema and your data model is regenerated then most likely corresponding view models will require changes as well and that is where maintenance becomes a nightmare on projects with more than 10 view models. Add to it all the business complexities and tracking control flow will be almost impossible. So the question basically is do you want to pollute your code with hardcoded string? If code is large  then probably NO, if code is small then I guess its OK. Now lets see what are the options (this list is not extensive). 1. Why not use reflection to get property names, when property names change we can simply recompile? So such solution was proposed by Karl Shifflett  a Program Manager on Patterns and Practices Prism Team, Microsoft Corporation. But later Sacha Barber found a flaw in such approach with Stack Frames. Karl since updated his post and commented on this problem. Basically if you compile your code for release and remove pdb (debug database with initial method and property names) files, due to compiler optimization the property names will not be properly resolved and some calls will end up in the wrong place inside the stack :(. Sacha since proposed his own framework for MVVM called Cinch . While it sounds like a solid piece of work it doesnt really address PropertyChanged problem. 2. Josh Smith blogged about this problem as well and proposed using lambda expressions to validate property names at compile time, thus avoiding StackFrame problem. He used a property observer pattern and created a generic class (PropertyObserverTPropertySource) to handle it along with weak event listeners. Still this approach requires changes to be propagates when properties change. While again there are tools which can do this in semi-automated fashion it could be error prone when there are too many dependencies between properties within and outside of the class. 3. Another approach is to use weak event referencing for all independent properties. Such solution called UpdateControls was proposed by Michael Perry. He has a great set of videos on his web site explaining this approach. And this is wonderful, but it because an overkill when working with large collections. It takes up too much memory to register for every single independent property in lets say a list of 100 000 records. 4. My approach. I decided to use a mix of things from the above. - All my Data Model classes are generated and use Property Changed events. Since there is no business logic which resides in these classes there is no complexity associated in maintaining custom event handlers or raising of events. Everything is generated and regenerated automatically when database schema changes. - For my view models I used a similar approach. But use UpdateControl for the properties. Partial view model classes are generated based off of a database schema (dbml XML model file) using slightly modified T4 template provided by Damien . Here is my template below. # // L2ST4 - LINQ to SQL templates for T4 v0.85 - http://www.codeplex.com/l2st4 // Copyright (c) Microsoft Corporation. All rights reserved. // This source code is made available under the terms of the Microsoft Public License (MS-PL) ##@ template language= "C#v3.5" hostspecific= "True" ##@ include file= "../DataModels/L2ST4.ttinclude" ##@ assembly name= "..My DocumentsVisual Studio 2008ProjectsSHMMPShmmpManagerShmmpManagerinDebugShmmpManager.exe" ##@ import namespace = "ShmmpManager.ViewModels" ##@ output extension= ".generated.cs" ## // Set options here var options = new { DbmlFileName = Host.TemplateFile.Replace( "ViewModel.tt" , "Data.dbml" ).Replace( "ViewModels" , "DataModels" ), // Which DBML file to operate on (same filename as template) SerializeDataContractSP1 = false , // Emit SP1 DataContract serializer attributes FilePerEntity = true , // Put each class into a separate file StoredProcedureConcurrency = false , // Table updates via an SP require @@rowcount to be returned to enable concurrency }; var code = new CSharpCodeLanguage(); var data = new Data(options.DbmlFileName); var manager = Manager.Create(Host, GenerationEnvironment); data.ContextNamespace = ( new string [] { manager.DefaultProjectNamespace }).FirstOrDefault(s = !String.IsNullOrEmpty(s)); data.EntityNamespace = ( new string [] { manager.DefaultProjectNamespace }).FirstOrDefault(s = !String.IsNullOrEmpty(s)); string baseClassName = "EntityViewModelBase" ; string entityBase = "ViewModelBase" ; manager.StartHeader(); ##pragma warning disable 1591 //------------------------------------------------------------------------------ // auto-generated // This code was generated by LINQ to SQL template for T4 C# // Generated at #=DateTime.Now# // The original template was modified by Ivan to meet certain // project related requirements. // // Changes to this file may cause incorrect behavior and will be lost if // the code is regenerated. // /auto-generated //------------------------------------------------------------------------------ using System; using System.Text; using System.Linq; using System.Data.Linq; using System.Data.Linq.Mapping; using System.Collections.Generic; using ShmmpManager.DataModels; # if (data.Functions.Count 0) {# using System.Reflection; #} string dataContractAttributes = (options.SerializeDataContractSP1) ? "IsReference=true" : "" ; if (data.Serialization) {# using System.Runtime.Serialization; #}# using UpdateControls; # manager.EndBlock(); foreach (Table table in data.Tables) { foreach (TableClass class1 in table.Classes) { manager.StartNewFile(Path.ChangeExtension(class1.Name + "ViewModel" , ".generated.cs" )); if (!String.IsNullOrEmpty(data.EntityNamespace)) {# namespace #=data.EntityNamespace#.ViewModels { # } if (data.Serialization class1.IsSerializable) { # [DataContract(#=dataContractAttributes#)] # } if (class1 == table.BaseClass) {# # foreach (TableClass subclass in data.TableClasses.Where(c = c.Table == table)) { if (!String.IsNullOrEmpty(subclass.InheritanceCode)) {# [InheritanceMapping(Code= @"#=subclass.InheritanceCode#" , Type= typeof (#=subclass.Name#)# if (subclass.IsInheritanceDefault) {#, IsDefault= true #}#)] # } if (data.Serialization subclass.IsSerializable) {#[KnownType( typeof (#=subclass.Name#))]#} } # #=code.Format(class1.TypeAttributes)# partial class #=class1.Name#ViewModel : # if (class1.Name == "EquipmentPremiumRisk" || class1.Name == "EquipmentPremiumManaged" || class1.Name == "EquipmentPremiumShared" || class1.Name == "EquipmentPremiumCapped" || class1.Name == "EquipmentPremiumDirectedService" ){ # EquipmentPremiumViewModelBase #} else if (class1.Name == "EquipmentPremiumAdjustmentRisk" || class1.Name == "EquipmentPremiumAdjustmentManaged" || class1.Name == "EquipmentPremiumAdjustmentShared" || class1.Name == "EquipmentPremiumAdjustmentCapped" || class1.Name == "EquipmentPremiumAdjustmentDirectedService" ){ # EquipmentPremiumAdjustmentViewModelBase #} else if (!String.IsNullOrEmpty(entityBase)) {# #=baseClassName# # }# # else { ##=class1.SuperClass.Name# #}}# { #region Fields // list to support error validations protected new static ListPropertyRuleBaseViewModelBase _propertiesRules = new ListPropertyRuleBaseViewModelBase(); // list to support warnings validations. protected new static ListPropertyRuleBaseViewModelBase _propertiesWarnings = new ListPropertyRuleBaseViewModelBase(); // static list to support singleton pattern. private static List#=class1.Name#ViewModel _instances = new List#=class1.Name#ViewModel(); #endregion #region Data Members private #=class1.Name# _#= class1.Name #; #endregion #region Properties public override bool IsNew { get { # if (class1.Name == "Agent" || class1.Name == "Message" || class1.Name == "TypeCoverageDetail" || class1.Name == "ErrorLog" || class1.Name.StartsWith( "vw" ) ){# return _#= class1.Name #.#= class1.PrimaryKey[0].Name # == default (#= class1.PrimaryKey[0].Type #); # } else {# return String.IsNullOrEmpty(_#= class1.Name #.AddedByStaffCode); # }# } } # if (class1.Name != "ErrorLog" !class1.Name.StartsWith( "vw" ) ) { # private Independent _indAudit = new Independent(); public IQueryable#= class1.Name #Audit Audit { get { _indAudit.OnGet(); return from #=class1.Name#Audit c in ShmmpAudit.#=class1.Name#Audits where c.#= class1.PrimaryKey[0].Name # == _#= class1.Name #.#= class1.PrimaryKey[0].Name # orderby c.DateStamp descending select c; } } private Independent _indHistory = new Independent(); public IQueryable#= class1.Name #Audit History { get { _indHistory.OnGet(); return ( new List#= class1.Name #Audit()).AsQueryable(); // TODO: write history query, needs to return IEnumerableENTITYHistory } } public override string HistoryMessage { get { string historyMessage = String.Empty; // getting #=class1.Name# Audit #=class1.QualifiedName#Audit auditRecord = (AuditRecord as #=class1.QualifiedName#Audit); if (auditRecord != null ) { historyMessage = String.Format( "Historical reference for: Date - ({0:d}), Time - ({1:T}), Operation - ({2}), Staff - ({3})." , auditRecord.DateStamp.Date, auditRecord.DateStamp, auditRecord.TableOperation.Description, auditRecord.StaffCode); } //#=class1.Name#History historyRecord = (HistoryRecord as #=class1.Name#History); //if (historyRecord != null) //{ // historyMessage = String.Format("Historical reference for: Bill Period - ({0}).", // historyRecord.BillPeriod.Date); //} return historyMessage; } } # } # #endregion //Properties #region Construction /// summary /// This is static constructor to emulate singleton pattern. /// /summary /// param name="#=class1.Name.ToLower()#"Entity record./param /// param name="baseTable"Base Table which stores current entity./param /// returns/returns public static #=class1.Name#ViewModel Create(#=class1.Name# #=class1.Name.ToLower()#,ITable baseTable) { #=class1.Name#ViewModel instance = null ; //= _instances.FirstOrDefault(i= i.DataEntity == #=class1.Name.ToLower()#); //if (instance == null) //{ if (#=class1.Name.ToLower()# != null ) { if (_propertiesRules == null ) { _propertiesRules = new ListPropertyRuleBaseViewModelBase(); } if (_propertiesWarnings == null ) { _propertiesWarnings = new ListPropertyRuleBaseViewModelBase(); } instance = new #=class1.Name#ViewModel(#=class1.Name.ToLower()#, baseTable); //_instances.Add(instance); } //} return instance; } private #=class1.Name#ViewModel(#=class1.Name# #=class1.Name.ToLower()#,ITable baseTable) { BaseTable = baseTable; DataEntity = #=class1.Name.ToLower()#; _#= class1.Name# = #=class1.Name.ToLower()#; # if (class1.HasPrimaryKey){# // initializing primary and foreign keys # foreach (Column column in class1.Columns) { # #=column.Storage # = _#= class1.Name#.#= column.Member #; # } }# // initializing data context DataContext = baseTable.Context as #=data.ContextName#; _propertyChangedHandler = new System.ComponentModel.PropertyChangedEventHandler(DataEntity_PropertyChanged); _#= class1.Name#.PropertyChanged += _propertyChangedHandler; Initialize(); # if (class1.Name != "ErrorLog" !class1.Name.StartsWith( "vw" ) ) {# _saveHandler = OnSaveEvent; this .SavedEvent += _saveHandler; # }# } partial void Initialize(); #endregion # int dataMemberIndex = 1; if (class1.Columns.Count 0) { # #region Column Mappings # foreach (Column column in class1.Columns) { # //data member private #=code.Format(column.StorageType)# #=column.Storage## if (column.IsReadOnly ) {# = default (#=code.Format(column.StorageType)#)#}#; private static PropertyRuleBaseViewModelBase #=column.Member#PropertyRules; private static PropertyRuleBaseViewModelBase #=column.Member#PropertyWarnings; # if (!column.IsPrimaryKey !column.IsReadOnly){# private Independent _ind#=column.Member# = new Independent(); # }# # # #=code.Format(column.MemberAttributes)## if (((class1.Name == "EquipmentPremiumRisk" || class1.Name == "EquipmentPremiumManaged" || class1.Name == "EquipmentPremiumShared" || class1.Name == "EquipmentPremiumCapped" || class1.Name == "EquipmentPremiumDirectedService" ) ( typeof (EquipmentPremiumViewModelBase) .GetMember(column.Member).FirstOrDefault() != null )) || ((class1.Name == "EquipmentPremiumAdjustmentRisk" || class1.Name == "EquipmentPremiumAdjustmentManaged" || class1.Name == "EquipmentPremiumAdjustmentShared" || class1.Name == "EquipmentPremiumAdjustmentCapped" || class1.Name == "EquipmentPremiumAdjustmentDirectedService" ) ( typeof (EquipmentPremiumAdjustmentViewModelBase) .GetMember(column.Member).FirstOrDefault() != null ))) {# override #}##=code.Format(column.Type)# #=column.Member# { get { # if (!column.IsPrimaryKey !column.IsReadOnly){# _ind#=column.Member#.OnGet(); # } if (column.CanBeNull){# if ( this .IsDisposed || this .DataContext == null || this .DataContext.IsDisposed) return null ; # } if (class1.Name == "ErrorLog" || class1.Name.StartsWith( "vw" ) ) {# return _#= class1.Name#.#=column.Member#; # } else {# return #=column.StorageValue#; # }# } # if (!column.IsReadOnly !column.IsPrimaryKey) { # set { if (#=column.StorageValue# != value ) { _ind#=column.Member#.OnSet(); #=column.StorageValue# = value ; OnPropertyChanged( "#=column.Member#" ); } } # }# } # }# #endregion #region Column Validation Warnings ' Registration public override ListPropertyRuleBaseViewModelBase PropertiesWarnings { get {return _propertiesWarnings;} protected set {_propertiesWarnings = value;} } protected override void AddAllPropertyWarningsDefinitions(#=entityBase# obj) { if (_propertiesWarnings == null) { _propertiesWarnings = new ListPropertyRuleBaseViewModelBase(); } if (_propertiesWarnings.Count == 0) // checking this condition twice to prevent dead locks. { lock (obj) { if (_propertiesWarnings.Count == 0) { // adding properties' warnings collections # foreach (Column column in class1.Columns) {# #=column.Member#PropertyWarnings = new PropertyRuleBaseViewModelBase { PropertyName = "#=column.Member#" }; _propertiesWarnings.Add(#=column.Member#PropertyWarnings); # } # AddPropertyWarnings(); } } } } #endregion #region Column Validation Rules ' Registration public override ListPropertyRuleBaseViewModelBase PropertiesRules { get {return _propertiesRules;} protected set {_propertiesRules = value;} } protected override void AddAllPropertyRulesDefinitions(#=entityBase# obj) { if (_propertiesRules == null) { _propertiesRules = new ListPropertyRuleBaseViewModelBase(); } if (_propertiesRules.Count == 0) // checking this condition twice to prevent dead locks. { lock (obj) { if (_propertiesRules.Count == 0) { // adding properties' rules collections # foreach (Column column in class1.Columns) {# #=column.Member#PropertyRules = new PropertyRuleBaseViewModelBase { PropertyName = "#=column.Member#" }; _propertiesRules.Add(#=column.Member#PropertyRules); # } # AddPropertyRules(); } } } } #endregion #region Methods protected override bool HasChanged() { return ( # int columnIndex = 0; foreach (Column column in class1.Columns){ if (columnIndex!=0){# || # }# #= column.Member # != _#= class1.Name #.#= column.Member # # columnIndex++; }# || base .HasChanged() ); } # if (class1.Name != "ErrorLog" !class1.Name.StartsWith( "vw" ) ) {# private void OnSaveEvent( object sender, EventArgs e) { _indAudit.OnSet(); } # }# #endregion #region Commands #region ReviewAuditCommand # if (class1.Name != "ErrorLog" !class1.Name.StartsWith( "vw" )) {# protected override void DoReviewAuditRefresh() { # foreach (Column column in class1.Columns) { if (!column.IsPrimaryKey !column.IsReadOnly) {# #=column.Member# = (AuditRecord as #=class1.Name#Audit).#=column.Member## if (column.Type.FullName == "System.Int32" || column.Type.FullName == "System.Boolean" || column.Type.FullName == "System.DateTime" || column.Type.FullName == "System.Decimal" || column.Type.FullName == "System.Byte" ){#.Value#}#; # } }# } # }# #endregion #region CurrentRecordCommand # if (class1.Name != "ErrorLog" !class1.Name.StartsWith( "vw" )) {# protected override void DoCurrentRecordRefresh() { base .DoCurrentRecordRefresh(); # foreach (Column column in class1.Columns) { if (!column.IsPrimaryKey !column.IsReadOnly) {# #=column.Member# = _#= class1.Name #.#= column.Member #; # } }# } # }# #endregion #region CancelCommand protected override void DoCancel() { base .DoCancel(); // restore values for all properties # foreach (Column column in class1.Columns){ if (!column.IsReadOnly !column.IsPrimaryKey){# #= column.Member # = _#= class1.Name #.#= column.Member #; # } }# } #endregion #region SaveCommand protected override void DoSave() { IsUpdating = true ; // set values for all properties # foreach (Column column in class1.Columns){ if (!column.IsReadOnly !column.IsPrimaryKey class1.Associations.Where(association = association.ThisKey.FirstOrDefault().Name == column.Name).Count()==0){# _#= class1.Name #.#= column.Member # = # if (column.Type.FullName == "System.String" ) {# ((#=column.Member# == null )?String.Empty : #= column.Member #);#} else {# #=column.Member #;# } # # } }# bool isNew = this .IsNew; # foreach (Association association in class1.Associations){ if (!association.IsMany association.IsForeignKey){# if (#=association.ThisKey.FirstOrDefault().Member#!= _#= class1.Name #.#= association.ThisKey.FirstOrDefault().Member #) { if (isNew) { _#= class1.Name #.#= association.ThisKey.FirstOrDefault().Member # = _#=association.ThisKey.FirstOrDefault().Member#; } else { _#= class1.Name #.#= association.Member# = DataContext.#= association.Type.Table.Member#.Where(entity = entity.#= association.OtherKey.FirstOrDefault().Member# == #=association.ThisKey.FirstOrDefault().Member#).FirstOrDefault(); } } # } }# base .DoSave(); IsUpdating = false ; } protected override void DoErase() { IsUpdating = true ; // set values for all properties # foreach (Column column in class1.Columns){ if (!column.IsReadOnly !column.IsPrimaryKey){# _#= class1.Name #.#= column.Member # = default (#=code.Format(column.Type)#); # } }# IsUpdating = false ; } #endregion #region RefreshCommand protected override void DoRefresh( bool fromDatabase) { if (fromDatabase) { //refresh values from database. DataContext.Refresh(RefreshMode.OverwriteCurrentValues, DataEntity); } # foreach (Column column in class1.Columns){ if (!column.IsReadOnly !column.IsPrimaryKey){# #=column.Member# = _#= class1.Name#.#=column.Member#; # } if (column.IsReadOnly !column.IsPrimaryKey) {# #=column.Storage# = _#= class1.Name #.#= column.Member #; # } }# # foreach (Association association in class1.Associations){ if (!association.IsMany association.IsForeignKey){# _#= class1.Name #.#= association.Member# = DataContext.#= association.Type.Table.Member#.Where(entity = entity.#= association.OtherKey.FirstOrDefault().Member# == _#=association.ThisKey.FirstOrDefault().Member#).FirstOrDefault(); # } }# } #endregion #endregion #region Cleanup protected override void Cleanup() { if (_saveHandler != null ) { this .SavedEvent -= _saveHandler; _saveHandler = null ; } if (_propertyChangedHandler != null _#= class1.Name# != null ) { this ._#= class1.Name #.PropertyChanged -= _propertyChangedHandler; _propertyChangedHandler = null ; } //_#= class1.Name # = null; base .Cleanup(); } #endregion //Cleanup # } if (class1.Associations.Count 0) { # #region Associations # foreach (Association association in class1.Associations) {# private # if (association.IsMany) { #IEnumerable #= association.Type.Name#ViewModel# } else {# #= association.Type.Name#ViewModel#}# _#=association.Member#; //private Independent _ind#=association.Member# = new Independent(); #=code.Format(association.MemberAttributes)## if (association.IsMany) { #IEnumerable#=association.Type.Name#ViewModel# } else {# #= association.Type.Name#ViewModel#} # #=association.Member# { get { if ( this .IsDisposed || this .DataContext == null || this .DataContext.IsDisposed) return null ; # if (class1.Name.StartsWith( "vw" ) || class1.Name == "ErrorLog" ) {# if (_#=association.Member#== null || this .#=association.ThisKey.FirstOrDefault().Name# != this ._#=class1.Name#.#=association.ThisKey.FirstOrDefault().Name# # if (!association.IsMany){# || _#=association.Member#.#=association.OtherKey.FirstOrDefault().Name# != this ._#=class1.Name#.#=association.ThisKey.FirstOrDefault().Name##}#) { # if (association.IsMany) {# List#= association.Type.Name#ViewModel list = new List#= association.Type.Name#ViewModel(); foreach (#=association.Type.Name# entity in DataContext.#=association.Type.Table.Member#.Where(t= t.#=association.OtherKey.FirstOrDefault().Name#== this .#=association.ThisKey.FirstOrDefault().Name#)# if (!association.IsMany){#.FirstOrDefault()#} else {##}#) { list.Add(#=association.Type.Name#ViewModel.Create(entity, DataContext.#=association.Type.Table.Member#)); } _#=association.Member# = list.AsEnumerable(); # } else {# _#=association.Member# = #=association.Type.Name#ViewModel.Create(DataContext.#=association.Type.Table.Member#.Where(t= t.#=association.OtherKey.FirstOrDefault().Name#== this .#=association.ThisKey.FirstOrDefault().Name#)# if (!association.IsMany){#.FirstOrDefault()#} else {##}#,DataContext.#=association.Type.Table.Member#); # } # } return _#=association.Member#; # } else {# //_ind#=association.Member#.OnGet(); //*(HistoryRecord as #=class1.Name#History).#=association.Member#*/ if (_#=association.Member#== null || this .#=association.ThisKey.FirstOrDefault().Name# != this ._#=class1.Name#.#=association.ThisKey.FirstOrDefault().Name# # if (!association.IsMany){# || _#=association.Member#.#=association.OtherKey.FirstOrDefault().Name# != this ._#=class1.Name#.#=association.ThisKey.FirstOrDefault().Name##}#) { # if (association.IsMany) {# List#= association.Type.Name#ViewModel list = new List#= association.Type.Name#ViewModel(); foreach (#=association.Type.Name# entity in DataContext.#=association.Type.Table.Member#.Where(t= t.#=association.OtherKey.FirstOrDefault().Name#== this .#=association.ThisKey.FirstOrDefault().Name#)# if (!association.IsMany){#.FirstOrDefault()#} else {##}#) { list.Add(#=association.Type.Name#ViewModel.Create(entity, DataContext.#=association.Type.Table.Member#)); } _#=association.Member# = list.AsEnumerable(); # } else {# _#=association.Member# = #=association.Member#ViewModel.Create(DataContext.#=association.Type.Table.Member#.Where(t= t.#=association.OtherKey.FirstOrDefault().Name#== this .#=association.ThisKey.FirstOrDefault().Name#)# if (!association.IsMany){#.FirstOrDefault()#} else {##}#,DataContext.#=association.Type.Table.Member#); # } # } # if (!association.Member.StartsWith( "vw" ) association.Member != "ErrorLog" ) {# #=class1.Name#Audit auditRecord = (AuditRecord as #=class1.Name#Audit); if (auditRecord != null ) { var key = auditRecord.#=association.ThisKey.FirstOrDefault().Name## if (association.ThisKey.FirstOrDefault().Type.FullName == "System.Int32" || association.ThisKey.FirstOrDefault().Type.FullName == "System.Boolean" || association.ThisKey.FirstOrDefault().Type.FullName == "System.DateTime" || association.ThisKey.FirstOrDefault().Type.FullName == "System.Decimal" || association.ThisKey.FirstOrDefault().Type.FullName == "System.Byte" ){#.Value#}#; # if (association.IsMany) {# //foreach(#=association.Type.Name#ViewModel vm in _#=association.Member#) //{ // DataModelAuditBase ar = ShmmpAudit.#=association.Type.Name#Audits // .Where(t=t.#=association.OtherKey.FirstOrDefault().Name# == key // t.DateStamp = auditRecord.DateStamp).FirstOrDefault(); // if (ar != null) // vm.AuditRecord = ar; //} List#= association.Type.Name#ViewModel list = new List#= association.Type.Name#ViewModel(); foreach (#=association.Type.Name# entity in DataContext.#=association.Type.Table.Member#.Where(t= t.#=association.OtherKey.FirstOrDefault().Name#==key)# if (!association.IsMany){#.FirstOrDefault()#} else {##}#) { list.Add(#=association.Type.Name#ViewModel.Create(entity, DataContext.#=association.Type.Table.Member#)); } _#=association.Member# = list.AsEnumerable(); # } else {# DataModelAuditBase ar = ShmmpAudit.#=association.Type.Name#Audits .Where(t=t.#=association.OtherKey.FirstOrDefault().Name# == key ((t.TransactionId == auditRecord.TransactionId (t.DateStamp - auditRecord.DateStamp).Minutes 60 ) || (t.DateStamp = auditRecord.DateStamp)) ).FirstOrDefault(); if (ar != null ) { if (_#=association.Member#.AuditRecord != ar) { _#=association.Member#.AuditRecord = ar; } } else _#=association.Member# = #=association.Member#ViewModel.Create( DataContext.#=association.Type.Table.Member#. Where(t= t.#=association.OtherKey.FirstOrDefault().Name#==key)# if (!association.IsMany){#.FirstOrDefault()#} else {##}#,DataContext.#=association.Type.Table.Member#); # }# } # }# return _#=association.Member#; # }# } } # }# #endregion # } # } # if (!String.IsNullOrEmpty(data.EntityNamespace)) {# } # } manager.EndBlock(); } } manager.StartFooter();# #pragma warning restore 1591 #manager.EndBlock(); manager.Process(options.FilePerEntity);# These templates are very similar to ASP, but reading such templates is very hard. So to read them easily there is a special and FREE markup tool developed by Tangible Engineering and a nice intro article written by Oleg Sych. - For collections I use ObservableCollections thus disabling UpdateControls for them and preventing creation of millions of event handlers and events for large collections. - For every object which gets into the view or somehow selected by a user a view model is generated. This dramatically improves performance of UpdateControls while still gives me a no worry approach for my PropertyChange events. Each view model may have custom validation logic or anything else since all classes are generated as partial. - Having business logic in model views also helps working around some limitations of LINQ to SQL 3.5. Such as inability to detach entities, undo operations or working in concurrent scenarios with multiple threads. Having being able to find solution to all these problems I realized that such approach brings many additional benefits to the table. In simple words in my application: - all lists are ObservableCollections of data models. Since data models are also partial classes, they can be easily extended with additional functionality. - users make changes only to view models. Thus having the ability to roll back changes if database errors occur or having the ability to change multiple objects at the same time. Both of these things are impossible with current LINQ to SQL, since all the changes are made directly to the model and there is no way to detach any objects. The whole model needs to be disposed. While disposing the whole model means all change tracking that is built into LINQ to SQL classes is useless. Now in my model I get to enjoy the benefits of change tracking, and the ability to roll back changes. When objects need to be saved (persisted) to a data store, it is done in a separate instance of database DataContext. If errors occur I throw away such temporary data context, since I cant detach entity from the model. And notify users about the error. If everything is fine, the new entity gets attached to the current model, or changed entity is refreshed from the data store. - another benefit is data mining. Since I am using only one data context internally I am able to do joins between multiple lists and run queries of different complexities. Current LINQ to SQL doesnt allow queries to overlap collections from different instances of the same data context. So this was another not so easy, but still ISolvableTProblem. Happy Coding and have a wonderful New Year! ]] I decided to start a small series of post related to one of my recent MVVM projects and the issues I faced and the solutions I found while working on this project. First let me give a little bit of history and an overview. I work at Biomedical Engineering Center and about year and a half ago our company put a freeze on spending as a result multiple project were shelved. This project, lets call it SHMMP Manager, was supposed to be developed by an outside vendor, which we already picked and provided with functional specifications. After the freeze we had to scramble and bail out of the contract. Likely the vendor was very accommodative even though they were loosing a big chunk of money. The decision was made to develop application internally, but only after all the issues with the current application were resolved. Current (now previous) application was developed 10 years ago in Access VBA with SQL Server back end. So it took several month to close outstanding issues and in October of 2009 we began planning and requirements gathering. Even though we had most of the specs developed we felt it would be wise to revisit the document once again and make any changes if necessary. A month after talking to subject matter experts I decided the document was sufficiently complete and started working on designing the applications architecture. Several month into the project I realized that my interrogation of subject matter experts was not thorough enough and had to make multiple adjustments to the applications design. Here is a high level overview of application features: - Automatic PDF web reports generation - Flexible permissions manager for report users - Document imaging, context search and retrieval - Flexible error validations - Robust billing - Full auditing - Automation of business processes which were not automated in the current application. - Better data mining. After requirements, functional specs and database design the next step was to develop data migration module. This module was suppose to convert old data to new structure. While this sounds simple it didnt turn out to be. Old data was dirty, not normalized and had completely different structure. So this step turned into a major cleaning project with multiple corrections to database design. Most of the changes were related to improving enforcement of data integrity. Additional steps were taking to split database into 3 parts: - online processing (tuned for making changes INSERTs and UPDATEs) - analytical processing (tuned for fast retrieval, for reports) - auditing (tracking changes made to the first database) Such separation allowed us to maintain high performance for data entry and reporting, and as a positive side effect this design also helped us to improved our disaster recovery procedures. For example if one database crashes the other two are not affected and may continue operating. On top of this we also have multiple hardware and software redundancies, such as back ups, transaction log shipping, extra server and HDD mirroring. At times it is easier to restore a single incorrect user operation from one of the two extra databases. Constant change requests early on in the project forced me to use code generation techniques. For SQL Server it was pretty straight forward as T-SQL allows to walk the structure of the database. So all the triggers, many stored procedures and two extra databases were automatically generated by a piece of code which was much smaller. This allowed me to rapidly introduce structural changes and generate about 50K lines of code in couple of minutes. It also improved robustness as code was tested and evaluated very frequently while its foot print was very small. For C# generation I had to use Text Templating Transformation Toolkit (T4) provided by Microsoft as part of Visual Studio IDE. Microsoft was already using a script for its LINQ to SQL classes and I decided to use it as a base example to generate view model (business layer) for the application. It turned out to be very helpful. Now it generates about 80K lines of code for all the tables, views and functions in data model. Here is a list of several screen shots from the application. Application Release Notes Users have the ability to change Welcome message, which appears on the reports web site. Web reports web site is auto generated as well. Once report is designed in SQL Server Report Designed it is dropped into Report Server, then using SHMMP Manager authorized users decide who may see what reports and what data on the reports. An external user who received permissions to view particular datasets and reports will see it appear in available reports section. All report parameters are automatically generated based on the XML structure of the selected report. An example of generated report. An example of data entry form with error validations. There are two validation pipeline in the application: Errors and warnings. Errors prevent changes to the database, while warnings notify users of a potential mistake. Users may change multiple settings in the application, such as scanner settings, or size of the font and forms. Or a background picture. But the most complicated piece of the application is data mining. Users may see and navigated to associated records within database thus uncovering problems or learning data easily. Plus some fun animations throughout the app to keep users attention  :) In the next post Ill dive deeper to cover some of the trouble points in developing MVVM apps and my personal goals for the project. ]] In this post I will show how to use Windows Image Acquisition (WIA) interface to scan images and convert them to PDF using converter from my previous post . There is a nice intro article on WIA written by Scott Hanselman several years ago. Please read it, I referred to it several times over the years. There is also some information available on Wikipedia. So what is WIA: WIA is a Microsoft driver model and application programming interface (API) for Microsoft Windows Me and later Windows operating systems that enables graphics software to communicate with imaging hardware such as scanners, digital cameras and Digital Video-equipment. It was first introduced in 2000 as part of Windows Me, and continues to be the standard imaging device and API model through successive Windows versions. It is implemented as an on-demand service in Windows XP and later Windows operating systems. If you are developing for Windows XP you will need to download this tool . It requires one time administrative installation on a client machine. I created a little batch file to install these files: echo off cls echo Copying WIA Files ... cd c: copy /YOUR_INSTALL_FOLDERWIAwiaaut.chi c:windowshelp copy /YOUR_INSTALL_FOLDERWIAwiaaut.chm c:windowshelp copy /YOUR_INSTALL_FOLDERWIAwiaaut.dll c:windowssystem32 RegSvr32 WIAAut.dll echo DONE!!! If you are developing for Windows Vista or Windows 7 you need to know this: In Windows XP, WIA runs in the LocalSystem context. Because of the security ramifications of running a service as LocalSystem whereby a buggy driver or malicious person would have unrestricted access to the system, the WIA service in Windows Server 2003 and Windows Vista operates in the LocalService context. This can result in compatibility issues when using a driver designed for Windows XP. Windows Vista has the WIA Automation library built-in. Also, WIA supports push scanning and multi-image scanning . Push scanning allows initiating scans and adjusting scanning parameters directly from the scanner control panel. Multi-image scanning allows you to scan several images at once and save them directly as separate files. However, video content support is removed from WIA for Windows Vista. Microsoft recommends using the newer Windows Portable Devices API. Now on to the hardware. Since in my project I needed to scan multiple pages at once, I used Fujitsu fi-5110C and newer model (works just as well) fi-6110. The problem is that in Windows XP for some reason these scanners dont report correctly the Empty Tray status. So I had to assume some things for multi page scanning. The logic is simple: - scan until no exception. - when exception occurs checks if anything is scanned. - if pages were scanned then simply exit. - if no pages were scanned then report a problem. Here is a snippet responsible for this: // if we reached this point, then scanner is probably initialized. bool isTransferring = true ; foreach ( string format in item.Formats) { while (isTransferring) { try { WIA.ImageFile file = (item.Transfer(format)) as WIA.ImageFile; if (file != null ) { Stream stream = new MemoryStream(); stream.Write(file.FileData.get_BinaryData() as Byte[], 0, (file.FileData.get_BinaryData() as Byte[]).Length); // resetting stream position to beginning after data was written into it. stream.Position = 0; Bitmap bitmap = new Bitmap(stream); images.Add(bitmap); } else isTransferring = false ; // something happend and we didn't get image } catch (Exception ex) { // most likely done transferring // I was not able to find a way to pole scanner for paper feed status. isTransferring = false ; // scanner's paper feeder was not loaded with paper. if (images.Count() == 0) throw new ImageScannerException( "Scanner is not loaded with paper or not ready." ); } } } Thats multi page scanning in Windows XP, unlike earlier statement from Wiki that it is supported only in Vista. Another not so straight forward part is how to initialize the scanner. By default when you call WIA API to scan it loads manufacturers interface. In my case I wanted to save all the settings using internally developed user form and from then on pass these settings on to a scanner every time scanner is used. I created a class to store scanner settings. ScannerSettings: using System; using System.Collections.Generic; using System.Linq; using System.Text; using System.Runtime.Serialization; using System.Security.Permissions; namespace Bmec.ScanLibrary.Scan { public enum ScanningSize { Letter, Legal } public enum Color { GrayScale=0, Color=1, BlackWhite=4 } public class ScannerSettings: ISerializable { #region Fields /// summary /// Width of the scanned image. /// Default is 8500. /// /summary private double _width = 8.5; /// summary /// Height of the scanned image. /// Default is 11000. /// /summary private double _height = 11; /// summary /// Optical Resolution. /// /summary private int _resolution = 120; /// summary /// Color setting, default is 0 - grayscale. /// /summary private int _color = 0; // grayscale /// summary /// Horizontal Cropping. /// Default is 0.5 in /// /summary private double _horizontalCrop = 0.5; // if cropping required it can be set in here for horizontal. /// summary /// Vertical Cropping /// Default is 0.5 in /// /summary private double _verticalCrop = 0.5; // if cropping required it can be set in here for vertical. /// summary /// Standard scanning size /// /summary private ScanningSize _size = ScanningSize.Letter; #endregion //Fields #region Constructors /// summary /// Default Constructor /// /summary public ScannerSettings() { } /// summary /// Creates settings object for WIA scanner. /// /summary /// param name="size"Standard paper size./param /// param name="resolution"scanning resolution (e.g. for 300x300 pass 300)./param /// param name="color"Color setting, default is gray scale./param public ScannerSettings(ScanningSize size, int resolution, Color color) { Size = size; _color = ( int )color; _resolution = resolution; } /// summary /// Creates settings object for WIA scanner and resolution of 150x150 pixels. /// /summary /// param name="size"Standard paper size./param public ScannerSettings(ScanningSize size) { Size = size; } /// summary /// Creates customized settings for WIA scanner /// /summary /// param name="width"Scanner's sheet feed width./param /// param name="height"Scanner's sheet feed height./param /// param name="resolution"Optical resolution./param public ScannerSettings( double width, double height, int resolution, double horizontalCrop, double verticalCrop) { Width = width; Height = height; _resolution = resolution; _horizontalCrop = horizontalCrop; _verticalCrop = verticalCrop; } #endregion //Constructors #region Properties /// summary /// Color setting, default is 0 - grayscale. /// /summary public int Color { get { return _color; } set { _color = value ; } } /// summary /// Height of the scanned image. /// Default is 11 in. /// /summary public double Height { get { return _height; } set { _height = value ; } } /// summary /// Horizontal Cropping. /// Default is 0.5 in. /// /summary public double HorizontalCrop { get { return _horizontalCrop; } set { _horizontalCrop = value ; } } /// summary /// Optical Resolution. Default is 120 pixels per inch. /// /summary public int Resolution { get { return _resolution; } set { _resolution = value ; } } /// summary /// Standard scanning size /// /summary public ScanningSize Size { get { return _size; } set { _size = value ; Width = GetWidth(_size); Height = GetHeight(_size); } } /// summary /// Vertical Cropping /// Default is 0.5 in. /// /summary public double VerticalCrop { get { return _verticalCrop; } set { _verticalCrop = value ; } } /// summary /// Width of the scanned image. /// Default is 8.5 in. /// /summary public double Width { get { return _width; } set { _width = value ; } } #endregion //Properties #region Methods public static double GetWidth(ScanningSize size) { if (size == ScanningSize.Legal) { return 8.5; } else if (size == ScanningSize.Letter) { return 8.5; } else return 8.5; } public static double GetHeight(ScanningSize size) { if (size == ScanningSize.Legal) { return 14; } else if (size == ScanningSize.Letter) { return 11; } else return 11; } #endregion //Methods #region ISerializable Members /// summary /// Constructor for serializer. /// /summary /// param name="info"Serialization data./param /// param name="context"Serialization streaming context./param public ScannerSettings(SerializationInfo info, StreamingContext context) { Color = info.GetInt32( "Color" ); Height = info.GetDouble( "Height" ); HorizontalCrop = info.GetDouble( "HorizontalCrop" ); Resolution = info.GetInt32( "Resolution" ); NullableScanningSize size = info.GetValue( "Size" , typeof (NullableScanningSize)) as NullableScanningSize; Size = ((size != null )? size.Value : ScanningSize.Letter); VerticalCrop = info.GetDouble( "VerticalCrop" ); Width = info.GetDouble( "Width" ); } /// summary /// Implementation for ISerializable. /// /summary /// param name="info"Serialization data./param /// param name="context"Serialization streaming context./param [SecurityPermission(SecurityAction.LinkDemand, Flags = SecurityPermissionFlag.SerializationFormatter)] public void GetObjectData(SerializationInfo info, StreamingContext context) { info.AddValue( "Color" , Color); info.AddValue( "Height" , Height); info.AddValue( "HorizontalCrop" , HorizontalCrop); info.AddValue( "Resolution" , Resolution); NullableScanningSize size = Size; info.AddValue( "Size" , size); info.AddValue( "VerticalCrop" , VerticalCrop); info.AddValue( "Width" , Width); } #endregion } } And then apply these settings in the following manner: // setting properties (dimensions and resolution of the scanning.) setItem(item, "6146" , _scannerSettings.Color); // color setting (default is gray scale) setItem(item, "6147" , _scannerSettings.Resolution); //horizontal resolution setItem(item, "6148" , _scannerSettings.Resolution); // vertical resolution setItem(item, "6149" , _scannerSettings.HorizontalCrop); // horizontal starting position setItem(item, "6150" , _scannerSettings.VerticalCrop); // vertical starting position setItem(item, "6151" , ( int )(( double )_scannerSettings.Resolution * (_scannerSettings.Width - _scannerSettings.HorizontalCrop))); // width setItem(item, "6152" , ( int )(( double )_scannerSettings.Resolution * (_scannerSettings.Height - _scannerSettings.VerticalCrop))); // height Another annoying window is selecting the scanner itself: This is taken care by saving device id into local user settings and then reading it off: #region Private Methods private void setItem(IItem item, object property, object value ) { WIA.Property aProperty = item.Properties.get_Item( ref property); aProperty.set_Value( ref value ); } private Device GetDevice(Settings settings) { Device device= null ; CommonDialogClass dialog = new CommonDialogClass(); if (String.IsNullOrEmpty(settings.DeviceId)) { device = dialog.ShowSelectDevice(WiaDeviceType.ScannerDeviceType, true , false ); if (device != null ) { settings.DeviceId = device.DeviceID; settings.Save(); } } return device; } private Device GetDevice( string deviceId) { WIA.DeviceManager manager = new DeviceManager(); Device device= null ; foreach ( DeviceInfo info in manager.DeviceInfos) { if (info.DeviceID == deviceId) { device = info.Connect(); break ; } } return device; } #endregion //Private Methods Here is the rest of the code. Scanner initialization was a little tricky without fully understanding the WIA API. So I had to resort to several hacks and empirical methods :). Below are the other two classes you will need to scan images. ImageScanner: using System; using System.Collections.Generic; using System.Linq; using System.Text; using Bmec.ScanLibrary.Properties; using System.Configuration; using WIA; using System.Drawing; using System.IO; using System.Security.Permissions; using System.Security.Principal; [assembly: System.Security.Permissions.FileIOPermission(SecurityAction.RequestMinimum, Unrestricted = true )] namespace Bmec.ScanLibrary.Scan { public class ImageScanner { #region Fields private Device _device= null ; private ScannerSettings _scannerSettings; #endregion //Fields #region Constructors /// summary /// /// /summary /// exception cref="ImageScannerException"This exception can be thrown, /// if any errors are present while initializing scanner./exception public ImageScanner(ScannerSettings scannerSettings) { AppDomain.CurrentDomain.SetPrincipalPolicy(PrincipalPolicy.WindowsPrincipal); if (scannerSettings == null ) throw new ImageScannerException( "Scanner Settings are not specified!" ); _scannerSettings = scannerSettings; Settings settings = new Settings(); try { // try automatically select scanner. if (!String.IsNullOrEmpty(settings.DeviceId)) { _device = GetDevice(settings.DeviceId); } // if didn't succeed try manually select scanner. if (_device == null ) { _device = GetDevice(settings); } } catch (Exception ex) { throw new ImageScannerException( "Scanner was not selected or is not available!" +ex.Message); } // if device is still null, then throw an error. if (_device == null ) throw new ImageScannerException( "Scanner was not selected or is not available!" ); } #endregion //Constructors #region Public Methods public void SetDevice() { Settings settings = new Settings(); _device = GetDevice(settings); } /// summary /// This will scan images, but everything must be ready before doing this. /// If device (scanner) is not ready an ImageScannerException will be thrown. /// /summary /// returnsCollection of scanned images./returns public IEnumerableBitmap Scan() { ListBitmap images = new ListBitmap(); if (_device == null ) throw new ImageScannerException( "Scanner is not available! Please select a scanner and try again." ); WIA.Item item = null ; try { foreach (DeviceCommand command in _device.Commands) { item = _device.ExecuteCommand(command.CommandID); } } catch (Exception ex) { // skip this } try { // if item is still not initialized, we'll try a different approach if (item == null ) { foreach (Item i in _device.Items) { foreach (DeviceCommand command in i.Commands) { item = _device.ExecuteCommand(command.CommandID); } } } } catch (Exception ex) { // skip this } try { // if item is still null, we'll pick the first available foreach (WIA.Item i in _device.Items) { item = i; break ; } } catch (Exception ex) { //skip this } if (item == null ) throw new ImageScannerException( "Scanner is not ready!Please turn scanner on, feed paper into the scanner and try again." ); try { // setting properties (dimensions and resolution of the scanning.) setItem(item, "6146" , _scannerSettings.Color); // color setting (default is gray scale) setItem(item, "6147" , _scannerSettings.Resolution); //horizontal resolution setItem(item, "6148" , _scannerSettings.Resolution); // vertical resolution setItem(item, "6149" , _scannerSettings.HorizontalCrop); // horizontal starting position setItem(item, "6150" , _scannerSettings.VerticalCrop); // vertical starting position setItem(item, "6151" , ( int )(( double )_scannerSettings.Resolution * (_scannerSettings.Width - _scannerSettings.HorizontalCrop))); // width setItem(item, "6152" , ( int )(( double )_scannerSettings.Resolution * (_scannerSettings.Height - _scannerSettings.VerticalCrop))); // height } catch (Exception ex) { throw new ImageScannerException( "Was not able to set scanning parameters." + ex.Message); } // if we reached this point, then scanner is probably initialized. bool isTransferring = true ; foreach ( string format in item.Formats) { while (isTransferring) { try { WIA.ImageFile file = (item.Transfer(format)) as WIA.ImageFile; if (file != null ) { Stream stream = new MemoryStream(); stream.Write(file.FileData.get_BinaryData() as Byte[], 0, (file.FileData.get_BinaryData() as Byte[]).Length); // resetting stream position to beginning after data was written into it. stream.Position = 0; Bitmap bitmap = new Bitmap(stream); images.Add(bitmap); } else isTransferring = false ; // something happend and we didn't get image } catch (Exception ex) { // most likely done transferring // I was not able to find a way to pole scanner for paper feed status. isTransferring = false ; // scanner's paper feeder was not loaded with paper. if (images.Count() == 0) throw new ImageScannerException( "Scanner is not loaded with paper or not ready." ); } } } return images; } #endregion //Public Methods #region Private Methods private void setItem(IItem item, object property, object value ) { WIA.Property aProperty = item.Properties.get_Item( ref property); aProperty.set_Value( ref value ); } private Device GetDevice(Settings settings) { Device device= null ; CommonDialogClass dialog = new CommonDialogClass(); if (String.IsNullOrEmpty(settings.DeviceId)) { device = dialog.ShowSelectDevice(WiaDeviceType.ScannerDeviceType, true , false ); if (device != null ) { settings.DeviceId = device.DeviceID; settings.Save(); } } return device; } private Device GetDevice( string deviceId) { WIA.DeviceManager manager = new DeviceManager(); Device device= null ; foreach ( DeviceInfo info in manager.DeviceInfos) { if (info.DeviceID == deviceId) { device = info.Connect(); break ; } } return device; } #endregion //Private Methods } } ImageScannerException: using System; using System.Runtime.Serialization; using System.Security.Permissions; namespace Bmec.ScanLibrary.Scan { public class ImageScannerException : Exception, ISerializable { #region Properties public string UserFriendlyMessage { get; private set; } #endregion //Properties #region Constructors /// summary /// All messages are set to String.Empty. /// /summary public ImageScannerException() : base (String.Empty) { UserFriendlyMessage = String.Empty; } /// summary /// UserFriendlyMessage is set to message. /// /summary /// param name="message"Error message./param public ImageScannerException( string message) : base (message) { UserFriendlyMessage = message; } /// summary /// UserFriendlyMessage is set to message. /// /summary /// param name="message"Error Message/param /// param name="innerException"Inner Exception if any, null otherwise. /param public ImageScannerException( string message, Exception innerException) : base (message, innerException) { UserFriendlyMessage = message; } /// summary /// Custom exception for ImageScanner. /// /summary /// param name="message"Detailed error message./param /// param name="userFriendlyMessage"User friendly error message./param public ImageScannerException( string message, string userFriendlyMessage) : base (message) { UserFriendlyMessage = userFriendlyMessage; } /// summary /// Custom exception for ImageScanner. /// /summary /// param name="message"Detailed error message./param /// param name="innerException"Inner Exception if any, null otherwise. /// (If inner exception is null use different overloaded constructor)/param /// param name="userFriendlyMessage"User friendly error message./param public ImageScannerException( string message, Exception innerException, string userFriendlyMessage) : base (message, innerException) { UserFriendlyMessage = userFriendlyMessage; } /// summary /// Custom exception for ImageScanner. /// /summary /// param name="info"Serialization data./param /// param name="context"Serialization streaming context./param public ImageScannerException(SerializationInfo info, StreamingContext context) : base (info, context) { UserFriendlyMessage = info.GetString( "UserFriendlyMessage" ); } #endregion //Constructors #region Methods /// summary /// Implementation for ISerializable. /// /summary /// param name="info"Serialization data./param /// param name="context"Serialization streaming context./param [SecurityPermission(SecurityAction.LinkDemand, Flags = SecurityPermissionFlag.SerializationFormatter)] public override void GetObjectData(SerializationInfo info, StreamingContext context) { base .GetObjectData(info, context); info.AddValue( "UserFriendlyMessage" , UserFriendlyMessage); } #endregion //Methods } } Now to be able to easily use these classes I created a helper class which combines scanning functionality with PDF conversion: ScannerHelper: using System; using System.Collections.Generic; using System.Linq; using System.Text; using Bmec.ScanLibrary.Converters; using System.IO; namespace Bmec.ScanLibrary.Scan { public class ScannerHelper { public ScannerSettings ScannerSettings { get; set; } public PdfSettings PdfSettings { get; set; } /// summary /// Constructor with default settings. /// /summary public ScannerHelper() { ScannerSettings = new ScannerSettings(ScanningSize.Letter,120, Color.BlackWhite); PdfSettings = new PdfSettings(PdfOrientation.Portrait, PdfSize.Letter); } /// summary /// Customizable constructor /// /summary /// param name="scannerSettings"Settings for the scanner./param /// param name="pdfSettings"Settings for PDF page(s)./param public ScannerHelper(ScannerSettings scannerSettings, PdfSettings pdfSettings) { ScannerSettings = scannerSettings; PdfSettings = pdfSettings; } public void ScanToPdf( string newPdfFileName) { ImageScanner scanner = new ImageScanner(ScannerSettings); PdfConverter converter = new PdfConverter(PdfSettings); converter.SaveFrom(scanner.Scan(), newPdfFileName); } /// summary /// This method scans to Pdf Stream. /// /summary /// returnsReturn open stream with PDF binary data in it. The stream needs to be flushed and closed after use./returns public Stream ScanToPdf() { ImageScanner scanner = new ImageScanner(ScannerSettings); PdfConverter converter = new PdfConverter(PdfSettings); return converter.ConvertFrom(scanner.Scan()); } } } Here are couple of overloads. One lets you scan into a file, the other one returns stream of PDF data, which you may use in your program. Thats it for scanning. ]] While reading forums and other discussions, I often notice that it is difficult for developers to  see practical applications of Covariance and Contravariance in Generics. This is due in part because of our Greco-Roman approach to learning. In this approach you learn as much as you can and when you face a real life problem you will try to dig through your memory archives in a futile attempt to find needed pieces of information or theoretical concepts that you have learned 10-20 years ago. This approach has not served me well over the years. I had 5 years of advance chapters of math in college but most of what I remember to this day is the names of those chapters and not much of the content. I recently needed to use Fourier series and could not remember the formula, thanks to God for giving wonderful ideas to some people who invented search engines. What we would be doing these days without search engines - I dont know. Therefore I am a firm believer of a different approach often seen in some middle eastern and Asian cultures. Face the problem first, try to solve it and learn as you go. I found that knowledge retention in this approach is so much greater. In addition you see practical application of a concept right there. Seeing this becomes even easier when your salary depends on solving such problems :).  This was the case with Covariance and Contravariance for me. I vaguely remembered the concept from computer science lectures, while the concept is simple, it is easy to forget it without needed practice. So when I faced a real problem, initially I have not associated it with the concept I learned earlier, but I went on to discovery route and found the answer! The Aha moment filled my brain :) this is something I learned before I thought to myself. So I had to re-introduce myself to the concept. To cover some of these learning gaps I decided to write this short article starting it with a discussion about a problem! Lets say there is some hierarchy of classes in a program and there is class Child which inherits from class Parent. Now there is a need to implement a method which will take a collection of objects of type Parent and do something very generic over these objects. So the method signature might look something like this: public void DoSomething(ListParent collection) { foreach (Parent p in collection) { p.SomeMethod(); } } Pretty simple, no magic here. Now the problem comes when you want to pass a collection of Child classes which also implement SomeMethod(). Logically it is known to the compiler that Child implements SomeMethod and is of type Parent and there should be no problem, but in reality it might not work. When you have something like the following it will work without problems: public void DoSomething(Parent object ) { object .SomeMethod(); } Wait a minute, isnt it a simple polymorphism? Yes it is. But when will passing ListChild not work? In all versions of C# prior to version C# 4.0 . Why? Because it doesnt support contravariance for generic types.While Parent and Child polymorphic through inheritance, ListParent and ListChild are not polymorphic :). Let me get back to this a little later, but for now let me give a real world example first. Microsoft has implemented great WPF binding mechanism. Suppose you have a common interface which supports binding to ListParent and there is a specific data template which can expose some of the common properties of objects of type Parent. When you try to implement collections of classes inherited from Parent and pass it via WPF data binding mechanism the data template will not be applied to the list but instead implementation of ToString() will be used. So instead of seeing neatly formatted objects you will get a list of object names: MyNamespace.Child MyNamespace.Child MyNamespace.Child etc. Here is an example of what you may see when DataTemplate is not properly applied: There are some other scenarios when such behavior may be observed, but it is a separate discussion. In this case default implementation of ToString(), unless it is explicitly overridden in Child class, is simply object.GetType().ToString(). So what most people end up doing is using LINQ or lambda function to quickly convert objects from type Child to type Parent like so: ListChild list = new ListChild(); // populate list ... // call DoSomething which accepts ListParent DoSomething(list.Select(t = t as Parent)); Not a big deal in a single case, but on large projects you might end up writing a lot of converters or manually converting collections like in the above example. This conversion (boxing-unboxing) may also negatively affect performance, although I have not tested it. Calling converters from WPF data binding pipeline generally reduces performance, this is one of the best practices advice for Windows Phone 7 development. Now lets get back to the previous example where we had DoSomething(Parent object) and explain in detail what happens in here when instance of Child is passed as a parameter. This is very common practice among developers and easy to understand, so lets now define this process. In theoretical Computer Science this process is called Contravariance. Here is a definition from Wikipedia ( http://en.wikipedia.org/wiki/Covariance_and_contravariance_(computer_science) ): Within the type system of a programming language, covariance and contravariance refers to the ordering of types from narrower to wider and their interchangeability or equivalence in certain situations (such as parameters, generics, and return types). covariant: converting from wider (double) to narrower (float). contravariant: converting from narrower (float) to wider (double). invariant: Not able to convert. In our example we were implicitly converting from a narrower Child type to a wider Parent type. In this case it is also called polymorphism and was supported for a long time in many programming languages. Lets take a look at our very first example. This however is not polymorphism but it does look like a good case for contravariance. Up until C# 4.0 type ListChild could not be implicitly converted to ListParent while Parent and Child are from the same hierarchy and can be converted implicitly. That is why there was a need for explicit conversion from a narrower type to a wider type. Now in C# 4.0 a support for Covariance and Contravariance in Generics was added. So such lists may be converted implicitly as in simple polymorphism. There are some limitation, however, for more details on this please refer to the following MSDN article ( http://msdn.microsoft.com/en-us/library/dd799517.aspx ). Hope this helps, please let me know if I introduced any misconceptions. :) and if it another ISolvableT problem. Happy coding! Technorati Tags: C# 4.0 , Contravariance , Covariance , Polymorphism , Object Oriented Programming , OOP , WPF , data binding , ListT ]] Technorati Tags: Wi-Fi , antenna , signal strength , dish , can Today I will show fast, inexpensive and easy way to boost signal strength of your Wi-Fi receiver. How fast? It took me about 1 hour to build two antennas with three little boys constantly running around and asking their questions. How cheap? I spent $5.99 + plus tax for each dish, and $20.00 for a Wi-Fi adapter from Amazon. You can be really cheap and use empty cans from canned food. How easy? I took many pictures so that every step is clear. Trust me it is very easy . I am going to be building two different antennas. Dish high gain antenna, which is based on the same principle as satellite dish antennas. Can high gain antenna, which is based on EM wave guiding principle. Basically it is just a filter for all other signals except the one in our frequency ~ 2.4 GHz. Here is a link to a theory and some other examples . Notice the can diameter calculator on that page, please use it before you make any purchases. At the end I will compare performance of these two antennas, regular Omni antenna and adapters internal antenna. CANTENNA I built my first antenna from tomato can about seven and a half years ago when I was a student and needed cheap Wi-Fi connection. Here is a picture: It worked very well but was a bit more expensive than what I did today. Primarily because of the tiny coaxial connecter and cable I used to connect to my Wi-Fi card, which was PCMCIA. PCMCIA stands for People Cant Memorize Computer Industrial Abbreviature... just kidding. But anyway the budget for this antenna was around $30.00. The foil wrapping you can see on the picture is just to make it look more than what it actually is and doesnt bear much functionality . This funnel suppose to increases the sensitivity of the antenna simply by collecting HF signal from larger area. The claimed 3dB improvement by the funnel was not achieved however. But with the above antenna I was able to extend Wi-Fi range to about 1 mile, from 200-300 yards of manufacturers recommendations. Fast-forward to 2011. Things have changed and many new things came out. Like cheap Wi-Fi adapters working off of USB and a aMAZEng stores like IKEA where when things are bought they shouldnt be use per manufacturers recommendations, because they wont last that way long enough to be economically viable. Cheap pine tables crumble under a weight of my morning coffee mug. But they do look great . So IKEA sell a bunch of nice metal shiny things and calls them kitchen utensils, organizers and you name it. That is where I got these two vegetable steamers: This is the unfoldable steamer. I like them for their holes. This looks more like a piece from Erector construction set, and you dont have to drill holes yourself. Well this will be used for the second  Dish antenna. This is a cylindrical shape steamer, which I used for my Cantenna. Both of these steamers were $5.99 (they used to be only $3.99, but oh well, inflation is around the corner and prices have risen dramatically for the past year). 1. Drill a hole if you need to. Here is an exort from ( Polomaki M. Ilmajoki (29 September, 2003). Wlan antenna. Waveguide type. Retrieved July, 27, 2003 from http://www.saunalahti.fi/elepal/antenna2.html ): There are three different wavelengths in the waveguide tube. Here they are marked as Lo, Lc and Lg. Lo is the wavelength of the high frequency (hf) signal in open air or Lo/mm = 300 / (f/GHz). Lc is the wavelength of the low cut frequency which depends on tube diameter only Lc = 1,706 x D and Lu is the wavelength of the upper cut frequency Lu = 1,306 x D. Lg is standing wavelength inside the tube; it is function of both Lo and Lc A waveguide which is closed on the other end acts similar as a short circuited coaxial cable. The coming hf signal reflects from ending point and there forms so called standing wave when incoming and reflecting signals in different places are either weakening or amplifying each others. If there is a measuring probe which is moving in axial direction inside the tube there can be found some minimum and maximum points in certain intervals. At the closed end the signal is zero and so will be in halfwave intervals. The first maximum point is quarter wavelength from the closed end. This will be the best place to outlet signal to coaxial line. You can notice that maximum area is quite flat. So the place of the outlet must not be very accurate. It is important to notice that the standing wavelength Lg is not the same as wavelength Lo counted from hf signal. Large tubes are near as open air where Lg and Lo are almost same but when tube diameter becomes smaller the Lg increases effective until there becomes a point when Lg becomes infinite. It corresponds to the diameter when hf signal does not come to the tube at all. So the waveguide tube acts as a high pass filter which limits wavelength Lc=1.706xD. Lo can be calculated from nominal frequency: Lo/mm = 300/(f/GHz). Inverse values of Lo, Lc and Lg forms a right angled triangle where becomes the equation of Pythagoras: (1/Lo) 2 = (1/Lc) 2 + (1/Lg) 2 which can be solved Lg = 1 / SQR((1/Lo) 2 - (1/Lc) 2 ) In the antenna the N connector is situated in maximum point or length of Lg/4 from the closed end. Total length of the tube is selected so that the next maximum place hits on the open end of the tube or 3/4xLg from the closed end.  Done with the hole and dimensions, whew. That was the most difficult part. 2. Tripod adapter. If you have a tripod it is good, if you dont then just be creative. Tape it to the stick or a broom for goodness sake. I found several bolts in the garage that fit my tripod adapter and use them to attach can to the adapter and then to the tripod itself. Take a look: 3. Wi-Fi USB adapter. As I mentioned earlier I bought my adapter on Amazon for about $20.00, it includes separate Omni antenna and USB extender cable. Attaching it to my can full of hole was not a problem. I simply unscrewed Omni antenna and stick coaxial connector through the hole. Then placed antenna inside and screw it back to the adapter. Of course this is going to be a little deviation from the theory, but we are here simply demonstrating the concept and it will still work, although not at its best ability. 4. Final assembly. Once adapter is attached I place the whole thing on the tripod and connect it to the computer. Here are the pictures of final assembly: Remember this is a very directional antenna, so angle between receiver and transmitter as well as obstacles will matter. 5. Future improvements. This antenna can be combined with Yagi-Uda antenna design by simply placing Yagi antenna inside the can. Can will filter noise (harmonics caused by other signals, such as electrical generators, mostly from cars). Yagi will act as a high gain component increasing strength of the incoming/outgoing signals. In addition to the above, precise design will help increase antenna performance. DISHTENNA Dish antenna is made from a different type of vegetable steamer found in IKEA , or any other kitchen supplier for that matter. The underline physical principle, however, is very different from can antenna, but it is easier to achieve better precision with this design and you dont event need to perform calculations. You will only need to measure Mirrors Focal Point, I will show you how in a little bit. Here is a little bit of theory behind it. Here is a picture of the main component: The problem with this steamer is that when you set it to its side it collapses inward due to gravity. Like in the picture below: The simple way to address this problem is to strap the sides to the center. Be creative, I used staples, you may use paper clips or tape. Here is a set of pictures which shows how staples were attaches. View from the back (outside): View from the front (inside): Now to the tripod adapter: There is a metal loop on the back of the steamer. which is used as a leg when and if used for its intended purpose. I put the same kind of bolt I used for can antenna and attach it to the tripod adapter. Black part on the picture is a tripod adapter. Now the hard part. Finding the focus. I taped the ruler to the middle point and use a flash light to shine on to the dish. I told you it is good this thing is shiny . Reflected light concentrates in the focal point. The two pictures below show you the setup and then (darker picture) the measurements. My apologies for the upside-down ruler. I am not a professional photographer, that is why I has to take two pictures with regular lighting and with dimmed lights and long exposure so focus is seen clear on the picture. It looks like focus is somewhere between 12 and 15 cm. This is good enough for our experiment. USB cable I bolted a little metal extender which came with the dish and taped USB cable which came with the card. This is just regular USB extender cable. See the pictures below: Final assembly Looks very interesting. Measurements Now the really interesting part. I use five different sets of measurements and a trial version of WirelessMon a Wi-Fi utility to monitor signal strength and other properties. Here is a list of experiments: - adapter without antenna - adapter with regular Omni antenna - Can antenna - Dish antenna with no Omni antenna - Dish and Omni antennas together. And below a the corresponding results: 1. With only internal adapters antenna the signal strength was measured at 86dB. Good enough. But notice that no other networks are visible like that. 2. Signal strength is much better with Omni antenna and it is able to pick up many more networks. A 3 dB increase on a logarithmic scale is a lot! The beam pattern of Omni antenna is just a circle, so it catches everything irrespective of the direction and thus more networks are seen. 3. Can antenna is directional, some networks just didnt get refreshed from previous experiment, but can also works as a filter lowering noise and thus increasing signal to noise ratio so it is easier for Wi-Fi adapter to extract useful signal out of everything else. Notice 76 dB  another 7 dB increase over previous configuration and 10 dB over first experiment. 4. Notice the strength diagram  it is very large. 73 dB and another 3 dB increase and no other networks are seen due to directionality. This antenna increases gain of the signal, but doesnt reduce any noise, but signal to noise ratio still increases. 5. This is dish with Omni antenna. The signal is 78 dB due to the fact that antenna is a little off of the focal point. Also notice more networks and Omni antenna contributes to this fact. Still overall signal strength is very good. Summary With a small budget, little time and little precision you may greatly improve the Wi-Fi reception of your adapter. The measurements shown here will likely be different in your configuration as it will be hard to precisely replicate physical conditions. In addition different hardware, distance between wi-fi components, layout and materials of the surrounding structures will affect results. However, you should be able to see similar changes when using these types of antennas. To increase performance antennas have to be constructed with better accuracy, in addition different designs can be implemented, such as Yagi element inside can antenna. This, however, will likely increase budget, time and complexity of the project. Please post your comments if you have any questions. Happy coding ... ooops... Happy cooking! P.S. Don't forget GPS when you plan your trip to IKEA. And if you don't have one IKEA provides nice cafeteria for their stranded shoppers and makes sure they go through every inch of their shelf space.]] Technorati Tags: Closure , Continuation , Inverse Control Flow , Async , Async CTP , Asynchronous , Task , TaskT , Lambda Functions In this post I will talk about new Async features in .NET Framework (still in CTP with SP1 Refresh mode as of May 2011). These features add new syntax and implicit control structures by compiler to seamlessly convert sequential code into asynchronous one. Let us see in detail what are the problems we currently have with asynchronous method invocations, handling control flow and exceptions, and also how new features can help us (e.g. programmers, developers, coders and etc.) manage our asynchronous code in much more elegant way. In essence using new Async features will help reduce complexity of asynchronous code by introducing new syntax. This syntax is simple enough and make asynchronous code look almost like sequential code. Under the hood compiler converts these new structures into implicit closures and returns a continuation delegate using class Task or TaskT. The caller in turn receives partial results right away in the form of instance of Task, TaskT, which holds continuation reference, or void (in case of fire and forget). Caller is also able to monitor task progress as well as get results once task is completed. Once asynchronous process is completed a continuation delegate is called which populates final result of type T (in case of TaskT) and an event on Task is raised, which allows callers to react appropriately. Errors are raised within the context of async method and handled just as any other errors are handled in sequential code. Most of this is compiler generated and there is no more a need to wire events, closures, continuations and error handling manually. What in the world is this and will I ever have a need for this? When will it make sense? It depends how much parallelism your application needs/requires. With a broad adoption of multi-core and recently many-core processors people expect applications to be faster and more responsive; until applications know how to run code asynchronously there isnt going to be much advantage in using multi-core processors by one application, besides many blocking threads are computationally expensive. For example, if there is a need or a way to request N (where N1) downloads from remote resource and network delay in accessing each resource as delta_t1..delta_tn then sequentially written code will have to wait at least Sum(delta_ti)(for all i=1..n) and on top of that actual download time. In case of asynchronous code the delay will be Max(delta_ti)(where i=1..n) + overhead_time. The overhead_time is the time it takes to manage different threads and effectively execute them in parallel, it also depends on the availability of resources (for multiple intensive IO threads it will just add too much overhead for context switching) and grows somewhat proportionately to the number of threads. Therefore it makes sense to use asynchrony when Sum(delta_ti) (Max(delta_ti) + overhead_time). Therefore processes such as web requests, data requests from a server, intensive computations are all good candidates for running in parallel or at least off of the main or UI threads. For IOs it will be when they are running separate from UIs or main threads. As I mentioned running multiple intensive IOs over a single shared resource can increase overhead rather rapidly. But in case of independent resources running IOs on different threads will actually improve performance. So the rule of thumb. Can the infrastructure effectively support parallelism? - If yes, then processing request over such infrastructure asynchronously will be beneficial. - If the answer is no, then there is a chance of overhead for context switching between different threads will be greater. Current State of Asynchrony Currently .NET 4.0 Framework implements Task and TaskT. MSDN has great examples and design patterns using this class for asynchronous method calls see here , here and here . I mostly use .NET 3.5 and thus have to have more plumbing. Lets look at how we can execute asynchronous task without Task and Async features and the problems we run into while doing it. Here is the main idea: some method // do some prelim work ... BackgroundWorker worker = new BackgroundWorker(); // some kind of way to spin-off tasks in the form of delegates(e.g. function pointers) // need to run long process, but want to free up UI Action myDelegate = delegate { do.this(); do.that(); do_for_very_long.this(); }; EventHandler workerCompleted = (s, ea) = { // unhook event handlers worker.Completed -= workerCompleted; worker.Cancelled -= workerCancelled; ... // handle results }; EventHandler workerCancelled = (s, ea) = { // unhook event handlers worker.Completed -= workerCompleted; worker.Cancelled -= workerCancelled; ... // handle errors }; worker.Completed += workerCompleted; worker.Cancelled += workerCancelled; worker.RunAsync(myDelegate); end of some method What you see above is inverse flow of control, closures, lambda functions, continuations. But this is typically what happens when there is a need to execute asynchronous process. 1. We create local variables on the main thread, which are part of continuation state for spin-off threads. Here is a definition from Wikipedia: In computer science and programming , a continuation is an abstract representation of the control state . A continuation reifies an instance of a computational process at a given point in the process's execution. It contains information such as the process's current stack (including all data whose lifetime is within the process e.g. "local variables"), as well the process's point in the computation. Such an instance can then be later resumed upon invocation. The " current continuation " or "continuation of the computation step" is the continuation that, from the perspective of running code, would be derived from the current point in a program's execution. The term continuations can also be used to refer to first-class continuations , which are constructs that give a programming language the ability to save the execution state at any point and return to that point at a later point in the program. In other words Async does continuations complitely behind the scenes while here we have to wire up events to listen for a callback when async method completes. 2. We define the execution steps for the asynchronous work, but do not run it yet. This may or may not use closure. Closure is a way of passing continuation state. So if we were to reference in this function variables which were previously defined on our main thread then compiler will automatically preserve the references for the time when function execution will occur, subject to GC rules. Most commonly closures are used with lambda functions or anonymous function delegates, while these are different concepts. The other way is to explicitly create a named function with arguments and pass all data via these arguments. Sometimes this could be a daunting task. Here is a definition for closures: In computer science , a closure is a function together with a referencing environment for the nonlocal names ( free variables ) of that function. Such a function is said to be "closed over" its free variables. The referencing environment binds the nonlocal names to the corresponding variables in scope at the time the closure is created, additionally extending their lifetime to at least as long as the lifetime of the closure itself. ... Closures are used to implement continuation passing style , and in this manner, hide state . Constructs such as objects and control structures can thus be implemented with closures. In some languages, a closure may occur when a function is defined within another function, and the inner function refers to local variables of the outer function. At runtime , when the outer function executes, a closure is formed, consisting of the inner functions code and references to any variables of the outer function required by the closure; such variables are called the upvalues of the closure. The term closure is often mistakenly used to mean anonymous function . This is probably because most languages implementing anonymous functions allow them to form closures and programmers are usually introduced to both concepts at the same time. These are, however, distinct concepts. A closure retains a reference to the environment at the time it was created (for example, to the current value of a local variable in the enclosing scope) while a generic anonymous function need not do this. and lambda functions : In computing , an anonymous function (also function constant or function literal ) is a function (or a subroutine ) defined, and possibly called, without being bound to an identifier . Anonymous functions are convenient to pass as an argument to a higher-order function and are ubiquitous in languages with first-class functions . Anonymous functions originate in the work of Alonzo Church in his invention of the lambda calculus in 1936 (prior to electronic computers), in which all functions are anonymous. The Y combinator can be utilised in these circumstances to provide anonymous recursion , which Church used to show that some mathematical questions are unsolvable by computation. (Note: this result was disputed at the time, and later Alan Turing - who became Church's student - provided a proof that was more generally accepted.) Anonymous functions have been a feature of programming languages since Lisp in 1958. An increasing number of modern programming languages support anonymous functions, and some notable mainstream languages have recently added support for them, the most widespread being JavaScript also C# and PHP support anonymous functions. Anonymous functions were added to the C++ language as of C++0x  3. We define two event handlers to handle successful completion and exceptional completion. These are defined using lambda functions and they do use closures to unhook event handlers after event is triggered. Still no execution occurred yet and therefore up to this point everything is running blazing fast. 4. We are registering for events by hooking up our error handlers. 5. We spin-off asynchronous thread and immediately return. This is where execution actually starts but function call has already completed. Now control flow goes through the function(s) we defined earlier in our method and, once completed, will either call Completed handler or Cancelled handler if there are errors. This way of executing is called Inversion of Control Flow . In computer programming , Inversion of control ( IoC ) is an abstract principle describing an aspect of some software architecture designs in which the flow of control of a system is inverted in comparison to procedural programming . In traditional programming the flow of the business logic is controlled by a central piece of code, which calls reusable subroutines that perform specific functions. Using Inversion of Control this "central control" design principle is abandoned. The caller's code deals with the program's execution order, but the business knowledge is encapsulated by the called subroutines. The problem with Inversion of Control is that it is not linear and adds a lot of noise (plumbing) to business logic. It is very easy to introduce bugs using this style of programming. What a mess, huh? So how is it different with Async? If you read  Async Whitepaper  you will notice a somewhat similar example. But let us walk use our example and see how we can benefit from Async. First we need to change our method declaration to use keyword async and a return type TaskT, that is if we are planning to return some results of type T. private async Taskstring MyFunctionAsync() { string result = String.Empty; Actionstring myDelegate = delegate { do.this(); do.that(); do_for_very_long.this(); return success; }; try { result = await Task.Run(myDelegate); // notice AWAIT keyword. This is new syntax!!! // the rest of the code will be wrapped into continuation, // and called only when the above finished executing. // while our MyFunctionAsync will return immediately with Taskstring, // but string will be empty until this part is finished. // handle when success ... } catch { // handle when failure result = failure; } return result; } This code looks a lot simpler and less error prone, while executes almost exactly the same as the one we described earlier! In conclusion. We looked at the mechanisms used to implement new Async functionality in C# Async CTP libraries. Talked about when it is appropriate to use asynchronous programming model. How it is currently being done and the number of problems associated with current asynchronous programming style. At the end we took a look at the new features of Async library (it will be part of C# 5.0 compiler later on) and a much more simplified programming model. We pointed to original whitepaper for Async library in hopes that readers will refer to it as well. In the process we have discussed some computer science concepts such as closures, lambda functions, continuations and inverse of control. I hope some of the readers will find this information useful. Happy coding! ]] Technorati Tags: SQL CLR , record linkage , Jaro , Winkler , fuzzy , SOUNDEX If you need to perform fast fuzzy record linkage and dont like SOUNDEX, than you might need to look into Jaro-Winkler algorithm. In this article I will explain what this algorithm does, give you a source code for SQL CLR function, and give an example of use cases for this algorithm such fuzzy linkage and probabilistic linkage. Lets bring everybody up to speed first. Consider the following scenario. You have thousands (or millions for that matter) of data records and you either need to weed out duplicate records or link two data sets together which dont have a common key. The typical approach for finding duplicates is to perform a cross join with some tricky but deterministic type queries, and/or use SOUNDEX, LIKE and other functions which are provided by SQL Server, to determine links between primary keys for records containing duplicate information. In essence this problem is similar to linking two distinct data sets, with only difference being that links are established between different primary keys. This works well for some scenarios, when data is well known, size is not large, it is easy to determine false duplicates and it is a one time job, which means that you will never need to do it again. For those of us who are linking millions of records more than once in a life time there are other ways to consider. Please welcome Jaro-Winkler algorithm . The JaroWinkler distance (Winkler, 1990) is a measure of similarity between two strings . It is a variant of the Jaro distance metric (Jaro, 1989, 1995) and mainly used in the area of record linkage (duplicate detection). The higher the JaroWinkler distance for two strings is, the more similar the strings are. The JaroWinkler distance metric is designed and best suited for short strings such as person names. The score is normalized such that 0 equates to no similarity and 1 is an exact match. How is it better than SOUNDEX? It is not. SOUNDEX provides a phonetic matching which can be useful in some scenarios, while Jaro-Winkler string distance algorithm estimates a difference or, roughly, a number of character replacements it takes to convert one string to another. Hence you may derive optimization technique, which I will talk about a little later in this article. Another advantage in comparison is that if there is a typo in a word and the typo produces different sound then SOUNDEX will not match it correctly. For example, if you typed  ytpe  instead of  type  then SOUNDEX will not give you a correct match. The codes returned for SOUNDEX(ytpe) and SOUNDEX(type) are the following: ytpe type Y310 T100 If I use Jaro-Winkler StringDistance(ytpe,type) here is what I get 0.91(6) , which means it is a good match. Remember 1  is exact match, so 0.91 is very close to it. Typically you will have something like this: SELECT * FROM myTable1 as t1 INNER JOIN myTable2 as t2 ON dbo.StringDistance(t1.MyField, t2.MyField) 0.85 AND -- some other filters such as string Length Jaro-Winkler In Code Below I will provide a SQL CLR version of this algorithm. Which is written in C#.NET and can be deployed and invoked as a function from T-SQL scripts to run on your SQL Server. using System; using System.Data; using System.Data.SqlClient; using System.Data.SqlTypes; using Microsoft.SqlServer.Server; using System.Text; using System.Text.RegularExpressions; public partial class UserDefinedFunctions { /// summary /// This region contains code related to Jaro Winkler string distance algorithm. /// /summary #region Jaro Distance private const double defaultMismatchScore = 0.0; private const double defaultMatchScore = 1.0; /// summary /// gets the similarity of the two strings using Jaro distance. /// /summary /// param name="firstWord"/param /// param name="secondWord"/param /// returnsa value between 0-1 of the similarity/returns /// [Microsoft.SqlServer.Server.SqlFunction] public static System.Data.SqlTypes.SqlDouble StringDistance( string firstWord, string secondWord) { if ((firstWord != null ) (secondWord != null )) { if (firstWord == secondWord) { return (SqlDouble)defaultMatchScore; } else { //get half the length of the string rounded up - (this is the distance used for acceptable transpositions) int halflen = Math.Min(firstWord.Length, secondWord.Length) / 2 + 1; //get common characters StringBuilder common1 = GetCommonCharacters(firstWord, secondWord, halflen); int commonMatches = common1.Length; //check for zero in common if (commonMatches == 0) { return (SqlDouble)defaultMismatchScore; } StringBuilder common2 = GetCommonCharacters(secondWord, firstWord, halflen); //check for same length common strings returning 0.0f is not the same if (commonMatches != common2.Length) { return (SqlDouble)defaultMismatchScore; } //get the number of transpositions int transpositions = 0; for ( int i = 0; i commonMatches; i++) { if (common1[i] != common2[i]) { transpositions++; } } int j = 0; j += 1; //calculate jaro metric transpositions /= 2; double tmp1; tmp1 = commonMatches / (3.0 * firstWord.Length) + commonMatches / (3.0 * secondWord.Length) + (commonMatches - transpositions) / (3.0 * commonMatches); return (SqlDouble)tmp1; } } return (SqlDouble)defaultMismatchScore; } /// summary /// returns a string buffer of characters from string1 within string2 if they are of a given /// distance seperation from the position in string1. /// /summary /// param name="firstWord"string one/param /// param name="secondWord"string two/param /// param name="distanceSep"separation distance/param /// returnsa string buffer of characters from string1 within string2 if they are of a given /// distance seperation from the position in string1/returns private static StringBuilder GetCommonCharacters( string firstWord, string secondWord, int distanceSep) { if ((firstWord != null ) (secondWord != null )) { StringBuilder returnCommons = new StringBuilder(20); StringBuilder copy = new StringBuilder(secondWord); int firstLen = firstWord.Length; int secondLen = secondWord.Length; for ( int i = 0; i firstLen; i++) { char ch = firstWord[i]; bool foundIt = false ; for ( int j = Math.Max(0, i - distanceSep); !foundIt j Math.Min(i + distanceSep, secondLen); j++) { if (copy[j] == ch) { foundIt = true ; returnCommons.Append(ch); copy[j] = '#'; } } } return returnCommons; } return null ; } #endregion #region String Functions [Microsoft.SqlServer.Server.SqlFunction] public static System.Data.SqlTypes.SqlInt32 FirstIndexOf( string text, string searchWord) { if ((searchWord == null ) || (searchWord.GetType() == typeof (DBNull))) { searchWord = " "; } if ((text == null ) || (text.GetType() == typeof (DBNull))) { text = " "; } return (SqlInt32) text.IndexOf(searchWord); } [Microsoft.SqlServer.Server.SqlFunction] public static System.Data.SqlTypes.SqlInt32 FirstIndexOfPattern( string text, string pattern) { Regex myRegEx = new Regex(pattern); if (!myRegEx.IsMatch(text)) { return -1; } return myRegEx.Match(text).Index; } [Microsoft.SqlServer.Server.SqlFunction] public static System.Data.SqlTypes.SqlInt32 LastIndexOf( string text, string searchWord) { if ((searchWord == null ) || (searchWord.GetType() == typeof (DBNull))) { searchWord = " "; } if ((text == null ) || (text.GetType() == typeof (DBNull))) { text = " "; } return (SqlInt32)text.LastIndexOf(searchWord); } [Microsoft.SqlServer.Server.SqlFunction] public static System.Data.SqlTypes.SqlBoolean Contains( string text, string searchWord) { if ((searchWord == null ) || (searchWord.GetType() == typeof (DBNull))) { searchWord = " "; } if ((text == null ) || (text.GetType() == typeof (DBNull))) { text = " "; } return (SqlBoolean)text.Contains(searchWord); } #endregion }; I do provide additional string functions for convenience. To deploy these functions you will need to create CLR project in Visual Studio and paste code from here. Specify your target server and click deploy. How to optimize? If you are working with a somewhat static set such as for spell check, then I recommend doing the following to optimize performance: calculate Length of the strings and save it into one of the columns, then during comparison exclude everything that is substantially different in length than a compared string. So if compared string has 10 characters then limit your search to only strings with 10  3 in length and dont worry about the rest. Most likely other strings will be a bad match anyway. add index to both the strings which participate in comparison and a sorted index for string length. In fact sort the whole table by string length and then by string values. you may also fine tune on how many results you like to get by adjusting the distance weight either lower to get more results or higher to get less results, 0.85 seems to work well for me. clean your data remove insignificant characters. If strings are too long and you have something like MYCOMPANY ABC and MYCOMPANY CDE you will get false positives, if these are in fact two different names. So in this case remove white spaces and MYCOMPANY . If these two strings represent the same name than simply remove white spaces and limit comparison by length, using substring as a percentage of the total length. LEARN YOUR DATA. Fine tuning will ultimately needs to be performed over a specific set, so knowing exactly what is in your tables is important. Typically in the vocalbulary of 100 000 entries such filters reduce the number of comparisons to about 300. Unless you are doing a full blown record linkage in that case you will need to apply probabilistic methods and calculate scores. Also in MS SQL Server Jaro Winkler string distance wrapped into CLR function perform much better, since SQL Server doesn't support arrays natively and much of the processing in this algorithm revolves around arrays. So implementation in T-SQL adds too much overhead, but SQL-CLR works extremely fast. Applications .csharpcode, .csharpcode pre { font-size: small; color: black; font-family: consolas, "Courier New", courier, monospace; background-color: #ffffff; /*white-space: pre;*/ } .csharpcode pre { margin: 0em; } .csharpcode .rem { color: #008000; } .csharpcode .kwrd { color: #0000ff; } .csharpcode .str { color: #006080; } .csharpcode .op { color: #0000c0; } .csharpcode .preproc { color: #cc6633; } .csharpcode .asp { background-color: #ffff00; } .csharpcode .html { color: #800000; } .csharpcode .attr { color: #ff0000; } .csharpcode .alt { background-color: #f4f4f4; width: 100%; margin: 0em; } .csharpcode .lnum { color: #606060; } I have personally applied this algorithm for a Spell-Check program against large medical dictionary. It worked really well (under 0.5 sec) on modest hardware (dual CPU with 2 GB RAM and a SCSI drives) and 150 000 dictionary words. Also it can be used in Probabilistic record linkage for string comparisons. Probabilistic Record linkage methodology calculates probability weights for strings within dataset. For common string values are lower than than for uncommon. The weights from different calculations are added to a final score. The scores most likely will have a normal distribution. There will be positives, false positives, false negatives and negatives. A threshold is determined for false positive or false negatives. You definitely keep positives and remove negatives, for the rest apply somewhat manual review. Some apps have these steps very well automated, for example SQL Match program developed by Patrick Smith. I know Patrick personally and worked with him for several years, what he claims on this web site is true and works just as described. The other use is for finding duplicates. This is most common scenario for data cleansing. I have also used it for fuzzy matching and data cleaning efforts multiple times, providing users with the ability to look at the matches in different ways. In my next post I will try to apply this algorithm for word suggestions on a mobile platform Windows Phone 7. I am interested in how well it will perform given the fact that SQL CE is very limited and some parts will have to be implemented in plain C#, Ill try to use LINQ though. Happy coding! ]] Over the years of working with databases I had to deal with recursive queries several times, first in Access, then in Oracle, then in SQL 2000. While Oracle 9i database had support for recursive queries using CONNECT BY, none of the other database management systems had a clean approach to execute recursive queries. In SQL Server 2005 Microsoft added a new feature called Common Table Expressions and many people picked it up as a way to perform recursive queries. The use case is typically for data which contains some sort of parent-child relationship where each child can be a parent as well. MSDN has an article describing how to use CTEs, please take a look. Lets take an Employees table which among other employee data has EmployeeId and ManagerId. Suppose we want to find all Smiths subordinates in that table. Of course, if there are multiple people with the same last name those will get pulled as well. If Mr. Smith is the head of the company then all employees will show up at different levels, if Mr. Smith doesnt manage anybody then only Mr. Smith will show up. To run a request like that we need: - to create a table valued function - define output table - define anchor member (or first level of the recursion) - define recursive member - execute expression and insert data into output table Please take a look at the code: CREATE FUNCTION [dbo] . [uft_GetEmployeeHierarchy_ByLastName] ( -- Add the parameters for the function here @LastName varchar ( 500 ) ) RETURNS @ReturnList TABLE ( -- Add the column definitions for the TABLE variable here EmployeeId int , ManagerId int , -- [Level] as int -- may also return level ) AS BEGIN -- Fill the table variable with the rows for your result set With EmployeeHierarchy ( [LastName] , EmployeeId , ManagerId , [Level] ) as ( -- Anchor member definition SELECT Distinct ee . [LastName] , ee . EmployeeId , ManagerId , 1 as [Level] FROM Employees as ee WHERE ee . [LastName] like ( '%' + @LastName + '%' ) UNION ALL -- Recursive member definition SELECT ee . [LastName] , ee . TypeModalityId , ee . ParentModalityId , [Level] + 1 as [Level] FROM EmployeeHierarchy as eh inner join Employees ee on eh . TypeModalityId = ee . ParentModalityId ) -- Statement that executes CTE INSERT INTO @ReturnList ( EmployeeId , ManagerId ) SELECT DISTINCT EmployeeId , ManagerId FROM EmployeeHierarchy ORDER BY [Level] , ManagerId , EmployeeId RETURN END That is it. Very simple. Please let me know if you have questions! Technorati Tags: CTE , Commong Table Expressions , T-SQL , Recursion ]] 